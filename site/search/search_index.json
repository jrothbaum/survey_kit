{"config":{"lang":["en"],"separator":"\\.","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Survey Kit","text":"<p>Tools for addressing missing data problems (nonresponse bias and item missingness) including extremely fast calibration weighting and machine learning-based imputation.</p>"},{"location":"#overview","title":"Overview","text":"<p>Survey Kit is a Python package designed to help researchers and data scientists tackle common challenges in survey research, primarily:</p> <ol> <li>Nonresponse bias - When your sample doesn't represent the population</li> <li>Item missingness - When respondents skip questions</li> </ol> <p>** Note, this is not only a survey issue ** Administrative data (adrecs) have these challenges, too.  Nonresponse bias can be caused by adminstrative rules on required compliance or selective compliance.  For example, many low income individuals are not required to file taxes.  Some workers may be paid \"under the table\".  Likewise, reported information in administrative data may be incomplete.</p> <ul> <li>Nonresponse bias through calibration weighting.  Calibration weighting is the standard practice of weighting the sample to match some set of known or estimated totals (like race x age x gender cells or race x income cells)</li> <li>Item missingness use Sequential Regression Multivariate Imputation (SRMI) to draw plausible values for missing data (NOT the mean or mode, but from the estimated distribution of values conditional on some set of observable characteristics in the data).  Tools for basic imputation (hot deck), more standard approaches (regression-based estimatation and predicted mean matching), and better approaches (quantile-based machine learning with LightGBM).</li> <li>Survey data manipulation some useful tools for exploring the data.  More usefully, tools to work with bootsrapped and replicate-weight based standard errors (as done by the U.S. Census Bureau) and multiple imputation.  With these tools, you can estimate statistics once (costly over all replicates) and then get SEs for any arbitrary set of comparisons quickly.</li> </ul> <p>The package provides three core components:</p>"},{"location":"#calibration-weighting","title":"Calibration Weighting","text":"<p>Extremely fast implementation of calibration weighting (akin to raking) to adjust survey samples to match known population totals (from census data, administrative records, etc.). </p> <p>This helps correct for nonresponse bias and which can make the sample more representative.  Similar to packages like Entropy Balancing.  It's been a while since I've benchmarked other tools in this space, but the algorithm in Carl Sanders's Accelerated Entropy Balancing package has been the fastest and most robust (as in it converges or can be used to get a \"best possible\"-ish set of weights) I've found. </p>"},{"location":"#imputation-using-sequential-regression-multivariate-imputation-srmi","title":"Imputation using Sequential Regression Multivariate Imputation (SRMI)","text":"<p>Any imputation model assigns plausible values of \\(y\\) given some set of observable characteristics \\(X\\), through the estimation and drawing from the distribution \\(f(y|X)\\).  How you define and estimate \\(f\\) and take draws varies across methods, but the basic idea is the same.  This package implements hot deck imputation (and its near-twin statistical matching), corresponding the methods used by the U.S. Census Bureau and other statistical agencies.  It also implements a regression-based imputation (where \\(f(y|X)\\) is estimated through OLS regression) with values drawn either based on the regression results (y hat and e hat) or using predicted mean matching.  </p> <p>However, the method I have found to be the best uses LightGBM to estimate \\(f(y|X)\\) either as in a regression or more akin to a quantile regression.  With the machine-learning estimate of \\(f(y|X)\\), you can draw values 1. using predicted mean matching (which I'd recommend for small-ish samples in the thousands) or 2. by imputing a rank in the distribution drawn from the estimated \\(f(y|X)\\) quantile regressions and a separately estimated marginal distribution (also estimated from \\(f(y|X)\\), but not from the quantile regressions which can be badly biased in the tails).  </p> <p>The machine-learning approach can be much better than other approaches because it allows for 1. flexibly estimating \\(f(y|X)\\) with fewer assumptions about what should be included in \\(X\\) (the variables themselves, which should be interacted, etc.), 2. it is much easier to estimate unconditional quantiles (i.e. rather than just estimating the expected earnings, we can quickly estimate the 90th percentile of $y|X) so that we can incorporate that information in our imputation.  For example, suppose college graduates earn a% more than high school graduates, but the 90th percentile of college graduates earns b% more and the 10th percentile earns c% more.  We can incorporate estimates of a, b, and c in the imputation through this approach.  </p> <p>This is not a trivial issue, many imputations fail in their intended use because of practical limitations on what can be included in \\(X\\) (see this paper for an example).</p>"},{"location":"#estimation-of-summary-stats-with-standard-errors-multiple-imputation-and-bootstrapped","title":"Estimation of summary stats with standard errors (Multiple Imputation and Bootstrapped)","text":"<p>The package has useful tools for exploring the data and estimating basic summary stats, and maybe more usefully, tools to work with bootsrapped and replicate-weight based standard errors (as done by the U.S. Census Bureau) and multiple imputation.  With these tools, you can estimate statistics once (costly over all replicates) and then get SEs for any arbitrary set of comparisons quickly.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>DataFrame agnostic - Works with Polars, Pandas, Arrow, DuckDB, etc. via Narwhals</li> <li>Fast - Optimized for large datasets (100K+ rows)</li> <li>Flexible formulas - R-style formulas via Formulaic</li> <li>Serializable - Save and load calibration/imputation so you can store results from long-running estimation processes for use later</li> <li>Parallel processing - Run imputations in parallel</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":""},{"location":"#quick-example-calibration","title":"Quick Example: Calibration","text":"<pre><code>from survey_kit.calibration import Calibration, Moment\nfrom survey_kit.utilities.formula_builder import FormulaBuilder\nimport polars as pl\n\n# Your survey data (doesn't match population)\ndf = pl.read_csv('survey.csv')\n\n# Define the moments (statistics) to match from target population\nf = FormulaBuilder(df=df, constant=False)\nf.continuous(columns=[\"age\", \"income\"])\n\nm = Moment(\n    df=target_population_data,\n    formula=f.formula,\n    weight=\"population_weight\",\n    index=\"id\"\n)\n\n# Calibrate your survey to match the population\nc = Calibration(\n    df=df,\n    moments=m,\n    weight=\"survey_weight\"\n)\n\n# Run calibration\ndiagnostics = c.run(min_obs=10, bounds=(0.1, 10.0))\n\n# Get calibrated weights\ncalibrated_weights = c.df['___final_weight']\n</code></pre>"},{"location":"#quick-example-imputation","title":"Quick Example: Imputation","text":"<pre><code>from survey_kit.imputation.srmi import SRMI\nfrom survey_kit.imputation.variable import Variable\nfrom survey_kit.imputation.parameters import Parameters\n\n# Define variables to impute\nvariables = [\n    Variable(\n        impute_var=\"income\",\n        modeltype=Variable.ModelType.LightGBM,\n        model=[\"age\", \"education\", \"occupation\"],\n        parameters=Parameters.LightGBM(tune=True)\n    ),\n    Variable(\n        impute_var=\"satisfaction\",\n        modeltype=Variable.ModelType.pmm,\n        model=\"~ age + income + education\"\n    )\n]\n\n# Run Sequential Regression Multiple Imputation\nsrmi = SRMI(\n    df=df,\n    variables=variables,\n    n_implicates=5,\n    n_iterations=10,\n    path_model=\"output/srmi\"\n)\n\nsrmi.run()\n\n# Get imputed datasets\nimputed_dfs = srmi.df_implicates\n</code></pre>"},{"location":"#quick-example-statistics","title":"Quick Example: Statistics","text":"<pre><code>from survey_kit.statistics.calculator import StatCalculator\nfrom survey_kit.statistics.statistics import Statistics\nfrom survey_kit.statistics.replicates import Replicates\n\n# Define replicate weights for variance estimation\nreplicates = Replicates(weight_stub=\"repwt_\", n_replicates=80)\n\n# Calculate statistics with proper variance estimation\nsc = StatCalculator(\n    df=df,\n    statistics=Statistics(\n        stats=[\"mean\", \"median\"],\n        columns=[\"income\", \"satisfaction\"]\n    ),\n    weight=\"final_weight\",\n    replicates=replicates,\n    by=dict(region=[\"region\"], year=[\"year\"])\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p><pre><code>pip install survey-kit\n</code></pre> or better yet,</p> <pre><code>uv add survey-kit\n</code></pre>"},{"location":"#why-survey-kit","title":"Why Survey Kit?","text":"<p>Compared to other tools: - Fastest calibration tool that I'm aware of - More flexible than scikit-learn's imputation (and others) and properly handles imputation uncertainty (Bayesian Bootstrap within imputation process, multiple imputation, etc.) - Works across dataframe backends (Polars, Pandas, Arrow) - Designed specifically for survey data workflows</p> <p>Built for: - Survey researchers and data scientists - Government statistical agencies - Market research firms - Academic researchers</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out:</p> <ul> <li>Installation Guide - Get set up</li> <li>Calibration Guide - Fix nonresponse bias</li> <li>Imputation Guide - Handle missing data</li> <li>Statistics Guide - Calculate proper standard errors</li> <li>Examples - See it all together</li> </ul>"},{"location":"#repository","title":"Repository","text":"<p>View the source code on GitHub</p>"},{"location":"#support","title":"Support","text":"<ul> <li>Report issues on GitHub Issues</li> <li>Questions? Open a discussion</li> </ul>"}]}