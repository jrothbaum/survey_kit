{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Survey Kit","text":"<p>Tools for addressing missing data problems (nonresponse bias and item missingness) including extremely fast calibration weighting and machine learning-based imputation.</p>"},{"location":"#what-is-it","title":"What Is It","text":"<p>Survey Kit is a Python package designed to help researchers and data scientists tackle common challenges in survey research, primarily:</p> <ol> <li>Nonresponse bias - When your sample doesn't represent the population</li> <li>Item missingness - When some records have incomplete data</li> <li>Proper measures of uncertainty - with the various strategies surveys use to address missing data, it can be a challenge to estimate proper measures of uncertainty (like standard errors for regression coefficients).  This package tries to cover your bases by having very flexible tools to handle multiply imputed data and replicate weights.</li> </ol> <p>Note: these are not necessarily only survey issues</p> <p>Administrative data have these challenges, too.  Nonresponse bias can be caused by adminstrative rules or selective compliance.  For example, many low income individuals are not required to file taxes.  Some workers may be paid \"under the table\".  Likewise, reported information in administrative data may be incomplete.  I've worked with job-level administrative data with missing gross earnings for millions of jobs. </p>"},{"location":"#why-use-it","title":"Why Use It","text":"<ul> <li>Nonresponse bias through calibration weighting.  Calibration weighting is the standard practice of weighting the sample to match some set of known or estimated totals (like race x age x gender cells or race x income cells)</li> <li>Item missingness use Sequential Regression Multivariate Imputation (SRMI) to draw plausible values for missing data (NOT the mean or mode, but from the estimated distribution of values conditional on some set of observable characteristics in the data).  Tools to implement widely-used (but problematic) imputation strategies (hot deck), more advanced approaches (regression-based estimatation and predicted mean matching), and machine-learning and quantile-statistic based approaches (quantile-based machine learning with LightGBM).</li> <li>Summary stats and proper standard errors there are some (hopefully) useful tools for exploring the data.  Also, tools to work with bootsrapped and replicate-weight based standard errors (as done for many data sets, see CPS ASEC documentation, ACS documentation, Consumer Expenditure Survey documentation, SCF documentation, MEPS documentation, ...)) and multiple imputation.  With these tools, you can estimate statistics once (costly over all replicates) and then get SEs for any arbitrary set of comparisons quickly.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>DataFrame agnostic - Works with Polars, Pandas, Arrow, DuckDB, etc. via Narwhals and that's what you'll get back (sometimes under the hood data will be converted to polars for calculations)</li> <li>Fast - Optimized for large datasets - it has been used for weighting and imputation of data with 10s to 100s of millions of rows and thousands of columns.</li> <li>Flexible - Allows the use of R-style formulas via formulaic in both calibration and imputation (to reduce the amount of work you need to spend manually creating interactions and recoding data), machine-learning imputation via LightGBM (which again makes model specification less time consuming for imputation - you don't have to make so many explicit choices about whether to code variable as a log or exponential, which interactions to include, etc.).</li> <li>Serializable - Save and load calibration, imputation, and calculated statistics so you can store results from long-running estimation processes or for use later in calculations or regressions.  That way, if you want to estimate something like earnings for men and women, then compare them over time, you run the time-consuming estimates once and save them.  Then you can calculate the standard errors for comparisons instantly without having to go back to the original data and recalculating things for each replicate weight and imputation.   </li> <li>Parallel processing - Run imputations and calculate statistics in parallel</li> </ul> <p>Compared to other tools:</p> <ul> <li>Fastest calibration tool that I'm aware of</li> <li>More flexible than scikit-learn's imputation (and others) and properly handles imputation uncertainty (Bayesian Bootstrap within imputation process, multiple imputation, etc.)</li> <li>Works across dataframe backends (Polars, Pandas, Arrow)</li> <li>Designed specifically for survey data workflows</li> </ul> <p>Built for:</p> <ul> <li>Survey researchers and data scientists</li> <li>Government statistical agencies</li> <li>Market research firms</li> <li>Academic researchers</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p><pre><code>pip install survey-kit\n</code></pre> or better yet,</p> <pre><code>uv add survey-kit\n</code></pre>"},{"location":"#user-guides","title":"User Guides","text":"<p>Want to learn more, go to the guides for each tool in the package.  Each guide </p> <ul> <li>Calibration Guide - nonresponse bias</li> <li>Imputation Guide - missing data</li> <li>Statistics Guide - proper standard errors</li> </ul>"},{"location":"#repository","title":"Repository","text":"<p>View the source code on GitHub</p>"},{"location":"#support","title":"Support","text":"<p>Report issues or ask questions on GitHub Issues</p>"},{"location":"api/basic_standard_errors/","title":"Basic Statistics and Standard","text":""},{"location":"api/basic_standard_errors/#survey_kit.utilities.dataframe.summary","title":"summary","text":"<pre><code>summary(\n    df: IntoFrameT,\n    columns: list[str] | str | None = None,\n    weight: str = \"\",\n    print: bool = True,\n    stats: list[str] | str | None = None,\n    detailed: bool = False,\n    additional_stats: list[str] | str | None = None,\n    by: list[str] | str | None = None,\n    quantile_interpolated: bool = False,\n    drb_round: bool = False,\n) -&gt; IntoFrameT\n</code></pre> <p>Generate summary statistics for a dataframe.</p> <p>A convenience function for quickly exploring data. Calculates common summary statistics (mean, std, min, max, etc.) with optional weighting and grouping. Works with any dataframe backend (Polars, Pandas, Arrow, DuckDB) via Narwhals.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Input dataframe to summarize.</p> required <code>columns</code> <code>list[str] | str | None</code> <p>Columns to summarize. Supports wildcards (e.g., \"income_*\"). If None, summarizes all columns. Default is None.</p> <code>None</code> <code>weight</code> <code>str</code> <p>Column name for weights. If provided, calculates weighted statistics. Default is \"\" (unweighted).</p> <code>''</code> <code>print</code> <code>bool</code> <p>Print the summary table. Default is True.</p> <code>True</code> <code>stats</code> <code>list[str] | str | None</code> <p>Statistics to calculate. If None, uses default set. See Statistics.available_stats() for options. Default is None.</p> <code>None</code> <code>detailed</code> <code>bool</code> <p>Use detailed statistics (includes quartiles).  Overrides stats parameter. Default is False.</p> <code>False</code> <code>additional_stats</code> <code>list[str] | str | None</code> <p>Additional statistics beyond the default/detailed set. Examples: [\"q10\", \"q90\", \"n|not0\", \"share|not0\"]. Default is None.</p> <code>None</code> <code>by</code> <code>list[str] | str | None</code> <p>Column(s) to group by before calculating statistics. Default is None.</p> <code>None</code> <code>quantile_interpolated</code> <code>bool</code> <p>Use interpolated quantiles (vs exact values from data). Default is False.</p> <code>False</code> <code>drb_round</code> <code>bool</code> <p>Apply DRB (Disclosure Review Board) rounding rules for 4 significant digits. Useful for publication-ready output. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>Dataframe of summary statistics (same type as input df).</p> <p>Examples:</p> <p>Basic unweighted summary:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.dataframe import summary\n&gt;&gt;&gt; from survey_kit.utilities.random import RandomData\n&gt;&gt;&gt; \n&gt;&gt;&gt; df = RandomData(n_rows=1000, seed=123).integer(\"income\", 0, 100_000).to_df()\n&gt;&gt;&gt; summary(df)\n</code></pre> <p>Weighted summary:</p> <pre><code>&gt;&gt;&gt; summary(df, weight=\"survey_weight\")\n</code></pre> <p>By groups:</p> <pre><code>&gt;&gt;&gt; summary(df, weight=\"survey_weight\", by=\"year\")\n</code></pre> <p>Detailed statistics with rounding:</p> <pre><code>&gt;&gt;&gt; summary(df, weight=\"survey_weight\", detailed=True, drb_round=True)\n</code></pre> <p>Custom statistics:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n&gt;&gt;&gt; Statistics.available_stats()  # See options\n&gt;&gt;&gt; summary(df, additional_stats=[\"q10\", \"q90\", \"n|not0\", \"share|not0\"])\n</code></pre> <p>Specific columns with wildcards:</p> <pre><code>&gt;&gt;&gt; summary(df, columns=[\"income_*\", \"age\"], weight=\"survey_weight\")\n</code></pre> <p>Get results without printing:</p> <pre><code>&gt;&gt;&gt; df_stats = summary(df, weight=\"survey_weight\", print=False)\n&gt;&gt;&gt; print(df_stats.collect())\n</code></pre> Notes <p>Default statistics (if stats=None and detailed=False): - n: Count of non-missing values - n|missing: Count of missing values - mean: Average - std: Standard deviation - min: Minimum - max: Maximum</p> <p>Detailed statistics (if detailed=True): - Adds: q25, q50 (median), q75</p> <p>The \"|not0\" suffix excludes zeros: \"n|not0\" counts non-zero values, \"share|not0\" calculates proportion among non-zero observations.</p> See Also <p>StatCalculator : For standard errors with replicate weights Statistics : For defining custom statistics</p> Source code in <code>src\\survey_kit\\utilities\\dataframe.py</code> <pre><code>@nw.narwhalify\ndef summary(\n    df: IntoFrameT,\n    columns: list[str] | str | None = None,\n    weight: str = \"\",\n    print: bool = True,\n    stats: list[str] | str | None = None,\n    detailed: bool = False,\n    additional_stats: list[str] | str | None = None,\n    by: list[str] | str | None = None,\n    quantile_interpolated: bool = False,\n    drb_round: bool = False,\n) -&gt; IntoFrameT:\n\n    \"\"\"\n    Generate summary statistics for a dataframe.\n\n    A convenience function for quickly exploring data. Calculates common summary\n    statistics (mean, std, min, max, etc.) with optional weighting and grouping.\n    Works with any dataframe backend (Polars, Pandas, Arrow, DuckDB) via Narwhals.\n\n    Parameters\n    ----------\n    df : IntoFrameT\n        Input dataframe to summarize.\n    columns : list[str] | str | None, optional\n        Columns to summarize. Supports wildcards (e.g., \"income_*\").\n        If None, summarizes all columns. Default is None.\n    weight : str, optional\n        Column name for weights. If provided, calculates weighted statistics.\n        Default is \"\" (unweighted).\n    print : bool, optional\n        Print the summary table. Default is True.\n    stats : list[str] | str | None, optional\n        Statistics to calculate. If None, uses default set.\n        See Statistics.available_stats() for options. Default is None.\n    detailed : bool, optional\n        Use detailed statistics (includes quartiles). \n        Overrides stats parameter. Default is False.\n    additional_stats : list[str] | str | None, optional\n        Additional statistics beyond the default/detailed set.\n        Examples: [\"q10\", \"q90\", \"n|not0\", \"share|not0\"]. Default is None.\n    by : list[str] | str | None, optional\n        Column(s) to group by before calculating statistics. Default is None.\n    quantile_interpolated : bool, optional\n        Use interpolated quantiles (vs exact values from data). Default is False.\n    drb_round : bool, optional\n        Apply DRB (Disclosure Review Board) rounding rules for 4 significant digits.\n        Useful for publication-ready output. Default is False.\n\n    Returns\n    -------\n    IntoFrameT\n        Dataframe of summary statistics (same type as input df).\n\n    Examples\n    --------\n    Basic unweighted summary:\n\n    &gt;&gt;&gt; from survey_kit.utilities.dataframe import summary\n    &gt;&gt;&gt; from survey_kit.utilities.random import RandomData\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; df = RandomData(n_rows=1000, seed=123).integer(\"income\", 0, 100_000).to_df()\n    &gt;&gt;&gt; summary(df)\n\n    Weighted summary:\n\n    &gt;&gt;&gt; summary(df, weight=\"survey_weight\")\n\n    By groups:\n\n    &gt;&gt;&gt; summary(df, weight=\"survey_weight\", by=\"year\")\n\n    Detailed statistics with rounding:\n\n    &gt;&gt;&gt; summary(df, weight=\"survey_weight\", detailed=True, drb_round=True)\n\n    Custom statistics:\n\n    &gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n    &gt;&gt;&gt; Statistics.available_stats()  # See options\n    &gt;&gt;&gt; summary(df, additional_stats=[\"q10\", \"q90\", \"n|not0\", \"share|not0\"])\n\n    Specific columns with wildcards:\n\n    &gt;&gt;&gt; summary(df, columns=[\"income_*\", \"age\"], weight=\"survey_weight\")\n\n    Get results without printing:\n\n    &gt;&gt;&gt; df_stats = summary(df, weight=\"survey_weight\", print=False)\n    &gt;&gt;&gt; print(df_stats.collect())\n\n    Notes\n    -----\n    Default statistics (if stats=None and detailed=False):\n    - n: Count of non-missing values\n    - n|missing: Count of missing values\n    - mean: Average\n    - std: Standard deviation\n    - min: Minimum\n    - max: Maximum\n\n    Detailed statistics (if detailed=True):\n    - Adds: q25, q50 (median), q75\n\n    The \"|not0\" suffix excludes zeros: \"n|not0\" counts non-zero values,\n    \"share|not0\" calculates proportion among non-zero observations.\n\n    See Also\n    --------\n    StatCalculator : For standard errors with replicate weights\n    Statistics : For defining custom statistics\n    \"\"\"\n\n    from ..statistics.calculator import StatCalculator\n    from ..statistics.statistics import Statistics\n\n    columns = list_input(columns)\n    stats = list_input(stats)\n    additional_stats = list_input(additional_stats)\n    by = list_input(by)\n    if len(by) &gt; 1:\n        by = [by]\n\n\n    if len(columns):\n        columns = columns_from_list(df=df, columns=columns)\n    else:\n        columns = df.lazy().collect_schema().names()\n\n    if len(stats) == 0:\n        if detailed:\n            stats = [\n                \"n\",\n                \"n|missing\",\n                \"mean\",\n                \"std\",\n                \"min\",\n                \"q25\",\n                \"q50\",\n                \"q75\",\n                \"max\",\n            ] + additional_stats\n\n        else:\n            stats = [\"n\", \"n|missing\", \"mean\", \"std\", \"min\", \"max\"] + additional_stats\n\n    if weight != \"\" and weight in columns:\n        columns.remove(weight)\n    stats = Statistics(\n        stats=stats, columns=columns, quantile_interpolated=quantile_interpolated\n    )\n\n    sc = StatCalculator(\n        df=df,\n        statistics=stats,\n        display=print,\n        round_output=drb_round,\n        weight=weight,\n        by=by,\n    )\n\n    return sc.df_estimates\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator","title":"StatCalculator","text":"<p>               Bases: <code>Serializable</code></p> <p>A comprehensive class for calculating statistical estimates with optional replicate weights.</p> <p>StatCalculator provides a unified interface for computing various statistics on datasets, with support for weighted calculations, replicate weight standard errors, grouping, and comparison operations. It handles both simple estimates and complex bootstrap or replicate weight variance estimation.</p> <p>The class supports: - Multiple statistics calculated simultaneously - Weighted and unweighted estimates - Replicate weight and bootstrap standard errors - Grouping/stratification via by - Automatic disclosure avoidance rounding (which can be disabled) - Comparison operations between sets of estimate</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>A narwhals-compliant dataframe</p> <code>None</code> <code>statistics</code> <code>list[Statistics] | Statistics | None</code> <p>Statistics object(s) defining what columns and statistics to calculate. Each Statistics object specifies variables and statistical measures. Default is None.</p> <code>None</code> <code>weight</code> <code>str</code> <p>Column name for survey weights if weighted estimates are desired. Default is \"\" (unweighted).</p> <code>''</code> <code>scale_wgts_to</code> <code>float</code> <p>Value to scale weights to sum to (proportional adjustment). Default is 0.0 (no scaling).</p> <code>0.0</code> <code>replicates</code> <code>Replicates | None</code> <p>Replicates object for calculating replicate weight standard errors. Generates weight lists from stub names and counts. Default is None.</p> <code>None</code> <code>by</code> <code>dict[str, list[str]] | list | None</code> <p>Dictionary defining grouping variables for stratified estimates. Keys are group names, values are lists of grouping variables. Example: {\"state\":[\"st\"], \"county\":[\"st\",\"cty\"]}. Default is None.</p> <code>None</code> <code>display</code> <code>bool</code> <p>Whether to print results to log automatically. Default is True.</p> <code>True</code> <code>display_all_vars</code> <code>bool</code> <p>Print all variables or truncate display. Default is True.</p> <code>True</code> <code>display_max_vars</code> <code>int</code> <p>Maximum variables to display when display_all_vars=False. Default is 20.</p> <code>20</code> <code>round_output</code> <code>bool | int</code> <p>Apply rounding to output (True for DRB rules, int for sig digits). Default is True.</p> <code>False</code> <code>calculate</code> <code>bool</code> <p>Internal parameter - whether to run calculations immediately. Default is True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>df_estimates</code> <code>IntoFrameT, narwhals compliant dataframe</code> <p>Main estimates dataframe with calculated statistics.</p> <code>df_ses</code> <code>IntoFrameT, narwhals compliant dataframe</code> <p>Standard errors dataframe (populated when replicates are used).</p> <code>df_replicates</code> <code>IntoFrameT, narwhals compliant dataframe</code> <p>Full replicate estimates for additional analysis.</p> <code>variable_ids</code> <code>list[str]</code> <p>Column names that identify unique estimates/variables.</p> <code>summarize_vars</code> <code>list[str]</code> <p>All grouping variables from by (flattened).</p> <code>bootstrap</code> <code>bool</code> <p>Whether bootstrap standard errors are to be calculated, as opposed to replicate weights.</p> <p>Examples:</p> <p>Basic usage with simple statistics:</p> <pre><code>&gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Statistics\n&gt;&gt;&gt; stats = Statistics(columns=[\"income\", \"age\"], statistics=[\"mean\", \"median\"])\n&gt;&gt;&gt; sc = StatCalculator(df=my_data, statistics=stats, weight=\"survey_wgt\")\n&gt;&gt;&gt; sc.print()\n</code></pre> <p>With replicate weights for standard errors:</p> <pre><code>&gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Replicates\n&gt;&gt;&gt; reps = Replicates(weight_stub=\"rep_wgt_\", n_replicates=80)\n&gt;&gt;&gt; sc = StatCalculator(df=my_data, statistics=stats, replicates=reps)\n&gt;&gt;&gt; sc.print()  # Will show standard errors\n</code></pre> <p>Grouped analysis:</p> <pre><code>&gt;&gt;&gt; calc = StatCalculator(\n...     df=my_data,\n...     statistics=stats,\n...     by={\"state\": [\"state_code\"], \"region\": [\"region_code\"]}\n... )\n</code></pre> <p>Comparison between two sets of estimates:</p> <pre><code>&gt;&gt;&gt; sc_1 = StatCalculator(df=data1, statistics=stats)\n&gt;&gt;&gt; sc_2 = StatCalculator(df=data2, statistics=stats)\n&gt;&gt;&gt; comparison = sc_1.compare(sc_2)\n&gt;&gt;&gt; comparison[\"difference\"].print()\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>class StatCalculator(Serializable):\n    \"\"\"\n    A comprehensive class for calculating statistical estimates with optional replicate weights.\n\n    StatCalculator provides a unified interface for computing various statistics on datasets,\n    with support for weighted calculations, replicate weight standard errors, grouping,\n    and comparison operations. It handles both simple estimates and complex bootstrap\n    or replicate weight variance estimation.\n\n    The class supports:\n    - Multiple statistics calculated simultaneously\n    - Weighted and unweighted estimates\n    - Replicate weight and bootstrap standard errors\n    - Grouping/stratification via by\n    - Automatic disclosure avoidance rounding (which can be disabled)\n    - Comparison operations between sets of estimate\n\n    Parameters\n    ----------\n    df : IntoFrameT\n        A narwhals-compliant dataframe\n    statistics : list[Statistics]|Statistics|None, optional\n        Statistics object(s) defining what columns and statistics to calculate.\n        Each Statistics object specifies variables and statistical measures.\n        Default is None.\n    weight : str, optional\n        Column name for survey weights if weighted estimates are desired.\n        Default is \"\" (unweighted).\n    scale_wgts_to : float, optional\n        Value to scale weights to sum to (proportional adjustment).\n        Default is 0.0 (no scaling).\n    replicates : Replicates|None, optional\n        Replicates object for calculating replicate weight standard errors.\n        Generates weight lists from stub names and counts. Default is None.\n    by : dict[str,list[str]]|list|None, optional\n        Dictionary defining grouping variables for stratified estimates.\n        Keys are group names, values are lists of grouping variables.\n        Example: {\"state\":[\"st\"], \"county\":[\"st\",\"cty\"]}. Default is None.\n    display : bool, optional\n        Whether to print results to log automatically. Default is True.\n    display_all_vars : bool, optional\n        Print all variables or truncate display. Default is True.\n    display_max_vars : int, optional\n        Maximum variables to display when display_all_vars=False. Default is 20.\n    round_output : bool|int, optional\n        Apply rounding to output (True for DRB rules, int for sig digits).\n        Default is True.\n    calculate : bool, optional\n        Internal parameter - whether to run calculations immediately.\n        Default is True.\n\n    Attributes\n    ----------\n    df_estimates : IntoFrameT, narwhals compliant dataframe\n        Main estimates dataframe with calculated statistics.\n    df_ses : IntoFrameT, narwhals compliant dataframe\n        Standard errors dataframe (populated when replicates are used).\n    df_replicates : IntoFrameT, narwhals compliant dataframe\n        Full replicate estimates for additional analysis.\n    variable_ids : list[str]\n        Column names that identify unique estimates/variables.\n    summarize_vars : list[str]\n        All grouping variables from by (flattened).\n    bootstrap : bool\n        Whether bootstrap standard errors are to be\n        calculated, as opposed to replicate weights.\n\n    Examples\n    --------\n    Basic usage with simple statistics:\n\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Statistics\n    &gt;&gt;&gt; stats = Statistics(columns=[\"income\", \"age\"], statistics=[\"mean\", \"median\"])\n    &gt;&gt;&gt; sc = StatCalculator(df=my_data, statistics=stats, weight=\"survey_wgt\")\n    &gt;&gt;&gt; sc.print()\n\n    With replicate weights for standard errors:\n\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Replicates\n    &gt;&gt;&gt; reps = Replicates(weight_stub=\"rep_wgt_\", n_replicates=80)\n    &gt;&gt;&gt; sc = StatCalculator(df=my_data, statistics=stats, replicates=reps)\n    &gt;&gt;&gt; sc.print()  # Will show standard errors\n\n    Grouped analysis:\n\n    &gt;&gt;&gt; calc = StatCalculator(\n    ...     df=my_data,\n    ...     statistics=stats,\n    ...     by={\"state\": [\"state_code\"], \"region\": [\"region_code\"]}\n    ... )\n\n    Comparison between two sets of estimates:\n\n    &gt;&gt;&gt; sc_1 = StatCalculator(df=data1, statistics=stats)\n    &gt;&gt;&gt; sc_2 = StatCalculator(df=data2, statistics=stats)\n    &gt;&gt;&gt; comparison = sc_1.compare(sc_2)\n    &gt;&gt;&gt; comparison[\"difference\"].print()\n    \"\"\"\n\n    _save_suffix = \"stats_calc\"\n\n    def __init__(\n        self,\n        df: IntoFrameT | None = None,\n        statistics: list[Statistics] | Statistics | None = None,\n        weight: str = \"\",\n        scale_wgts_to: float = 0.0,\n        replicates: Replicates | None = None,\n        by: dict[str, list[str]] | list | None = None,\n        display: bool = True,\n        display_all_vars: bool = True,\n        display_max_vars: int = 20,\n        round_output: bool | int = False,\n        allow_slow_pandas: bool = False,\n        calculate: bool = True,\n    ):\n        if statistics is None:\n            self.statistics = []\n            calculate = False\n        elif type(statistics) is Statistics:\n            statistics = [statistics]\n\n        self.df = df\n        if statistics is not None:\n            if type(statistics) is not list:\n                statistics = [statistics]\n        self.statistics = statistics\n        self.weight = weight\n        if by is not None:\n            if len(by) == 0:\n                by = None\n\n        self.by = by\n        self.display = display\n        self.display_all_vars = display_all_vars\n        self.display_max_vars = display_max_vars\n        self.round_output = round_output\n        self.summarize_vars = self._by_vars()\n        self.rounding = Rounding(round_output=round_output, round_all=False)\n\n        #   Default columns for variable name id\n        self.variable_ids = [\"Variable\"]\n\n        self.replicates = replicates\n        self.scale_wgts_to = scale_wgts_to\n\n        self.allow_slow_pandas = allow_slow_pandas\n\n        self.replicate_stats = ReplicateStats()\n        if replicates is not None:\n            self.replicate_stats.bootstrap = replicates.bootstrap\n\n        self.scale_weights()\n\n        if calculate:\n            if self.replicates is None:\n                self._calculate()\n            else:\n                self._calculate_replicates()\n\n            self.df = None\n\n    def copy(self):\n        sc_copy = StatCalculator(\n            df=self.df,\n            statistics=copy(self.statistics),\n            weight=self.weight,\n            scale_wgts_to=0,\n            replicates=copy(self.replicates),\n            by=copy(self.by),\n            display=self.display,\n            display_all_vars=self.display_all_vars,\n            display_max_vars=self.display_max_vars,\n            round_output=False,\n            allow_slow_pandas=allow_slow_pandas,\n            calculate=False,\n        )\n\n        sc_copy.scale_wgts_to = self.scale_wgts_to\n        sc_copy.rounding = copy(self.rounding)\n        sc_copy.replicate_stats = self.replicate_stats.copy()\n        sc_copy.variable_ids = self.variable_ids\n\n        return sc_copy\n\n    @property\n    def df_estimates(self):\n        \"\"\"\n        IntoFrameT : Main estimates dataframe containing calculated statistics.\n\n        This property provides access to the primary results table with all\n        calculated statistics. Includes variable identifiers, grouping variables,\n        and statistical estimates as columns.\n        \"\"\"\n        return self.replicate_stats.df_estimates\n\n    @df_estimates.setter\n    def df_estimates(self, value):\n        self.replicate_stats.df_estimates = value\n\n    @property\n    def df_ses(self):\n        \"\"\"\n        IntoFrameT : Standard errors dataframe (when replicate weights are used).\n\n        Contains standard error estimates for all statistics calculated using\n        replicate weight methods. Has the same structure as df_estimates but\n        with standard errors instead of point estimates. Only populated when\n        replicates parameter is provided.\n        \"\"\"\n        return self.replicate_stats.df_ses\n\n    @df_ses.setter\n    def df_ses(self, value):\n        self.replicate_stats.df_ses = value\n\n    @property\n    def df_replicates(self):\n        \"\"\"\n        IntoFrameT : Full replicate estimates dataframe.\n\n        Contains individual estimates for each replicate weight, allowing for\n        custom variance calculations or additional analysis. Includes all\n        columns from df_estimates plus a replicate identifier column.\n        Only populated when replicates parameter is provided.\n        \"\"\"\n        return self.replicate_stats.df_replicates\n\n    @df_replicates.setter\n    def df_replicates(self, value):\n        self.replicate_stats.df_replicates = value\n\n    @property\n    def bootstrap(self):\n        return self.replicate_stats.bootstrap\n\n    def _by_vars(self=None, by: dict | None = None) -&gt; list[str]:\n        \"\"\"\n        Just get a list of the variables to be used as indexes for\n            summary stats (for a select statement)\n        Returns\n        -------\n        list[str]\n            A full list of all the indexes (with no duplicates)\n\n        \"\"\"\n\n        if by is None:\n            by = self.by\n\n        summarize_list = []\n\n        if by is not None:\n            if type(by) is dict:\n                for listi in by.values():\n                    summarize_list.extend(listi)\n            elif type(by) is list:\n                for itemi in by:\n                    if type(itemi) is list:\n                        summarize_list.extend(itemi)\n                    else:\n                        summarize_list.append(itemi)\n\n            summarize_list = list(dict.fromkeys(summarize_list))\n\n        return summarize_list\n\n    def _calculate(self, weight: str | None = None, display: bool | None = None):\n        \"\"\"\n        Parameters\n        ----------\n        weight : str|None, optional\n            Programmer option, do not use. The default is None.\n        display : bool|None, optional\n            Display the results?. The default is None.\n\n        Returns\n        -------\n        Calculate the estimates (no SEs).  Mostly should only be called internally\n        Populates df_estimates\n\n        \"\"\"\n\n        df_collected = None\n\n        if weight is None:\n            weight = self.weight\n\n        for statsi in self.statistics:\n            dfi = statsi.calculate(\n                df=self.df,\n                weight=weight,\n                by=self.by,\n                summarize_vars=self.summarize_vars,\n                rounding=self.rounding,\n                allow_slow_pandas=self.allow_slow_pandas,\n            )\n\n            if df_collected is None:\n                df_collected = dfi\n            else:\n                cols_prior = df_collected.drop(\n                    self.variable_ids + self.summarize_vars\n                ).columns\n                cols_now = dfi.drop(self.variable_ids + self.summarize_vars).columns\n                cols_match = list(set(cols_prior).intersection(cols_now))\n\n                n_rows = df_collected.select(nw.len()).collect().item()\n                df_collected = (\n                    join_wrapper(\n                        df=df_collected.with_row_index(\"__summary_index__\"),\n                        df_to=dfi.with_row_index(\"__summary_index2__\", offset=n_rows),\n                        on=self.variable_ids + self.summarize_vars,\n                        how=\"full\",\n                    )\n                    # df_collected = (\n                    #                     JoinFileList_Simple(\n                    #                         dflist=[\n                    #                                     df_collected.with_row_index(\"__summary_index__\"),\n                    #                                     dfi.with_row_index(\"__summary_index2__\",\n                    #                                                        offset=dfRowCount(df_collected)),\n                    #                                 ],\n                    #                         Join=\"outer\",\n                    #                         JoinOn=self.variable_ids + self.summarize_vars,\n                    #                         join_nulls=True,\n                    #                         quietly=True\n                    #                     )\n                    .with_columns(\n                        nw.coalesce(\n                            nw.col([\"__summary_index__\", \"__summary_index2__\"]).alias(\n                                \"__summary_index__\"\n                            )\n                        )\n                    )\n                    .sort(self.summarize_vars + [\"__summary_index__\"])\n                    .drop([\"__summary_index__\", \"__summary_index2__\"])\n                )\n\n                if len(cols_match):\n                    cols_new = list(set(cols_now).difference(cols_prior))\n                    cols_select = cols_prior + cols_new\n                    with_coalesce = [\n                        nw.coalesce(nw.col(coli, f\"{coli}_right\"))\n                        for coli in cols_match\n                    ]\n\n                    df_collected = df_collected.with_columns(with_coalesce).select(\n                        self.variable_ids + self.summarize_vars + cols_select\n                    )\n\n        self.df_estimates = df_collected\n\n        if self.rounding.round_output:\n            self.df_estimates = self.round_results()\n\n        if display is None:\n            display = self.display\n\n        if display:\n            self.print()\n\n        return self.df_estimates\n\n    def _calculate_replicates(self):\n        \"\"\"\n        Calculate the estimates for each replicate weight\n\n        Returns\n        -------\n        Populates df_estimates, df_ses and df_replicates\n\n        \"\"\"\n\n        replicate_se_return = replicates_ses_from_function(\n            delegate=self._calculate,\n            arguments={\"display\": False},\n            join_on=self.variable_ids + self.summarize_vars,\n            weights=self.replicates.rep_list,\n            bootstrap=self.replicates.bootstrap,\n        )\n\n        self.df_estimates = replicate_se_return.df_estimates\n        self.df_ses = replicate_se_return.df_ses\n        self.df_replicates = replicate_se_return.df_replicates\n\n        if self.rounding.round_output:\n            self.df_ses = self.round_results(df=self.df_ses)\n\n        if self.display:\n            self.print()\n\n    def round_results(\n        self,\n        df: IntoFrameT | None = None,\n        rounding: Rounding | None = None,\n        display_only: bool = False,\n    ) -&gt; IntoFrameT:\n        \"\"\"\n        Parameters\n        ----------\n        df : IntoFrameT, optional\n            Table of estimates. The default is the estimates in df_estimates\n        rounding : Rounding|None, optional\n            Rounding (True for DRB rules) and an integer for specific number of significant digits. The default is self's rounding.\n        display_only : bool, optional\n            If True, affects the display of numbers (casts to strings). The default is False.\n\n        Returns\n        -------\n        df : IntoFrameT\n            The rounded estimates.\n\n        \"\"\"\n\n        if df is None:\n            df = self.df_estimates\n\n        if rounding is None:\n            rounding = self.rounding\n\n        if df is not None:\n            df = drb_round_table(\n                df=df,\n                columns=rounding.cols_round,\n                columns_n=rounding.cols_n,\n                columns_exclude=rounding.cols_exclude,\n                round_all=rounding.round_all,\n                digits=rounding.round_digits,\n                display_only=display_only,\n            )\n\n        return df\n\n    def scale_weights(self=None):\n        if self.df is None:\n            return None\n\n        if self.scale_wgts_to &gt; 0:\n            if self.weight != \"\":\n                self.df = self.df.with_columns(\n                    (\n                        nw.col(self.weight) / nw.sum(self.weight) * self.scale_wgts_to\n                    ).alias(self.weight)\n                )\n\n            if self.replicates is not None:\n                for weighti in self.replicates.rep_list:\n                    self.df = self.df.with_columns(\n                        (nw.col(weighti) / nw.sum(weighti) * self.scale_wgts_to).alias(\n                            weighti\n                        )\n                    )\n\n    def print(\n        self,\n        df: IntoFrameT | None = None,\n        round_output: bool | int | None = None,\n        estimates_per_page: int = 0,\n        sub_log: logging = None,\n    ):\n        \"\"\"\n        Print the estimates (with SEs if applicable) to the log.\n\n        Parameters\n        ----------\n        df : IntoFrameT, optional\n            The estimates to display. Default is the estimates in self.\n        round_output : bool|int|None, optional\n            Rounding rule (True for DRB, integer for number of significant digits).\n            Default is self's rounding rule.\n        estimates_per_page : int, optional\n            Repeat the header every k estimates. Defaults to 0 (don't repeat).\n        sub_log : logging, optional\n            Override logger. Default is None (no override).\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if self.df_replicates is not None:\n            self._print_replicates(\n                round_output=round_output,\n                estimates_per_page=estimates_per_page,\n                sub_log=sub_log,\n            )\n        else:\n            self._print_estimates(\n                df=df,\n                round_output=round_output,\n                estimates_per_page=estimates_per_page,\n                sub_log=sub_log,\n            )\n\n    def _print_estimates(\n        self,\n        df: IntoFrameT | None = None,\n        round_output: bool | int | None = None,\n        estimates_per_page: int = 0,\n        sub_log: logging = None,\n    ):\n        \"\"\"\n        Prints the estimates (when there are no SEs) to the log\n\n        Parameters\n        ----------\n        df : IntoFrameT, optional\n            The estimates to show The default is the estimates in self.\n        round_output : bool|int|None, optional\n            Rounding rule (True for DRB, integer for number of significant digits). The default is self's rounding rule.\n        estimates_per_page : int, optional\n            Repeat the header every k estimates.  Defaults to 0 (don't)\n        sub_log : logging , optional\n            Override logger?  Default is None (no override)\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        if df is None:\n            df = self.df_estimates\n\n        nw_type = NarwhalsType(df)\n        df = nw_type.to_polars().lazy().collect()\n        if sub_log is None:\n            sub_log = logger\n\n        #   f_print = print\n        f_print = sub_log.info\n        #   Round?\n        if round_output:\n            rounding = deepcopy(self.rounding)\n            rounding.set_round_digits(round_output)\n\n            df = self.round_results(df=df, rounding=rounding, display_only=True)\n\n        if self.display_all_vars:\n            n_rows = df.height\n        else:\n            n_rows = min(self.display_max_vars, df.height)\n\n        with pl.Config(fmt_str_lengths=50) as cfg:\n            #   Basic formatting\n            cfg.set_tbl_cell_alignment(\"RIGHT\")\n            cfg.set_tbl_hide_column_data_types(True)\n            cfg.set_tbl_hide_dataframe_shape(True)\n            cfg.set_thousands_separator(True)\n            cfg.set_tbl_width_chars(600)\n            cfg.set_tbl_cols(len(df.lazy().collect_schema()))\n\n            cfg.set_tbl_rows(n_rows)\n\n            if estimates_per_page &gt; 0 and n_rows &gt; estimates_per_page:\n                slices = math.ceil(n_rows / estimates_per_page)\n\n                for slicei in range(slices):\n                    f_print(\n                        df.slice(\n                            offset=estimates_per_page * slicei,\n                            length=estimates_per_page,\n                        )\n                    )\n            else:\n                f_print(df)\n\n    def _print_replicates(\n        self,\n        round_output: bool | int | None = None,\n        estimates_per_page: int = 0,\n        sub_log: logging = None,\n    ):\n        \"\"\"\n        Prints the estimates (when there are SEs) to the log\n\n        Parameters\n        ----------\n        round_output : bool|int|None, optional\n            Rounding rule (True for DRB, integer for number of significant digits). The default is self's rounding rule.\n        estimates_per_page : int, optional\n            Repeat the header every k estimates.  Defaults to 0 (don't)\n        sub_log : logging , optional\n            Override logger?  Default is None (no override)\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        if sub_log is None:\n            sub_log = logger\n\n        #   Round?\n        if round_output:\n            rounding = deepcopy(self.rounding)\n            rounding.set_round_digits(round_output)\n\n            df_estimates = self.round_results(\n                df=self.df_estimates, rounding=rounding, display_only=True\n            )\n            df_ses = self.round_results(\n                df=self.df_ses, rounding=rounding, display_only=True\n            )\n        else:\n            df_estimates = self.df_estimates\n            df_ses = self.df_ses\n\n        print_se_table(\n            df_estimates=df_estimates,\n            df_ses=df_ses,\n            display_all_vars=self.display_all_vars,\n            display_max_vars=self.display_max_vars,\n            sort_vars=self.variable_ids + self.summarize_vars,\n            round_output=False,\n            sub_log=sub_log,\n        )\n\n    def table_of_estimates(\n        self,\n        round_output: bool | int | None = None,\n        estimates_to_show: list[str] | None = None,\n        variable_prefix: str = \"\",\n        estimate_type_variable_name: str = \"Statistic\",\n        ci_level: float = 0.95,\n    ) -&gt; IntoFrameT:\n        \"\"\"\n        Create a formatted table of estimates with option of statistics to report.\n\n        Parameters\n        ----------\n        round_output : bool|int|None, optional\n            Rounding rule for display.\n        estimates_to_show : list[str] | None, optional\n            List of estimate types to include. Options: \"estimate\", \"se\", \"t\", \"p\", \"ci\".\n            Default is [\"estimate\", \"se\"].\n        variable_prefix : str, optional\n            Prefix to add to variable column names. Default is \"\".\n        estimate_type_variable_name : str, optional\n            Name for the column indicating statistic type. Default is \"Statistic\".\n        ci_level : float, optional\n            Confidence interval level for \"ci\" estimates. Default is 0.95.\n\n        Returns\n        -------\n        IntoFrameT\n            Formatted table with estimates arranged by statistic type.\n        \"\"\"\n        if estimates_to_show is None:\n            estimates_to_show = [\"estimate\", \"se\"]\n\n        df_ordered = []\n        nw_ordered = []\n        col_sort = \"__order_output_table__\"\n        for index, esti in enumerate(estimates_to_show):\n            dfi = None\n            if esti.lower() == \"estimate\":\n                dfi = self.df_estimates\n\n            elif esti.lower() == \"se\" and self.df_replicates is not None:\n                dfi = self.df_ses\n            elif esti.lower() == \"t\" and self.df_replicates is not None:\n                dfi = self._df_t()\n            elif esti.lower() == \"p\" and self.df_replicates is not None:\n                dfi = self._df_p()\n            elif esti.lower() == \"ci\" and self.df_replicates is not None:\n                dfi = self._df_ci(ci_level=ci_level)\n            else:\n                message = f\"{esti} not allowed for estimates_to_show\"\n                logger.error(message)\n                raise Exception(message)\n\n            if dfi is not None:\n                nwi = NarwhalsType(dfi)\n                nw_ordered.append(nwi)\n                dfi = nwi.to_polars()\n                df_ordered.append(\n                    dfi.with_columns(\n                        [\n                            pl.lit(index).alias(col_sort),\n                            pl.lit(esti.lower()).alias(estimate_type_variable_name),\n                        ]\n                    )\n                )\n\n        col_row_index = \"___estimate_row_count___\"\n        df_ordered[0] = df_ordered[0].with_row_index(col_row_index)\n        df_display = concat_wrapper(df_ordered, how=\"diagonal\").lazy()\n\n        sort_vars = self.variable_ids + self.summarize_vars\n        df_display = df_display.sort(sort_vars + [col_sort]).with_columns(\n            pl.col(col_row_index).forward_fill()\n        )\n        df_display = df_display.sort([col_row_index] + [col_sort]).drop(col_row_index)\n\n        #   Clear extraneous information\n        with_clear = []\n        for coli in sort_vars:\n            c_col = pl.col(coli)\n            with_clear.append(\n                pl.when(pl.col(col_sort) != 0)\n                .then(pl.lit(\"\"))\n                .otherwise(c_col.cast(pl.String))\n                .alias(coli)\n            )\n\n        select_order = sort_vars + [estimate_type_variable_name]\n        remaining = []\n        rename = {}\n        for coli in df_display.lazy.collect_schema().names():\n            if coli not in select_order and coli != col_sort:\n                if variable_prefix != \"\":\n                    rename[coli] = f\"{variable_prefix}{coli}\"\n\n                remaining.append(coli)\n\n        df_display = df_display.with_columns(with_clear).select(\n            select_order + remaining\n        )\n        #   Round?\n        if round_output:\n            rounding = deepcopy(self.rounding)\n            rounding.set_round_digits(round_output)\n\n            if (\n                len(rounding.cols_n) == 0\n                and len(rounding.cols_round) == 0\n                and not rounding.round_all\n            ):\n                #   Nothing set to round, round all\n                rounding.cols_round = remaining\n\n            df_display = self.round_results(\n                df=df_display, rounding=rounding, display_only=True\n            )\n\n        if len(rename):\n            df_display = df_display.rename(rename)\n        return nw_ordered[0].lazy().from_polars(df_display)\n\n    def join_tables_of_estimates(\n        self, df_list: list[IntoFrameT], estimate_type_variable_name: str = \"Statistic\"\n    ) -&gt; IntoFrameT:\n        df_list = [nw.from_native(dfi) for dfi in df_list]\n        sort_vars = (\n            self.variable_ids + self.summarize_vars + [estimate_type_variable_name]\n        )\n\n        variable_filled = []\n        for coli in self.variable_ids:\n            if df_list[0].schema[coli] == nw.String:\n                c_missing = nw.col(coli).is_null() | (pl.col(coli) == \"\")\n            else:\n                c_missing = nw.col(coli).is_missing()\n\n            variable_filled.append(\n                (\n                    nw.when(c_missing)\n                    .then(nw.lit(None))\n                    .otherwise(nw.col(coli))\n                    .alias(coli)\n                    .fill_null(strategy=\"forward\")\n                )\n            )\n\n        row_indices = []\n        for i in range(0, len(df_list)):\n            row_indices.append(f\"__row_index_{i}\")\n            df_list[i] = (\n                df_list[i]\n                .with_row_index(f\"__row_index_{i}\")\n                .with_columns(variable_filled)\n            )\n\n        df_out = join_list(df_list, on=sort_vars, how=\"full\").sort(row_indices)\n\n        with_clear = []\n        c_index = nw.col(\"__row_index_0\")\n        for coli in self.variable_ids:\n            c_col = nw.col(coli)\n\n            with_clear.append(\n                nw.when(c_index == nw.min(\"__row_index_0\").over(self.variable_ids))\n                .then(c_col)\n                .otherwise(nw.lit(\"\"))\n                .alias(coli)\n            )\n\n        df_out = df_out.with_columns(with_clear).news.drop_if_exists([\"__row_index_*\"])\n        return df_out.to_native()\n\n    def _df_t(self) -&gt; IntoFrameT:\n        join_on = self.variable_ids + self.summarize_vars\n        nw_type = NarwhalsType(self.df_estimates)\n        cols_stats = (\n            nw.from_native(self.df_estimates)\n            .lazy()\n            .drop(join_on)\n            .collect_schema()\n            .names()\n        )\n\n        with_t = []\n        for coli in cols_stats:\n            c_est = nw.col(coli)\n            c_se = nw.col(f\"{coli}_se\")\n\n            with_t.append((c_est / c_se).abs().alias(coli))\n\n        df_out = join_list(\n            [self.df_estimates, self.df_ses],\n            how=\"left\",\n            on=join_on,\n            suffixes=[\"\", \"_se\"],\n        ).select(join_on + with_t)\n        return NarwhalsType.return_df(df_out, nw_type)\n\n    def _df_p(self) -&gt; pl.DataFrame | pl.LazyFrame:\n        nw_p = NarwhalsType(self._df_t())\n        df_p = nw_p.to_polars()\n\n        join_on = self.variable_ids + self.summarize_vars\n        cols_stats = self.df_estimates.drop(join_on).columns\n\n        def p_value(t):\n            if t == float(\"inf\") or t == float(\"nan\"):\n                return 0\n            df = 1_000_000\n\n            return scipy.stats.t.sf(t, df) * 2\n\n        def p_value_lambda(t, col_t):\n            try:\n                return p_value(t[col_t])\n            except:\n                return None\n\n        for coli in cols_stats:\n            index_t = df_p.columns.index(f\"{coli}\")\n\n            df_p = (\n                df_p.with_columns(df_p.map_rows(lambda t: p_value_lambda(t, index_t)))\n                .drop(coli)\n                .rename({\"map\": coli})\n            )\n\n        return nw_p.from_polars(df_p.select(join_on + cols_stats))\n\n    def _df_ci(self, ci_level: float = 0.95):\n        return self.replicate_stats._df_ci(\n            ci_level=ci_level, join_on=self.variable_ids + self.summarize_vars\n        )\n\n    def compare(\n        self,\n        compare_to,\n        difference: bool = True,\n        ratio: bool = True,\n        display: bool = True,\n        ratio_minus_1: bool = True,\n        compare_list_variables: list[ComparisonItem.Variable] | None = None,\n        compare_list_columns: list[ComparisonItem.Column] | None = None,\n        quietly: bool = False,\n    ):\n        \"\"\"\n        Compare this set of estimates to another set of estimates,\n        including MultipleImputation estimates.\n\n        Parameters\n        ----------\n        compare_to : StatCalculator | MultipleImputation\n            The other object to compare to.\n        difference : bool, optional\n            Calculate and return the difference (with key \"difference\"). Default is True.\n        ratio : bool, optional\n            Calculate and return the ratio (with key \"ratio\"). Default is True.\n        ratio_minus_1 : bool, optional\n            Rescale ratio by subtracting 1 from it. Default is True.\n        display : bool, optional\n            Print the difference/ratio to the log. Default is True.\n        compare_list_variables : list[ComparisonItem.Variable] | None, optional\n            List of variables to compare (i.e. compare rows from prior calculations)\n        compare_list_columns : list[ComparisonItem.Column], optional\n            List of columns to compare\n            For example if compare_list_variables = [ComparisonItem.Column(\"mean\",\"median\")]\n                then compare the mean of 1 to the median of 2\n        quietly : bool, optional\n            Suppress informational messages. Default is False.\n\n        Returns\n        -------\n        dict[str, StatCalculator]\n            Dictionary with keys [\"difference\",\"ratio\"] containing\n            StatCalculator objects with the comparison estimates\n            (with SEs if applicable).\n        \"\"\"\n\n        outputs = {}\n        if statistical_comparison_item(self) and statistical_comparison_item(\n            compare_to\n        ):\n            if not quietly:\n                if self.bootstrap:\n                    logger.info(\"Comparing estimates using bootstrap weights\")\n                else:\n                    logger.info(\"Comparing estimates using replicate weights\")\n\n            outputs = compare(\n                stats1=self,\n                stats2=compare_to,\n                join_on=self.variable_ids + self.summarize_vars,\n                rounding=self.rounding,\n                difference=difference,\n                ratio=ratio,\n                ratio_minus_1=ratio_minus_1,\n                compare_list_variables=compare_list_variables,\n                compare_list_columns=compare_list_columns,\n            )\n\n            if display:\n                if difference:\n                    logger.info(\"  Difference\")\n                    outputs[\"difference\"].print(round_output=self.round_output)\n                    logger.info(\"\\n\")\n                if ratio:\n                    logger.info(\"  Ratio\")\n                    outputs[\"ratio\"].print(round_output=self.round_output)\n                    logger.info(\"\\n\")\n\n        else:\n            if not quietly:\n                logger.info(\"Comparing estimates\")\n\n            df1 = self.df_estimates\n            df2 = compare_to.df_estimates\n\n            (df1, df2) = StatComp.process_compare_lists(\n                df1=df1,\n                df2=df2,\n                join_on=self._by_vars() + self.variable_ids,\n                compare_list_variables=compare_list_variables,\n                compare_list_columns=compare_list_columns,\n            )\n\n            sm_compare = StatCalculator(\n                df=None, statistics=self.statistics, by=self.by, calculate=False\n            )\n\n            cols_index = self.variable_ids + self.summarize_vars\n            cols_nonindex = df1.drop(cols_index).columns\n\n            df1 = SafeCollect(df1)\n            df2 = SafeCollect(df2)\n\n            #   logger.info(df1.schema)\n            #   Upcast any columns that need to be\n            [df1, df2] = _polars_safe_upcast(\n                df1.with_columns(pl.col(pl.Boolean).cast(pl.Int8)),\n                df2.with_columns(pl.col(pl.Boolean).cast(pl.Int8)),\n                cols1=cols_nonindex,\n                cols2=cols_nonindex,\n            )\n\n            df_difference = SafeCollect(df2.select(cols_nonindex)) - df1.select(\n                cols_nonindex\n            )\n            df_ratio = (df_difference) / df1.select(cols_nonindex)\n\n            if difference:\n                sm_diff = sm_compare\n\n                if ratio:\n                    sm_compare = deepcopy(sm_diff)\n\n                sm_diff.df_estimates = pl.concat(\n                    [df1.select(cols_index), df_difference], how=\"horizontal\"\n                )\n\n                outputs[\"difference\"] = sm_diff\n\n                if display:\n                    logger.info(\"  Difference\")\n                    sm_diff.print(round_output=sm_diff.round_output)\n                    logger.info(\"\\n\")\n\n            if ratio:\n                sm_ratio = sm_compare\n\n                sm_ratio.df_estimates = pl.concat(\n                    [df1.select(cols_index), df_ratio], how=\"horizontal\"\n                )\n\n                outputs[\"ratio\"] = sm_ratio\n\n                if display:\n                    logger.info(\"  Ratio\")\n                    sm_ratio.print(round_output=sm_ratio.round_output)\n                    logger.info(\"\\n\")\n\n        return outputs\n\n    def from_function(delegate:Callable,\n                      estimate_ids:list | str,\n                      df:IntoFrameT | None=None,\n                      df_argument:str=\"df\",\n                      arguments:dict|None=None,\n                      weight:str=\"\",\n                      replicates:Replicates|None=None,\n                      scale_wgts_to:float=0.0,\n                      weight_argument_name:str=\"weight\",\n                      by:dict[str,list[str]]|None=None,\n                      display:bool=True,\n                      display_all_vars:bool=True,\n                      display_max_vars:int=20,\n                      round_output:bool|int=True) -&gt; StatCalculator:\n        \"\"\"\n        Create a StatCalculator from a custom function that returns estimates.\n\n        This static method allows wrapping any function that returns estimates\n        in a StatCalculator object for easy display and comparison.\n\n        Parameters\n        ----------\n        delegate : callable\n            Function that returns a table of estimates. Must accept weight\n            parameter if replicates are used.\n        estimate_ids : list | str\n            Column names that identify each unique estimate.\n        df : pl.LazyFrame | pl.DataFrame, optional\n            Dataframe passed as \"df\" argument to delegate. Allows dynamic\n            subsetting with by. Default is None.\n        df_argument : str, optional\n            Name of argument with data. Defaults is \"df\".\n        arguments : dict, optional\n            Static arguments (other than weight) passed to delegate. Default is None.\n        weight : str, optional\n            Weight column name for weighted statistics. Default is \"\".\n        replicates : Replicates|None, optional\n            Replicates object for replicate weight standard errors. Default is None.\n        scale_wgts_to : float, optional\n            Scale weights to sum to this value. Default is 0.0 (no scaling).\n        weight_argument_name : str, optional\n            Keyword argument name for passing weight to delegate. Default is \"weight\".\n        by : dict[str,list[str]]|None, optional\n            Dictionary defining grouping variables for summary statistics.\n        display : bool, optional\n            Print results to log. Default is True.\n        display_all_vars : bool, optional\n            Print all variables rather than truncated summary. Default is True.\n        display_max_vars : int, optional\n            Maximum variables to print if display_all_vars=False. Default is 20.\n        round_output : bool|int, optional\n            Round the output. Default is True.\n\n        Returns\n        -------\n        StatCalculator\n            StatCalculator object containing the function results with\n            estimates, SEs, and replicates as applicable.\n        \"\"\"\n\n        #   Input parsing\n        if arguments is None:\n            arguments = {}\n\n        if type(estimate_ids) is str:\n            estimate_ids = [estimate_ids]\n\n        if df is None:\n            by = None\n        else:\n            if scale_wgts_to &gt; 0:\n                if weight != \"\":\n                    weights_to_cast = [weight]\n                    if replicates is not None:\n                        weights_to_cast.extend(replicates.rep_list)\n                    df = safe_sum_cast(df,\n                                       weights_to_cast)\n\n                    with_scale = [(nw.col(weighti)/nw.col(weighti).sum()*scale_wgts_to).alias(nw.col(weighti)) for weighti in weights_to_cast]\n                    df = (\n                        nw.from_native(df)\n                        .with_columns(with_scale)\n                        .to_native()\n                    )\n\n        if by is None:\n            by = {\"All\":[]}\n\n        replicate_name = \"___replicate___\"\n\n        df_estimates = []\n        df_ses = []\n        df_replicates = []\n        by_vars = StatCalculator._by_vars(by=by)\n\n        nw_type = NarwhalsType(df)\n        for keyi, valuei in by.items():\n            if keyi == \"All\":\n                logger.info(f\"Running {delegate.__name__}\")\n            else:\n                logger.info(f\"Running {delegate.__name__} for {keyi}\")\n\n            df_list = []\n            if df is not None:\n                if len(valuei):\n                    df_partitioned = (\n                        nw_type.to_polars()\n                        .lazy()\n                        .collect()\n                        .partition_by(\n                            by=valuei,\n                            maintain_order=True,\n                            include_key=True\n                        )\n                    )\n\n                    df_partitioned = [nw_type.from_polars(dfi) for dfi in df_partitioned]\n\n                    df_list.extend(df_partitioned)\n                else:\n                    df_list.append(df)\n\n            if len(df_list) == 0:\n                df_list = [None]\n\n            for dfi in df_list:\n                append_by = []\n                append_values = []\n\n                if dfi is not None:\n                    arguments[df_argument] = dfi\n\n                    if len(valuei):\n                        append_values = dfi.select(valuei).unique().to_dicts()\n                        append_by = [nw.lit(valuei).alias(keyi) for keyi, valuei in append_values[0].items()]\n\n                if replicates is None:\n                    df_esti = delegate(**arguments)\n                    if len(append_by):\n                        df_esti = (\n                            nw.from_native(df_esti)\n                            .with_columns(append_by)\n                            .to_native()\n                        )\n\n                    df_estimates.append(df_esti)\n                else:\n                    if len(append_values):\n                        logger.info(append_values)\n\n                    rep_return = replicates_ses_from_function(delegate=delegate,\n                                                             arguments=arguments,\n                                                             join_on=estimate_ids,\n                                                             weight_argument_name=weight_argument_name,\n                                                             weights=replicates.rep_list,\n                                                             replicate_name=replicate_name)\n\n                    df_esti = rep_return.df_estimates\n                    df_sei = rep_return.df_ses\n                    df_repi = rep_return.df_replicates\n\n                    if len(append_by):\n                        df_esti = (\n                            nw.from_native(df_esti)\n                            .with_columns(append_by)\n                            .to_native()\n                        )\n                        df_sei = (\n                            nw.from_native(df_sei)\n                            .with_columns(append_by)\n                            .to_native()\n                        )\n\n                        df_repi = (\n                            nw.from_native(df_repi)\n                            .with_columns(append_by)\n                            .to_native()\n                        )\n\n                    df_estimates.append(df_esti)\n                    df_ses.append(df_sei)\n                    df_replicates.append(df_repi)\n\n            del df_list\n\n        #   Set up the output\n        ss_out = StatCalculator(df=None,\n                                weight=weight,\n                                replicates=replicates,\n                                by=by,\n                                display=display,\n                                display_all_vars=display_all_vars,\n                                display_max_vars=display_max_vars,\n                                round_output=round_output,\n                                calculate=False)\n\n        ss_out.variable_ids = estimate_ids\n\n        if len(df_estimates):\n            df_estimates = concat_wrapper(df_estimates,\n                                          how=\"diagonal\")\n            #   Final variable order\n            if len(by_vars):\n                select_order = estimate_ids + by_vars\n                select_order.extend([coli for coli in safe_columns(df_estimates) if coli not in select_order])\n            else:\n                select_order = safe_columns(df_estimates)\n            ss_out.df_estimates = df_estimates.select(select_order)\n        if len(df_ses):\n            ss_out.df_ses = concat_wrapper(df_ses,\n                                       how=\"diagonal\").select(select_order)\n\n        if len(df_ses):\n            ss_out.df_replicates = concat_wrapper(df_replicates,\n                                                  how=\"diagonal\").select(select_order + [replicate_name])\n\n        ss_out.df_estimates = ss_out.round_results(df=ss_out.df_estimates)\n        ss_out.df_ses = ss_out.round_results(df=ss_out.df_ses)\n\n        if display:\n            ss_out.print()\n\n        return ss_out\n\n    def filter(self, filter_expr: nw.Expr) -&gt; StatCalculator:\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.filter(filter_expr)\n\n        return self\n\n    def select(\n        self, select_expr: nw.Expr | str | list[str] | list[nw.Expr]\n    ) -&gt; StatCalculator:\n        cols_keep = (\n            nw.from_native(self.df_estimates)\n            .lazy()\n            .select(select_expr)\n            .collect_schema()\n            .names()\n        )\n        add_join_on = list(set(self.variable_ids).difference(cols_keep))\n        cols_keep = add_join_on + cols_keep\n\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.select(cols_keep)\n\n        return self\n\n    def with_columns(self, with_expr: nw.Expr | list[nw.Expr]) -&gt; StatCalculator:\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.with_columns(with_expr)\n\n        return self\n\n    def sort(\n        self, sort_expr: nw.Expr | list[nw.Expr] | str | list[str]\n    ) -&gt; StatCalculator:\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.sort(sort_expr)\n\n        return self\n\n    def drop(\n        self, drop_expr: nw.Expr | list[nw.Expr] | str | list[str]\n    ) -&gt; ReplicateStats:\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.drop(drop_expr)\n\n        return self\n\n    def rename(self, d_rename: dict[str, str]) -&gt; StatCalculator:\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.rename(d_rename)\n\n        return self\n\n    def scale_by(\n        self, factor: float, columns: list[str] | str | None = None\n    ) -&gt; StatCalculator:\n        if columns is None:\n            #   Any columns that aren't the join_on ones\n            columns = (\n                nw.from_native(self.df_estimates.columns)\n                .lazy()\n                .collect_schema()\n                .names()\n            )\n            columns = list(set(columns).difference(self.join_on))\n\n        return self.with_columns(with_expr=nw.col(columns) * factor)\n\n    def pipe(self, function: Callable, *args, **kwargs) -&gt; StatCalculator:\n        \"\"\"\n        Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)\n\n        Parameters\n        ----------\n        function : Callable\n            Function to pipe.\n        *args : TYPE\n            arguments to function\n        **kwargs : TYPE\n            keyword arguments to function\n\n        Returns\n        -------\n        StatCalculator\n\n        \"\"\"\n\n        self = self.copy()\n        self.replicate_stats = self.replicate_stats.pipe(\n            function=function, *args, **kwargs\n        )\n\n        return self\n\n    # def reshape_groups_wide_long(self,\n    #                              copy:bool=False,\n    #                              group_first:bool=True,\n    #                              group_col:str=\"Group\",\n    #                              invert_group:bool=False) -&gt; StatCalculator:\n    #     if copy:\n    #         self = self.copy()\n\n    #     def _reshape(df:pl.DataFrame | pl.LazyFrame,\n    #                  join_on:list[str]) -&gt; pl.DataFrame | pl.LazyFrame:\n    #         if \"___replicate___\" in df.columns :\n    #             join_on = join_on + [\"___replicate___\"]\n\n    #         df_concat = []\n    #         for coli in df.columns:\n    #             if coli not in join_on:\n    #                 coli_group = coli.split(\":\")[0]\n    #                 coli_value = coli.split(\":\")[1]\n    #                 c_name = pl.col(join_on[0])\n    #                 rename = {coli:coli_value}\n\n    #                 if group_first:\n    #                     with_name = pl.concat_str([pl.lit(coli_group),\n    #                                                pl.lit(\":\"),\n    #                                                c_name]).alias(c_name.meta.output_name())\n    #                 else:\n    #                     with_name = pl.concat_str([c_name,\n    #                                                pl.lit(\":\"),\n    #                                                pl.lit(coli_group)]).alias(c_name.meta.output_name())\n\n    #                 with_name = with_name.alias(c_name.meta.output_name())\n    #                 if group_col != \"\":\n    #                     if invert_group:\n    #                         with_name = [with_name,\n    #                                      coli_value.alias(group_col)]\n    #                     else:\n    #                         with_name = [with_name,\n    #                                      pl.lit(coli_group).alias(group_col)]\n\n    #                 df_concat.append((df.select(join_on + [coli])\n    #                                     .rename(rename)\n    #                                     .with_columns(with_name)))\n\n    #         return pl.concat(df_concat,\n    #                          how=\"vertical_relaxed\")\n\n    #     self = self.pipe(_reshape,\n    #                      join_on=self.variable_ids)\n\n    #     return self\n    def concat_with(\n        self, sc_concat: StatCalculator, how: str = \"horizontal\"\n    ) -&gt; StatCalculator:\n        \"\"\"\n        Concatenate this with another StatCalculator object\n\n        Parameters\n        ----------\n        sc_concat : StatCalculator\n            Other mi object to concatenate with.\n        how : str, optional\n            horizontal or vertical?\n            Horizontal will actually do a join and vertical will just stack them\n            The default is \"horizontal\".\n\n        Returns\n        -------\n        StatCalculator\n\n        \"\"\"\n\n        self = self.copy()\n\n        self.replicate_stats.concat_with(\n            rs_concat=sc_concat.replicate_stats,\n            join_on_self=self.variable_ids,\n            join_on_concat=sc_concat.variable_ids,\n        )\n\n        return self\n\n    def drb_round_table(\n        self,\n        columns: list | str | None = None,\n        columns_n: list | str | None = None,\n        columns_exclude: list | str | None = None,\n        round_all: bool = True,\n        digits: int = 4,\n        compress: bool = False,\n    ) -&gt; StatCalculator:\n        \"\"\"\n        Apply DRB (Disclosure Review Board) rounding rules to the estimates.\n\n        Parameters\n        ----------\n        columns : list|str|None, optional\n            Specific columns to round. Default is None.\n        columns_n : list|str|None, optional\n            Columns to treat as counts for rounding. Default is None.\n        columns_exclude : list|str|None, optional\n            Columns to exclude from rounding. Default is None.\n        round_all : bool, optional\n            Apply rounding to all numeric columns. Default is True.\n        digits : int, optional\n            Number of significant digits for rounding. Default is 4.\n        compress : bool, optional\n            Use compressed rounding format. Default is False.\n\n        Returns\n        -------\n        StatCalculator\n            StatCalculator with DRB rounding applied.\n        \"\"\"\n        kwargs = copy(locals())\n        del kwargs[\"self\"]\n        self.replicate_stats = self.replicate_stats.pipe(\n            function=drb_round_table, **kwargs\n        )\n\n        return self\n\n    #####################################################\n    #   Serializable - BEGIN\n    #####################################################\n    @classmethod\n    def _init_from_dict(cls, data: dict):\n        return super()._init_from_dict(data, calculate=False)\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.df_estimates","title":"df_estimates  <code>property</code> <code>writable</code>","text":"<pre><code>df_estimates\n</code></pre> <p>IntoFrameT : Main estimates dataframe containing calculated statistics.</p> <p>This property provides access to the primary results table with all calculated statistics. Includes variable identifiers, grouping variables, and statistical estimates as columns.</p>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.df_replicates","title":"df_replicates  <code>property</code> <code>writable</code>","text":"<pre><code>df_replicates\n</code></pre> <p>IntoFrameT : Full replicate estimates dataframe.</p> <p>Contains individual estimates for each replicate weight, allowing for custom variance calculations or additional analysis. Includes all columns from df_estimates plus a replicate identifier column. Only populated when replicates parameter is provided.</p>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.df_ses","title":"df_ses  <code>property</code> <code>writable</code>","text":"<pre><code>df_ses\n</code></pre> <p>IntoFrameT : Standard errors dataframe (when replicate weights are used).</p> <p>Contains standard error estimates for all statistics calculated using replicate weight methods. Has the same structure as df_estimates but with standard errors instead of point estimates. Only populated when replicates parameter is provided.</p>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.compare","title":"compare","text":"<pre><code>compare(\n    compare_to,\n    difference: bool = True,\n    ratio: bool = True,\n    display: bool = True,\n    ratio_minus_1: bool = True,\n    compare_list_variables: list[Variable] | None = None,\n    compare_list_columns: list[Column] | None = None,\n    quietly: bool = False,\n)\n</code></pre> <p>Compare this set of estimates to another set of estimates, including MultipleImputation estimates.</p> <p>Parameters:</p> Name Type Description Default <code>compare_to</code> <code>StatCalculator | MultipleImputation</code> <p>The other object to compare to.</p> required <code>difference</code> <code>bool</code> <p>Calculate and return the difference (with key \"difference\"). Default is True.</p> <code>True</code> <code>ratio</code> <code>bool</code> <p>Calculate and return the ratio (with key \"ratio\"). Default is True.</p> <code>True</code> <code>ratio_minus_1</code> <code>bool</code> <p>Rescale ratio by subtracting 1 from it. Default is True.</p> <code>True</code> <code>display</code> <code>bool</code> <p>Print the difference/ratio to the log. Default is True.</p> <code>True</code> <code>compare_list_variables</code> <code>list[Variable] | None</code> <p>List of variables to compare (i.e. compare rows from prior calculations)</p> <code>None</code> <code>compare_list_columns</code> <code>list[Column]</code> <p>List of columns to compare For example if compare_list_variables = [ComparisonItem.Column(\"mean\",\"median\")]     then compare the mean of 1 to the median of 2</p> <code>None</code> <code>quietly</code> <code>bool</code> <p>Suppress informational messages. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, StatCalculator]</code> <p>Dictionary with keys [\"difference\",\"ratio\"] containing StatCalculator objects with the comparison estimates (with SEs if applicable).</p> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def compare(\n    self,\n    compare_to,\n    difference: bool = True,\n    ratio: bool = True,\n    display: bool = True,\n    ratio_minus_1: bool = True,\n    compare_list_variables: list[ComparisonItem.Variable] | None = None,\n    compare_list_columns: list[ComparisonItem.Column] | None = None,\n    quietly: bool = False,\n):\n    \"\"\"\n    Compare this set of estimates to another set of estimates,\n    including MultipleImputation estimates.\n\n    Parameters\n    ----------\n    compare_to : StatCalculator | MultipleImputation\n        The other object to compare to.\n    difference : bool, optional\n        Calculate and return the difference (with key \"difference\"). Default is True.\n    ratio : bool, optional\n        Calculate and return the ratio (with key \"ratio\"). Default is True.\n    ratio_minus_1 : bool, optional\n        Rescale ratio by subtracting 1 from it. Default is True.\n    display : bool, optional\n        Print the difference/ratio to the log. Default is True.\n    compare_list_variables : list[ComparisonItem.Variable] | None, optional\n        List of variables to compare (i.e. compare rows from prior calculations)\n    compare_list_columns : list[ComparisonItem.Column], optional\n        List of columns to compare\n        For example if compare_list_variables = [ComparisonItem.Column(\"mean\",\"median\")]\n            then compare the mean of 1 to the median of 2\n    quietly : bool, optional\n        Suppress informational messages. Default is False.\n\n    Returns\n    -------\n    dict[str, StatCalculator]\n        Dictionary with keys [\"difference\",\"ratio\"] containing\n        StatCalculator objects with the comparison estimates\n        (with SEs if applicable).\n    \"\"\"\n\n    outputs = {}\n    if statistical_comparison_item(self) and statistical_comparison_item(\n        compare_to\n    ):\n        if not quietly:\n            if self.bootstrap:\n                logger.info(\"Comparing estimates using bootstrap weights\")\n            else:\n                logger.info(\"Comparing estimates using replicate weights\")\n\n        outputs = compare(\n            stats1=self,\n            stats2=compare_to,\n            join_on=self.variable_ids + self.summarize_vars,\n            rounding=self.rounding,\n            difference=difference,\n            ratio=ratio,\n            ratio_minus_1=ratio_minus_1,\n            compare_list_variables=compare_list_variables,\n            compare_list_columns=compare_list_columns,\n        )\n\n        if display:\n            if difference:\n                logger.info(\"  Difference\")\n                outputs[\"difference\"].print(round_output=self.round_output)\n                logger.info(\"\\n\")\n            if ratio:\n                logger.info(\"  Ratio\")\n                outputs[\"ratio\"].print(round_output=self.round_output)\n                logger.info(\"\\n\")\n\n    else:\n        if not quietly:\n            logger.info(\"Comparing estimates\")\n\n        df1 = self.df_estimates\n        df2 = compare_to.df_estimates\n\n        (df1, df2) = StatComp.process_compare_lists(\n            df1=df1,\n            df2=df2,\n            join_on=self._by_vars() + self.variable_ids,\n            compare_list_variables=compare_list_variables,\n            compare_list_columns=compare_list_columns,\n        )\n\n        sm_compare = StatCalculator(\n            df=None, statistics=self.statistics, by=self.by, calculate=False\n        )\n\n        cols_index = self.variable_ids + self.summarize_vars\n        cols_nonindex = df1.drop(cols_index).columns\n\n        df1 = SafeCollect(df1)\n        df2 = SafeCollect(df2)\n\n        #   logger.info(df1.schema)\n        #   Upcast any columns that need to be\n        [df1, df2] = _polars_safe_upcast(\n            df1.with_columns(pl.col(pl.Boolean).cast(pl.Int8)),\n            df2.with_columns(pl.col(pl.Boolean).cast(pl.Int8)),\n            cols1=cols_nonindex,\n            cols2=cols_nonindex,\n        )\n\n        df_difference = SafeCollect(df2.select(cols_nonindex)) - df1.select(\n            cols_nonindex\n        )\n        df_ratio = (df_difference) / df1.select(cols_nonindex)\n\n        if difference:\n            sm_diff = sm_compare\n\n            if ratio:\n                sm_compare = deepcopy(sm_diff)\n\n            sm_diff.df_estimates = pl.concat(\n                [df1.select(cols_index), df_difference], how=\"horizontal\"\n            )\n\n            outputs[\"difference\"] = sm_diff\n\n            if display:\n                logger.info(\"  Difference\")\n                sm_diff.print(round_output=sm_diff.round_output)\n                logger.info(\"\\n\")\n\n        if ratio:\n            sm_ratio = sm_compare\n\n            sm_ratio.df_estimates = pl.concat(\n                [df1.select(cols_index), df_ratio], how=\"horizontal\"\n            )\n\n            outputs[\"ratio\"] = sm_ratio\n\n            if display:\n                logger.info(\"  Ratio\")\n                sm_ratio.print(round_output=sm_ratio.round_output)\n                logger.info(\"\\n\")\n\n    return outputs\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.concat_with","title":"concat_with","text":"<pre><code>concat_with(\n    sc_concat: StatCalculator, how: str = \"horizontal\"\n) -&gt; StatCalculator\n</code></pre> <p>Concatenate this with another StatCalculator object</p> <p>Parameters:</p> Name Type Description Default <code>sc_concat</code> <code>StatCalculator</code> <p>Other mi object to concatenate with.</p> required <code>how</code> <code>str</code> <p>horizontal or vertical? Horizontal will actually do a join and vertical will just stack them The default is \"horizontal\".</p> <code>'horizontal'</code> <p>Returns:</p> Type Description <code>StatCalculator</code> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def concat_with(\n    self, sc_concat: StatCalculator, how: str = \"horizontal\"\n) -&gt; StatCalculator:\n    \"\"\"\n    Concatenate this with another StatCalculator object\n\n    Parameters\n    ----------\n    sc_concat : StatCalculator\n        Other mi object to concatenate with.\n    how : str, optional\n        horizontal or vertical?\n        Horizontal will actually do a join and vertical will just stack them\n        The default is \"horizontal\".\n\n    Returns\n    -------\n    StatCalculator\n\n    \"\"\"\n\n    self = self.copy()\n\n    self.replicate_stats.concat_with(\n        rs_concat=sc_concat.replicate_stats,\n        join_on_self=self.variable_ids,\n        join_on_concat=sc_concat.variable_ids,\n    )\n\n    return self\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.drb_round_table","title":"drb_round_table","text":"<pre><code>drb_round_table(\n    columns: list | str | None = None,\n    columns_n: list | str | None = None,\n    columns_exclude: list | str | None = None,\n    round_all: bool = True,\n    digits: int = 4,\n    compress: bool = False,\n) -&gt; StatCalculator\n</code></pre> <p>Apply DRB (Disclosure Review Board) rounding rules to the estimates.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list | str | None</code> <p>Specific columns to round. Default is None.</p> <code>None</code> <code>columns_n</code> <code>list | str | None</code> <p>Columns to treat as counts for rounding. Default is None.</p> <code>None</code> <code>columns_exclude</code> <code>list | str | None</code> <p>Columns to exclude from rounding. Default is None.</p> <code>None</code> <code>round_all</code> <code>bool</code> <p>Apply rounding to all numeric columns. Default is True.</p> <code>True</code> <code>digits</code> <code>int</code> <p>Number of significant digits for rounding. Default is 4.</p> <code>4</code> <code>compress</code> <code>bool</code> <p>Use compressed rounding format. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>StatCalculator</code> <p>StatCalculator with DRB rounding applied.</p> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def drb_round_table(\n    self,\n    columns: list | str | None = None,\n    columns_n: list | str | None = None,\n    columns_exclude: list | str | None = None,\n    round_all: bool = True,\n    digits: int = 4,\n    compress: bool = False,\n) -&gt; StatCalculator:\n    \"\"\"\n    Apply DRB (Disclosure Review Board) rounding rules to the estimates.\n\n    Parameters\n    ----------\n    columns : list|str|None, optional\n        Specific columns to round. Default is None.\n    columns_n : list|str|None, optional\n        Columns to treat as counts for rounding. Default is None.\n    columns_exclude : list|str|None, optional\n        Columns to exclude from rounding. Default is None.\n    round_all : bool, optional\n        Apply rounding to all numeric columns. Default is True.\n    digits : int, optional\n        Number of significant digits for rounding. Default is 4.\n    compress : bool, optional\n        Use compressed rounding format. Default is False.\n\n    Returns\n    -------\n    StatCalculator\n        StatCalculator with DRB rounding applied.\n    \"\"\"\n    kwargs = copy(locals())\n    del kwargs[\"self\"]\n    self.replicate_stats = self.replicate_stats.pipe(\n        function=drb_round_table, **kwargs\n    )\n\n    return self\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.from_function","title":"from_function","text":"<pre><code>from_function(\n    delegate: Callable,\n    estimate_ids: list | str,\n    df: IntoFrameT | None = None,\n    df_argument: str = \"df\",\n    arguments: dict | None = None,\n    weight: str = \"\",\n    replicates: Replicates | None = None,\n    scale_wgts_to: float = 0.0,\n    weight_argument_name: str = \"weight\",\n    by: dict[str, list[str]] | None = None,\n    display: bool = True,\n    display_all_vars: bool = True,\n    display_max_vars: int = 20,\n    round_output: bool | int = True,\n) -&gt; StatCalculator\n</code></pre> <p>Create a StatCalculator from a custom function that returns estimates.</p> <p>This static method allows wrapping any function that returns estimates in a StatCalculator object for easy display and comparison.</p> <p>Parameters:</p> Name Type Description Default <code>delegate</code> <code>callable</code> <p>Function that returns a table of estimates. Must accept weight parameter if replicates are used.</p> required <code>estimate_ids</code> <code>list | str</code> <p>Column names that identify each unique estimate.</p> required <code>df</code> <code>LazyFrame | DataFrame</code> <p>Dataframe passed as \"df\" argument to delegate. Allows dynamic subsetting with by. Default is None.</p> <code>None</code> <code>df_argument</code> <code>str</code> <p>Name of argument with data. Defaults is \"df\".</p> <code>'df'</code> <code>arguments</code> <code>dict</code> <p>Static arguments (other than weight) passed to delegate. Default is None.</p> <code>None</code> <code>weight</code> <code>str</code> <p>Weight column name for weighted statistics. Default is \"\".</p> <code>''</code> <code>replicates</code> <code>Replicates | None</code> <p>Replicates object for replicate weight standard errors. Default is None.</p> <code>None</code> <code>scale_wgts_to</code> <code>float</code> <p>Scale weights to sum to this value. Default is 0.0 (no scaling).</p> <code>0.0</code> <code>weight_argument_name</code> <code>str</code> <p>Keyword argument name for passing weight to delegate. Default is \"weight\".</p> <code>'weight'</code> <code>by</code> <code>dict[str, list[str]] | None</code> <p>Dictionary defining grouping variables for summary statistics.</p> <code>None</code> <code>display</code> <code>bool</code> <p>Print results to log. Default is True.</p> <code>True</code> <code>display_all_vars</code> <code>bool</code> <p>Print all variables rather than truncated summary. Default is True.</p> <code>True</code> <code>display_max_vars</code> <code>int</code> <p>Maximum variables to print if display_all_vars=False. Default is 20.</p> <code>20</code> <code>round_output</code> <code>bool | int</code> <p>Round the output. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>StatCalculator</code> <p>StatCalculator object containing the function results with estimates, SEs, and replicates as applicable.</p> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def from_function(delegate:Callable,\n                  estimate_ids:list | str,\n                  df:IntoFrameT | None=None,\n                  df_argument:str=\"df\",\n                  arguments:dict|None=None,\n                  weight:str=\"\",\n                  replicates:Replicates|None=None,\n                  scale_wgts_to:float=0.0,\n                  weight_argument_name:str=\"weight\",\n                  by:dict[str,list[str]]|None=None,\n                  display:bool=True,\n                  display_all_vars:bool=True,\n                  display_max_vars:int=20,\n                  round_output:bool|int=True) -&gt; StatCalculator:\n    \"\"\"\n    Create a StatCalculator from a custom function that returns estimates.\n\n    This static method allows wrapping any function that returns estimates\n    in a StatCalculator object for easy display and comparison.\n\n    Parameters\n    ----------\n    delegate : callable\n        Function that returns a table of estimates. Must accept weight\n        parameter if replicates are used.\n    estimate_ids : list | str\n        Column names that identify each unique estimate.\n    df : pl.LazyFrame | pl.DataFrame, optional\n        Dataframe passed as \"df\" argument to delegate. Allows dynamic\n        subsetting with by. Default is None.\n    df_argument : str, optional\n        Name of argument with data. Defaults is \"df\".\n    arguments : dict, optional\n        Static arguments (other than weight) passed to delegate. Default is None.\n    weight : str, optional\n        Weight column name for weighted statistics. Default is \"\".\n    replicates : Replicates|None, optional\n        Replicates object for replicate weight standard errors. Default is None.\n    scale_wgts_to : float, optional\n        Scale weights to sum to this value. Default is 0.0 (no scaling).\n    weight_argument_name : str, optional\n        Keyword argument name for passing weight to delegate. Default is \"weight\".\n    by : dict[str,list[str]]|None, optional\n        Dictionary defining grouping variables for summary statistics.\n    display : bool, optional\n        Print results to log. Default is True.\n    display_all_vars : bool, optional\n        Print all variables rather than truncated summary. Default is True.\n    display_max_vars : int, optional\n        Maximum variables to print if display_all_vars=False. Default is 20.\n    round_output : bool|int, optional\n        Round the output. Default is True.\n\n    Returns\n    -------\n    StatCalculator\n        StatCalculator object containing the function results with\n        estimates, SEs, and replicates as applicable.\n    \"\"\"\n\n    #   Input parsing\n    if arguments is None:\n        arguments = {}\n\n    if type(estimate_ids) is str:\n        estimate_ids = [estimate_ids]\n\n    if df is None:\n        by = None\n    else:\n        if scale_wgts_to &gt; 0:\n            if weight != \"\":\n                weights_to_cast = [weight]\n                if replicates is not None:\n                    weights_to_cast.extend(replicates.rep_list)\n                df = safe_sum_cast(df,\n                                   weights_to_cast)\n\n                with_scale = [(nw.col(weighti)/nw.col(weighti).sum()*scale_wgts_to).alias(nw.col(weighti)) for weighti in weights_to_cast]\n                df = (\n                    nw.from_native(df)\n                    .with_columns(with_scale)\n                    .to_native()\n                )\n\n    if by is None:\n        by = {\"All\":[]}\n\n    replicate_name = \"___replicate___\"\n\n    df_estimates = []\n    df_ses = []\n    df_replicates = []\n    by_vars = StatCalculator._by_vars(by=by)\n\n    nw_type = NarwhalsType(df)\n    for keyi, valuei in by.items():\n        if keyi == \"All\":\n            logger.info(f\"Running {delegate.__name__}\")\n        else:\n            logger.info(f\"Running {delegate.__name__} for {keyi}\")\n\n        df_list = []\n        if df is not None:\n            if len(valuei):\n                df_partitioned = (\n                    nw_type.to_polars()\n                    .lazy()\n                    .collect()\n                    .partition_by(\n                        by=valuei,\n                        maintain_order=True,\n                        include_key=True\n                    )\n                )\n\n                df_partitioned = [nw_type.from_polars(dfi) for dfi in df_partitioned]\n\n                df_list.extend(df_partitioned)\n            else:\n                df_list.append(df)\n\n        if len(df_list) == 0:\n            df_list = [None]\n\n        for dfi in df_list:\n            append_by = []\n            append_values = []\n\n            if dfi is not None:\n                arguments[df_argument] = dfi\n\n                if len(valuei):\n                    append_values = dfi.select(valuei).unique().to_dicts()\n                    append_by = [nw.lit(valuei).alias(keyi) for keyi, valuei in append_values[0].items()]\n\n            if replicates is None:\n                df_esti = delegate(**arguments)\n                if len(append_by):\n                    df_esti = (\n                        nw.from_native(df_esti)\n                        .with_columns(append_by)\n                        .to_native()\n                    )\n\n                df_estimates.append(df_esti)\n            else:\n                if len(append_values):\n                    logger.info(append_values)\n\n                rep_return = replicates_ses_from_function(delegate=delegate,\n                                                         arguments=arguments,\n                                                         join_on=estimate_ids,\n                                                         weight_argument_name=weight_argument_name,\n                                                         weights=replicates.rep_list,\n                                                         replicate_name=replicate_name)\n\n                df_esti = rep_return.df_estimates\n                df_sei = rep_return.df_ses\n                df_repi = rep_return.df_replicates\n\n                if len(append_by):\n                    df_esti = (\n                        nw.from_native(df_esti)\n                        .with_columns(append_by)\n                        .to_native()\n                    )\n                    df_sei = (\n                        nw.from_native(df_sei)\n                        .with_columns(append_by)\n                        .to_native()\n                    )\n\n                    df_repi = (\n                        nw.from_native(df_repi)\n                        .with_columns(append_by)\n                        .to_native()\n                    )\n\n                df_estimates.append(df_esti)\n                df_ses.append(df_sei)\n                df_replicates.append(df_repi)\n\n        del df_list\n\n    #   Set up the output\n    ss_out = StatCalculator(df=None,\n                            weight=weight,\n                            replicates=replicates,\n                            by=by,\n                            display=display,\n                            display_all_vars=display_all_vars,\n                            display_max_vars=display_max_vars,\n                            round_output=round_output,\n                            calculate=False)\n\n    ss_out.variable_ids = estimate_ids\n\n    if len(df_estimates):\n        df_estimates = concat_wrapper(df_estimates,\n                                      how=\"diagonal\")\n        #   Final variable order\n        if len(by_vars):\n            select_order = estimate_ids + by_vars\n            select_order.extend([coli for coli in safe_columns(df_estimates) if coli not in select_order])\n        else:\n            select_order = safe_columns(df_estimates)\n        ss_out.df_estimates = df_estimates.select(select_order)\n    if len(df_ses):\n        ss_out.df_ses = concat_wrapper(df_ses,\n                                   how=\"diagonal\").select(select_order)\n\n    if len(df_ses):\n        ss_out.df_replicates = concat_wrapper(df_replicates,\n                                              how=\"diagonal\").select(select_order + [replicate_name])\n\n    ss_out.df_estimates = ss_out.round_results(df=ss_out.df_estimates)\n    ss_out.df_ses = ss_out.round_results(df=ss_out.df_ses)\n\n    if display:\n        ss_out.print()\n\n    return ss_out\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.pipe","title":"pipe","text":"<pre><code>pipe(function: Callable, *args, **kwargs) -&gt; StatCalculator\n</code></pre> <p>Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to pipe.</p> required <code>*args</code> <code>TYPE</code> <p>arguments to function</p> <code>()</code> <code>**kwargs</code> <code>TYPE</code> <p>keyword arguments to function</p> <code>{}</code> <p>Returns:</p> Type Description <code>StatCalculator</code> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def pipe(self, function: Callable, *args, **kwargs) -&gt; StatCalculator:\n    \"\"\"\n    Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)\n\n    Parameters\n    ----------\n    function : Callable\n        Function to pipe.\n    *args : TYPE\n        arguments to function\n    **kwargs : TYPE\n        keyword arguments to function\n\n    Returns\n    -------\n    StatCalculator\n\n    \"\"\"\n\n    self = self.copy()\n    self.replicate_stats = self.replicate_stats.pipe(\n        function=function, *args, **kwargs\n    )\n\n    return self\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.print","title":"print","text":"<pre><code>print(\n    df: IntoFrameT | None = None,\n    round_output: bool | int | None = None,\n    estimates_per_page: int = 0,\n    sub_log: logging = None,\n)\n</code></pre> <p>Print the estimates (with SEs if applicable) to the log.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>The estimates to display. Default is the estimates in self.</p> <code>None</code> <code>round_output</code> <code>bool | int | None</code> <p>Rounding rule (True for DRB, integer for number of significant digits). Default is self's rounding rule.</p> <code>None</code> <code>estimates_per_page</code> <code>int</code> <p>Repeat the header every k estimates. Defaults to 0 (don't repeat).</p> <code>0</code> <code>sub_log</code> <code>logging</code> <p>Override logger. Default is None (no override).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def print(\n    self,\n    df: IntoFrameT | None = None,\n    round_output: bool | int | None = None,\n    estimates_per_page: int = 0,\n    sub_log: logging = None,\n):\n    \"\"\"\n    Print the estimates (with SEs if applicable) to the log.\n\n    Parameters\n    ----------\n    df : IntoFrameT, optional\n        The estimates to display. Default is the estimates in self.\n    round_output : bool|int|None, optional\n        Rounding rule (True for DRB, integer for number of significant digits).\n        Default is self's rounding rule.\n    estimates_per_page : int, optional\n        Repeat the header every k estimates. Defaults to 0 (don't repeat).\n    sub_log : logging, optional\n        Override logger. Default is None (no override).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if self.df_replicates is not None:\n        self._print_replicates(\n            round_output=round_output,\n            estimates_per_page=estimates_per_page,\n            sub_log=sub_log,\n        )\n    else:\n        self._print_estimates(\n            df=df,\n            round_output=round_output,\n            estimates_per_page=estimates_per_page,\n            sub_log=sub_log,\n        )\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.round_results","title":"round_results","text":"<pre><code>round_results(\n    df: IntoFrameT | None = None,\n    rounding: Rounding | None = None,\n    display_only: bool = False,\n) -&gt; IntoFrameT\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Table of estimates. The default is the estimates in df_estimates</p> <code>None</code> <code>rounding</code> <code>Rounding | None</code> <p>Rounding (True for DRB rules) and an integer for specific number of significant digits. The default is self's rounding.</p> <code>None</code> <code>display_only</code> <code>bool</code> <p>If True, affects the display of numbers (casts to strings). The default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>IntoFrameT</code> <p>The rounded estimates.</p> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def round_results(\n    self,\n    df: IntoFrameT | None = None,\n    rounding: Rounding | None = None,\n    display_only: bool = False,\n) -&gt; IntoFrameT:\n    \"\"\"\n    Parameters\n    ----------\n    df : IntoFrameT, optional\n        Table of estimates. The default is the estimates in df_estimates\n    rounding : Rounding|None, optional\n        Rounding (True for DRB rules) and an integer for specific number of significant digits. The default is self's rounding.\n    display_only : bool, optional\n        If True, affects the display of numbers (casts to strings). The default is False.\n\n    Returns\n    -------\n    df : IntoFrameT\n        The rounded estimates.\n\n    \"\"\"\n\n    if df is None:\n        df = self.df_estimates\n\n    if rounding is None:\n        rounding = self.rounding\n\n    if df is not None:\n        df = drb_round_table(\n            df=df,\n            columns=rounding.cols_round,\n            columns_n=rounding.cols_n,\n            columns_exclude=rounding.cols_exclude,\n            round_all=rounding.round_all,\n            digits=rounding.round_digits,\n            display_only=display_only,\n        )\n\n    return df\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.StatCalculator.table_of_estimates","title":"table_of_estimates","text":"<pre><code>table_of_estimates(\n    round_output: bool | int | None = None,\n    estimates_to_show: list[str] | None = None,\n    variable_prefix: str = \"\",\n    estimate_type_variable_name: str = \"Statistic\",\n    ci_level: float = 0.95,\n) -&gt; IntoFrameT\n</code></pre> <p>Create a formatted table of estimates with option of statistics to report.</p> <p>Parameters:</p> Name Type Description Default <code>round_output</code> <code>bool | int | None</code> <p>Rounding rule for display.</p> <code>None</code> <code>estimates_to_show</code> <code>list[str] | None</code> <p>List of estimate types to include. Options: \"estimate\", \"se\", \"t\", \"p\", \"ci\". Default is [\"estimate\", \"se\"].</p> <code>None</code> <code>variable_prefix</code> <code>str</code> <p>Prefix to add to variable column names. Default is \"\".</p> <code>''</code> <code>estimate_type_variable_name</code> <code>str</code> <p>Name for the column indicating statistic type. Default is \"Statistic\".</p> <code>'Statistic'</code> <code>ci_level</code> <code>float</code> <p>Confidence interval level for \"ci\" estimates. Default is 0.95.</p> <code>0.95</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>Formatted table with estimates arranged by statistic type.</p> Source code in <code>src\\survey_kit\\statistics\\calculator.py</code> <pre><code>def table_of_estimates(\n    self,\n    round_output: bool | int | None = None,\n    estimates_to_show: list[str] | None = None,\n    variable_prefix: str = \"\",\n    estimate_type_variable_name: str = \"Statistic\",\n    ci_level: float = 0.95,\n) -&gt; IntoFrameT:\n    \"\"\"\n    Create a formatted table of estimates with option of statistics to report.\n\n    Parameters\n    ----------\n    round_output : bool|int|None, optional\n        Rounding rule for display.\n    estimates_to_show : list[str] | None, optional\n        List of estimate types to include. Options: \"estimate\", \"se\", \"t\", \"p\", \"ci\".\n        Default is [\"estimate\", \"se\"].\n    variable_prefix : str, optional\n        Prefix to add to variable column names. Default is \"\".\n    estimate_type_variable_name : str, optional\n        Name for the column indicating statistic type. Default is \"Statistic\".\n    ci_level : float, optional\n        Confidence interval level for \"ci\" estimates. Default is 0.95.\n\n    Returns\n    -------\n    IntoFrameT\n        Formatted table with estimates arranged by statistic type.\n    \"\"\"\n    if estimates_to_show is None:\n        estimates_to_show = [\"estimate\", \"se\"]\n\n    df_ordered = []\n    nw_ordered = []\n    col_sort = \"__order_output_table__\"\n    for index, esti in enumerate(estimates_to_show):\n        dfi = None\n        if esti.lower() == \"estimate\":\n            dfi = self.df_estimates\n\n        elif esti.lower() == \"se\" and self.df_replicates is not None:\n            dfi = self.df_ses\n        elif esti.lower() == \"t\" and self.df_replicates is not None:\n            dfi = self._df_t()\n        elif esti.lower() == \"p\" and self.df_replicates is not None:\n            dfi = self._df_p()\n        elif esti.lower() == \"ci\" and self.df_replicates is not None:\n            dfi = self._df_ci(ci_level=ci_level)\n        else:\n            message = f\"{esti} not allowed for estimates_to_show\"\n            logger.error(message)\n            raise Exception(message)\n\n        if dfi is not None:\n            nwi = NarwhalsType(dfi)\n            nw_ordered.append(nwi)\n            dfi = nwi.to_polars()\n            df_ordered.append(\n                dfi.with_columns(\n                    [\n                        pl.lit(index).alias(col_sort),\n                        pl.lit(esti.lower()).alias(estimate_type_variable_name),\n                    ]\n                )\n            )\n\n    col_row_index = \"___estimate_row_count___\"\n    df_ordered[0] = df_ordered[0].with_row_index(col_row_index)\n    df_display = concat_wrapper(df_ordered, how=\"diagonal\").lazy()\n\n    sort_vars = self.variable_ids + self.summarize_vars\n    df_display = df_display.sort(sort_vars + [col_sort]).with_columns(\n        pl.col(col_row_index).forward_fill()\n    )\n    df_display = df_display.sort([col_row_index] + [col_sort]).drop(col_row_index)\n\n    #   Clear extraneous information\n    with_clear = []\n    for coli in sort_vars:\n        c_col = pl.col(coli)\n        with_clear.append(\n            pl.when(pl.col(col_sort) != 0)\n            .then(pl.lit(\"\"))\n            .otherwise(c_col.cast(pl.String))\n            .alias(coli)\n        )\n\n    select_order = sort_vars + [estimate_type_variable_name]\n    remaining = []\n    rename = {}\n    for coli in df_display.lazy.collect_schema().names():\n        if coli not in select_order and coli != col_sort:\n            if variable_prefix != \"\":\n                rename[coli] = f\"{variable_prefix}{coli}\"\n\n            remaining.append(coli)\n\n    df_display = df_display.with_columns(with_clear).select(\n        select_order + remaining\n    )\n    #   Round?\n    if round_output:\n        rounding = deepcopy(self.rounding)\n        rounding.set_round_digits(round_output)\n\n        if (\n            len(rounding.cols_n) == 0\n            and len(rounding.cols_round) == 0\n            and not rounding.round_all\n        ):\n            #   Nothing set to round, round all\n            rounding.cols_round = remaining\n\n        df_display = self.round_results(\n            df=df_display, rounding=rounding, display_only=True\n        )\n\n    if len(rename):\n        df_display = df_display.rename(rename)\n    return nw_ordered[0].lazy().from_polars(df_display)\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.statistics.Statistics","title":"Statistics","text":"<p>Parameters:</p> Name Type Description Default <code>stats</code> <code>(list[str],)</code> <p>List of statistics to calculate (mean, median, etc.) Call Statistics.available_stats() for options</p> required <code>formula</code> <code>str</code> <p>formulaic (or R)-style formula for defining statistics to be calculated. The default is \"\".  This takes precedence over columns</p> <code>''</code> <code>columns</code> <code>list[str] | str | None</code> <p>List of columns to calculate statistics over. The default is None.</p> <code>None</code> <code>quantile_interpolated</code> <code>bool</code> <p>Use linear interpolation (census-style) for quantiles. The default is False.</p> <code>False</code> <code>quantile_interpolated_interval</code> <code>int</code> <p>If quantile_interpolated, what is the bin interval? The default is 2500.</p> <code>2500</code> Source code in <code>src\\survey_kit\\statistics\\statistics.py</code> <pre><code>class Statistics:\n    \"\"\"\n    Parameters\n    ----------\n    stats : list[str],\n        List of statistics to calculate (mean, median, etc.)\n        Call Statistics.available_stats() for options\n    formula : str, optional\n        formulaic (or R)-style formula for defining statistics to be calculated.\n        The default is \"\".  This takes precedence over columns\n    columns : list[str]|str|None, optional\n        List of columns to calculate statistics over. The default is None.\n    quantile_interpolated : bool, optional\n        Use linear interpolation (census-style) for quantiles. The default is False.\n    quantile_interpolated_interval : int, optional\n        If quantile_interpolated, what is the bin interval? The default is 2500.\n\n    \"\"\"\n    def __init__(\n        self,\n        stats: list[str],\n        formula: str = \"\",\n        columns: list[str] | str | None = None,\n        quantile_interpolated: bool = False,\n        quantile_interpolated_interval: int = 2500,\n    ):\n\n        #   Input parsing/set defaults\n        if columns is None:\n            columns = []\n        elif type(columns) is str:\n            columns = [columns]\n\n        self.formula = formula\n        self.columns = columns\n        self.quantile_interpolated = quantile_interpolated\n        self.quantile_interpolated_interval = quantile_interpolated_interval\n        self.stats = stats\n\n    @nw.narwhalify\n    def calculate(\n        self,\n        df: IntoFrameT,\n        weight: str = \"\",\n        by: dict[str, list[str]] | None = None,\n        summarize_vars: list | None = None,\n        rounding: Rounding | None = None,\n        allow_slow_pandas: bool = False,\n    ):\n        nw_type = NarwhalsType(df)\n\n        if summarize_vars is None:\n            summarize_vars = []\n        if rounding is None:\n            rounding = Rounding(round_output=False)\n\n        if by is None:\n            by = {\"All\": []}\n\n        if type(by) is list:\n            by = {f\"{i}\":itemi for i, itemi in enumerate(by)}\n\n        if self.formula != \"\":\n            #   It's a formula, process accordingly\n            df_summary = nw.from_native(\n                Formula(self.formula).get_model_matrix(df)\n            ).lazy_backend(nw_type)\n            cols_summary = df_summary.collect_schema().names()\n        else:\n            #   It's a variable list\n            if len(self.columns):\n                cols = []\n                for coli in self.columns:\n                    cols.extend(columns_from_list(df=df, columns=[coli]))\n                df_summary = df.select(cols)\n                cols_summary = cols\n            else:\n                cols_summary = df.lazy_backend(nw_type).collect_schema().names()\n\n        #   Keep the weights\n        if (\n            weight != \"\"\n            and weight not in df_summary.lazy_backend(nw_type).collect_schema().names()\n        ):\n            df_summary = concat_wrapper(\n                [df_summary, df.select(weight)], how=\"horizontal\"\n            )\n\n        if len(summarize_vars):\n            df_summary = concat_wrapper(\n                [\n                    drop_if_exists(df_summary,summarize_vars), \n                    df.select(summarize_vars)\n                ], \n                how=\"horizontal\"\n            )\n\n        #   Rename the stats for more useful table headers\n        stats_rename = {\n            \"count\": \"n, weighted\",\n            \"rawcount\": \"n\",\n            \"weight\": \"n, weighted\",\n        }\n\n        #   Same with the \"modifiers\"\n        modifiers_output = {\n            \"not0\": \" (not 0)\",\n            \"is0\": \" (== 0)\",\n            \"notmissing\": \" (not null)\",\n            \"share\": \" (share)\",\n            \"missing\": \" (missing)\",\n        }\n\n        #   Process the stats\n        stats_headers = {}\n        stats_dict = {}\n        for stati in self.stats:\n            stat_mod = stati.split(\"|\")\n\n            stati_raw = stat_mod[0]\n\n            modifier = \"\"\n            mod_header = \"\"\n            if len(stat_mod) == 2:\n                modifier = stat_mod[1]\n\n                if modifier in modifiers_output.keys():\n                    mod_header = modifiers_output[modifier]\n\n            if stati_raw in stats_rename.keys():\n                stat_headeri = stats_rename[stati_raw]\n            else:\n                stat_headeri = stati_raw\n            stats_headers[stati] = f\"{stat_headeri}{mod_header}\"\n\n            if modifier != \"\":\n                cols_include = [f\"{coli}|{modifier}\" for coli in cols_summary]\n            else:\n                cols_include = cols_summary.copy()\n            stats_dict = column_stats_builder(\n                column_stats=stats_dict,\n                cols_include=cols_include,\n                df=df_summary,\n                stat=[stati_raw],\n            )\n\n        summary_tables = calculate_by(\n            df=df_summary,\n            column_stats=stats_dict,\n            by=by,\n            always_return_as_collection=True,\n            weight=weight,\n            quantile_interpolated=self.quantile_interpolated,\n            quantile_interpolated_interval=self.quantile_interpolated_interval,\n            allow_slow_pandas=allow_slow_pandas,\n        )\n\n        suffixes = {}\n\n        for stati in self.stats:\n            stat_mod = stati.split(\"|\")\n            stat_onlyi = stat_mod[0]\n            if len(stat_mod) == 2:\n                modifier = stat_mod[1]\n            else:\n                modifier = \"\"\n\n            suffixes[stati] = self.stat_suffix(stat_onlyi, modifier)\n\n        stat_cols_final = None\n        default_index = \"___index___\"\n\n        for keyi, valuei in summary_tables.items():\n            valuei = nw.from_native(valuei)\n            b_default_index = False\n\n            if keyi in by.keys():\n                index = list_input(by[keyi])\n\n                if index is None:\n                    index = default_index\n                elif len(index) == 0:\n                    index = default_index\n            else:\n                index = default_index\n\n            if index == default_index:\n                #   Just use the row number as the index\n                valuei = (\n                    valuei.lazy()\n                    .collect()\n                    .with_row_index(name=index)\n                    .lazy_backend(nw_type)\n                )\n                index = [default_index]\n                b_default_index = True\n\n            summaries_by_var = []\n\n            #   Reshape to wide\n            for coli in cols_summary:\n                #   Catch if it's been renamed, like n-&gt;rawcount\n                (coli, _, coli_original) = _check_special_modifiers(coli)\n\n                cols_stats = [f\"{coli}_{suffixi}\" for suffixi in set(suffixes.values())]\n                keep_list = index + cols_stats\n\n                summaryi = valuei.select(keep_list)\n\n                if stat_cols_final is None:\n                    stat_cols_final = [\n                        f\"{stats_headers[keyi]}\" for keyi in stats_headers.keys()\n                    ]\n\n                summaryi = summaryi.rename(\n                    {\n                        f\"{coli}_{suffixes[keyi]}\": stats_headers[keyi]\n                        for keyi in stats_headers.keys()\n                    }\n                )\n                if b_default_index:\n                    summaryi = summaryi.drop(index)\n                    keep_list_final = [\"Variable\"] + list(\n                        dict.fromkeys(stats_headers.values())\n                    )\n                else:\n                    keep_list_final = (\n                        [\"Variable\"]\n                        + index\n                        + list(dict.fromkeys(stats_headers.values()))\n                    )\n\n                summaryi = summaryi.with_columns(\n                    nw.lit(coli_original).alias(\"Variable\")\n                ).select(keep_list_final)\n                summaries_by_var.append(summaryi)\n\n            valuei = concat_wrapper(summaries_by_var, how=\"vertical\")\n            # summary_tables[keyi] = Compress(valuei,\n            #                                 no_boolean=True)\n            summary_tables[keyi] = valuei\n\n        cols_by = []\n        for keyi, valuei in summary_tables.items():\n            cols_by.extend(\n                list(\n                    set(\n                        summary_tables[keyi].lazy().collect_schema().names()\n                    ).difference(stat_cols_final + cols_by)\n                )\n            )\n            cols_by.remove(\"Variable\")\n\n        cols_dedupped = list(set(stat_cols_final))\n        if len(cols_dedupped) != len(stat_cols_final):\n            stat_cols_final = _columns_original_order(\n                cols_unordered=cols_dedupped, cols_ordered=stat_cols_final\n            )\n        keep_order = [\"Variable\"] + cols_by + stat_cols_final\n        output_table = concat_wrapper(\n            list(summary_tables.values()), how=\"diagonal\"\n        ).select(keep_order)\n\n        if len(cols_by):\n            output_table = output_table.sort(cols_by)\n\n        #   Get the information for rounding\n        (cols_round, cols_n) = self.rounding_columns(\n            output_table.drop([\"Variable\"] + summarize_vars)\n        )\n\n        rounding.cols_round = list(set(rounding.cols_round + cols_round))\n        rounding.cols_n = list(set(rounding.cols_n + cols_n))\n\n        return output_table.lazy_backend(nw_type)\n\n    def stat_suffix(\n        self=None,\n        Statistic: str = \"\",\n        #   not0, missing, nonmissing\n        modifier: str = \"\",\n    ) -&gt; str:\n        if modifier != \"\":\n            modifier_suffix = f\"_{modifier}\"\n        else:\n            modifier_suffix = \"\"\n\n        if Statistic in [\"mean\", \"sum\", \"var\", \"std\", \"max\", \"min\", \"first\", \"gini\"]:\n            suffix = Statistic + modifier_suffix\n        elif Statistic == \"median\":\n            suffix = \"q0_5\" + modifier_suffix\n        elif Statistic.startswith(\"q\") or Statistic.startswith(\"p\"):\n            quantile = float(Statistic.replace(\"q\", \"\").replace(\"p\", \"\")) / 100\n            suffix = f\"q{str(quantile).replace('.', '_')}\" + modifier_suffix\n        elif (\n            Statistic.startswith(\"count\")\n            or Statistic.startswith(\"rawcount\")\n            or Statistic.startswith(\"share\")\n            or Statistic.startswith(\"rawshare\")\n            or Statistic == \"n\"\n            or Statistic == \"weight\"\n        ):\n            if Statistic.startswith(\"count\") or Statistic == \"weight\":\n                count_prefix = \"n\"\n            elif Statistic.startswith(\"rawcount\") or Statistic == \"n\":\n                count_prefix = \"rawn\"\n            elif Statistic.startswith(\"share\"):\n                count_prefix = \"share\"\n            elif Statistic.startswith(\"rawshare\"):\n                count_prefix = \"rawshare\"\n\n            count_suffix = \"\"\n            suffixes = [\"_not0\", \"_is0\", \"_notmissing\", \"_missing\", \"_share\"]\n            for si in suffixes:\n                if Statistic.endswith(si):\n                    count_suffix = si\n\n            suffix = f\"{count_prefix}{count_suffix}{modifier_suffix}\"\n\n        try:\n            return suffix\n        except:\n            message = f\"{Statistic} is not a valid statistic\"\n            logger.error(message)\n            raise Exception(message)\n\n    @nw.narwhalify\n    def rounding_columns(self, df: IntoFrameT) -&gt; tuple[list[str], list[str]]:\n        cols_n = [\n            \"n\",\n            \"n (missing)\",\n            \"n (not null)\",\n            \"n (not 0)\",  # ,\n            # \"n, weighted\",\n            # \"n missing, weighted\",\n            # \"n (not null), weighted\",\n            # \"n (not 0), weighted\"\n        ]\n        columns = df.lazy().collect_schema().names()\n        cols_n = list(set(cols_n).intersection(columns))\n        cols_round = list(set(columns).difference(cols_n))\n\n        return (cols_round, cols_n)\n\n    def available_stats():\n        examples = [\n            \"mean\",\n            \"sum\",\n            \"median\",\n            \"q10\",\n            \"q97.5\",\n            \"std\",\n            \"var\",\n            \"max\",\n            \"min\",\n            \"weight\",\n            \"n\",\n            \"gini\",\n        ]\n        logger.info(\"\")\n        logger.info(f\"Some examples: {examples}\")\n        logger.info(\"\")\n        logger.info(\n            \"Stats can also have 'modifiers' appended to them separated by a pipe ('|'), including\"\n        )\n        modifiers = [\"not0\", \"missing\", \"notmissing\", \"is0\", \"share\"]\n        logger.info(modifiers)\n\n        logger.info(\"\")\n        logger.info(\"For quantiles, pass q{number} where number in (0,100)\")\n        logger.info(\"\")\n        logger.info(\"n is the unweighted count and weight is the weighted count\")\n        logger.info(f\"     for n/weight: {modifiers}\")\n\n        modifiers = [\"not0\"]\n        logger.info(f\"     for all other stats: {modifiers}\")\n\n        examples = [\n            \"mean|not0\",\n            \"sum|not0\",\n            \"median\",\n            \"min|not0\",\n            \"count|missing\",\n            \"n|notmissing\",\n            \"n|share\",\n        ]\n        logger.info(\"\")\n        logger.info(f\"\"\"Some examples: {examples}\"\"\")\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.replicates.Replicates","title":"Replicates","text":"<p>               Bases: <code>Serializable</code></p> <p>Configuration for replicate weight variance estimation.</p> <p>Replicates defines the structure of replicate weights in survey data for calculating proper standard errors that account for complex sample designs. Supports two variance estimation methods: Bootstrap and Balanced Repeated Replication (BRR).</p> <p>Replicate weights are commonly used by statistical agencies (Census Bureau, BLS, etc.) to enable users to calculate design-based standard errors without sharing the full sample design details (strata, clusters, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>weight_stub</code> <code>str</code> <p>Prefix for weight column names. For example, if weight_stub=\"weight_\", the function looks for columns: weight_0, weight_1, ..., weight_n where weight_0 is the base weight and weight_1 through weight_n are the replicate weights.</p> required <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe containing the weight columns. Used to automatically detect the number of replicates. Default is None.</p> <code>None</code> <code>n_replicates</code> <code>int | None</code> <p>Number of replicate weights (excluding the base weight). If None, will be inferred from df. Default is None.</p> <code>None</code> <code>bootstrap</code> <code>bool</code> <p>Type of variance estimation: - True: Bootstrap variance (standard bootstrap resampling) - False: Balanced Repeated Replication (BRR) variance If you don't know which to use, use bootstrap=True. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>weight_stub</code> <code>str</code> <p>The weight column prefix.</p> <code>n_replicates</code> <code>int</code> <p>Number of replicate weights.</p> <code>bootstrap</code> <code>bool</code> <p>Variance estimation method flag.</p> <code>rep_list</code> <code>list[str]</code> <p>List of all weight column names (base + replicates).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If neither df nor n_replicates is provided.</p> <p>Examples:</p> <p>Infer number of replicates from dataframe:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.replicates import Replicates\n&gt;&gt;&gt; replicates = Replicates(\n...     df=df,\n...     weight_stub=\"weight_\",\n...     bootstrap=True\n... )\n&gt;&gt;&gt; print(replicates.n_replicates)\n&gt;&gt;&gt; print(replicates.rep_list)\n</code></pre> <p>Specify number of replicates directly:</p> <pre><code>&gt;&gt;&gt; replicates = Replicates(\n...     weight_stub=\"weight_\",\n...     n_replicates=80,\n...     bootstrap=False  # Use BRR variance\n... )\n</code></pre> <p>Use with StatCalculator:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.calculator import StatCalculator\n&gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n&gt;&gt;&gt; \n&gt;&gt;&gt; stats = Statistics(stats=[\"mean\", \"median\"], columns=[\"income\"])\n&gt;&gt;&gt; replicates = Replicates(weight_stub=\"weight_\", n_replicates=80, bootstrap=True)\n&gt;&gt;&gt; \n&gt;&gt;&gt; sc = StatCalculator(\n...     df=df,\n...     statistics=stats,\n...     weight=\"weight_0\",\n...     replicates=replicates\n... )\n&gt;&gt;&gt; sc.print()\n</code></pre> <p>Use with multiple imputation:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.multiple_imputation import mi_ses_from_function\n&gt;&gt;&gt; \n&gt;&gt;&gt; arguments = {\n...     \"statistics\": stats,\n...     \"replicates\": replicates,\n...     \"weight\": \"weight_0\"\n... }\n&gt;&gt;&gt; \n&gt;&gt;&gt; mi_results = mi_ses_from_function(\n...     delegate=StatCalculator,\n...     df_implicates=srmi.df_implicates,\n...     df_noimputes=weights_df,\n...     arguments=arguments,\n...     join_on=[\"Variable\"]\n... )\n</code></pre> Notes <p>Bootstrap variance (bootstrap=True): - Standard bootstrap resampling variance estimation - SE = sqrt(\u03a3(\u03b8\u0302\u1d63 - \u03b8\u0302\u2080)\u00b2 / R) where \u03b8\u0302\u2080 is the base weight estimate, \u03b8\u0302\u1d63 are replicate estimates, and R is number of replicates - Use when you generated your own bootstrap weights</p> <p>BRR variance (bootstrap=False): - Balanced Repeated Replication variance - Used by Census Bureau and other statistical agencies - SE = sqrt(4 \u03a3(\u03b8\u0302\u1d63 - \u03b8\u0302)\u00b2 / R) where \u03b8\u0304\u1d63 is the mean across all replicate estimates - Use when working with official government surveys that provide BRR weights</p> <p>The rep_list attribute provides all weight column names in order: [weight_0, weight_1, ..., weight_n] where weight_0 is the base weight.</p> See Also <p>StatCalculator : Calculate statistics with replicate weight SEs mi_ses_from_function : Combine replicate weights with multiple imputation</p> Source code in <code>src\\survey_kit\\statistics\\replicates.py</code> <pre><code>class Replicates(Serializable):\n    \"\"\"\n    Configuration for replicate weight variance estimation.\n\n    Replicates defines the structure of replicate weights in survey data for\n    calculating proper standard errors that account for complex sample designs.\n    Supports two variance estimation methods: Bootstrap and Balanced Repeated\n    Replication (BRR).\n\n    Replicate weights are commonly used by statistical agencies (Census Bureau, BLS,\n    etc.) to enable users to calculate design-based standard errors without sharing\n    the full sample design details (strata, clusters, etc.).\n\n    Parameters\n    ----------\n    weight_stub : str\n        Prefix for weight column names. For example, if weight_stub=\"weight_\",\n        the function looks for columns: weight_0, weight_1, ..., weight_n\n        where weight_0 is the base weight and weight_1 through weight_n are\n        the replicate weights.\n    df : IntoFrameT | None, optional\n        Dataframe containing the weight columns. Used to automatically detect\n        the number of replicates. Default is None.\n    n_replicates : int | None, optional\n        Number of replicate weights (excluding the base weight). If None,\n        will be inferred from df. Default is None.\n    bootstrap : bool, optional\n        Type of variance estimation:\n        - True: Bootstrap variance (standard bootstrap resampling)\n        - False: Balanced Repeated Replication (BRR) variance\n        If you don't know which to use, use bootstrap=True. Default is False.\n\n    Attributes\n    ----------\n    weight_stub : str\n        The weight column prefix.\n    n_replicates : int\n        Number of replicate weights.\n    bootstrap : bool\n        Variance estimation method flag.\n    rep_list : list[str]\n        List of all weight column names (base + replicates).\n\n    Raises\n    ------\n    Exception\n        If neither df nor n_replicates is provided.\n\n    Examples\n    --------\n    Infer number of replicates from dataframe:\n\n    &gt;&gt;&gt; from survey_kit.statistics.replicates import Replicates\n    &gt;&gt;&gt; replicates = Replicates(\n    ...     df=df,\n    ...     weight_stub=\"weight_\",\n    ...     bootstrap=True\n    ... )\n    &gt;&gt;&gt; print(replicates.n_replicates)\n    &gt;&gt;&gt; print(replicates.rep_list)\n\n    Specify number of replicates directly:\n\n    &gt;&gt;&gt; replicates = Replicates(\n    ...     weight_stub=\"weight_\",\n    ...     n_replicates=80,\n    ...     bootstrap=False  # Use BRR variance\n    ... )\n\n    Use with StatCalculator:\n\n    &gt;&gt;&gt; from survey_kit.statistics.calculator import StatCalculator\n    &gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; stats = Statistics(stats=[\"mean\", \"median\"], columns=[\"income\"])\n    &gt;&gt;&gt; replicates = Replicates(weight_stub=\"weight_\", n_replicates=80, bootstrap=True)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; sc = StatCalculator(\n    ...     df=df,\n    ...     statistics=stats,\n    ...     weight=\"weight_0\",\n    ...     replicates=replicates\n    ... )\n    &gt;&gt;&gt; sc.print()\n\n    Use with multiple imputation:\n\n    &gt;&gt;&gt; from survey_kit.statistics.multiple_imputation import mi_ses_from_function\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; arguments = {\n    ...     \"statistics\": stats,\n    ...     \"replicates\": replicates,\n    ...     \"weight\": \"weight_0\"\n    ... }\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; mi_results = mi_ses_from_function(\n    ...     delegate=StatCalculator,\n    ...     df_implicates=srmi.df_implicates,\n    ...     df_noimputes=weights_df,\n    ...     arguments=arguments,\n    ...     join_on=[\"Variable\"]\n    ... )\n\n    Notes\n    -----\n    **Bootstrap variance** (bootstrap=True):\n    - Standard bootstrap resampling variance estimation\n    - SE = sqrt(\u03a3(\u03b8\u0302\u1d63 - \u03b8\u0302\u2080)\u00b2 / R) where \u03b8\u0302\u2080 is the base weight estimate, \u03b8\u0302\u1d63 are replicate estimates, and R is number of replicates\n    - Use when you generated your own bootstrap weights\n\n    **BRR variance** (bootstrap=False):\n    - Balanced Repeated Replication variance\n    - Used by Census Bureau and other statistical agencies\n    - SE = sqrt(4 \u03a3(\u03b8\u0302\u1d63 - \u03b8\u0302)\u00b2 / R) where \u03b8\u0304\u1d63 is the mean across all replicate estimates\n    - Use when working with official government surveys that provide BRR weights\n\n    The rep_list attribute provides all weight column names in order:\n    [weight_0, weight_1, ..., weight_n] where weight_0 is the base weight.\n\n    See Also\n    --------\n    StatCalculator : Calculate statistics with replicate weight SEs\n    mi_ses_from_function : Combine replicate weights with multiple imputation\n    \"\"\"\n    _save_suffix = \"replicates\"\n\n    def __init__(\n        self,\n        weight_stub: str,\n        df: IntoFrameT | None = None,\n        n_replicates: int | None = None,\n        bootstrap: bool = False,\n    ):\n        if n_replicates is None and df is None:\n            message = \"You must pass either df or n_replicates to Replicates\"\n            logger.error(message)\n            raise Exception(message)\n\n        if n_replicates is None:\n            cols_replicates = columns_from_list(df=df, columns=f\"{weight_stub}*\")\n            n_replicates = len(set(cols_replicates).difference([f\"{weight_stub}0\"]))\n        elif df is not None:\n            logger.info(\"Passed both df and n_replicates to Replicates, ignoring df\")\n\n        self.weight_stub = weight_stub\n        self.n_replicates = n_replicates\n        self.bootstrap = bootstrap\n\n        self.rep_list = [f\"{weight_stub}{repi}\" for repi in range(0, n_replicates + 1)]\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.ComparisonItem","title":"ComparisonItem","text":"<p>Helpers for specifying comparisons in StatCalculator  and MultipleImputation.</p> <p>Provides two types of comparisons:</p> <ul> <li>Variable: Compare different variables (e.g., income vs income_2)</li> <li>Column: Compare different statistics (e.g., mean vs median)</li> </ul> Source code in <code>src\\survey_kit\\statistics\\comparisons.py</code> <pre><code>class ComparisonItem:\n    \"\"\"\n    Helpers for specifying comparisons in [StatCalculator][survey_kit.statistics.calculator.StatCalculator] \n    and [MultipleImputation][survey_kit.statistics.multiple_imputation.MultipleImputation].\n\n    Provides two types of comparisons:\n\n    - Variable: Compare different variables (e.g., income vs income_2)\n    - Column: Compare different statistics (e.g., mean vs median)\n\n    \"\"\"\n    class Variable:\n        \"\"\"\n        Specify a comparison between two variables.\n\n        Used to compare different variables within the same dataset, such as\n        comparing income from two sources or comparing outcomes across groups.\n\n        Parameters\n        ----------\n        value1 : str\n            First variable value to compare (e.g., \"income\").\n        value2 : str\n            Second variable value to compare (e.g., \"income_2\").\n        column : str, optional\n            Name of the column containing variable names in the estimates dataframe.\n            Default is \"Variable\".\n        name : str, optional\n            Name for the comparison result. If empty, uses f\"{value1}_vs_{value2}\".\n            Default is \"\".\n\n        Examples\n        --------\n        &gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Compare two income variables\n        &gt;&gt;&gt; comp = ComparisonItem.Variable(\n        ...     value1=\"wage_income\",\n        ...     value2=\"self_employment_income\",\n        ...     name=\"wage_vs_self_employment\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; comparison = sc.compare(\n        ...     sc,\n        ...     difference=False,\n        ...     compare_list_variables=[comp]\n        ... )[\"ratio\"]\n        \"\"\"\n        def __init__(self, value1: str, value2: str, column: str=\"Variable\", name: str = \"\"):\n            self.column = column\n            self.value1 = value1\n            self.value2 = value2\n            self.name = name\n\n    class Column:\n        \"\"\"\n        Specify a comparison between two statistics/columns.\n\n        Used to compare different statistics for the same variable, such as\n        comparing mean vs median or comparing different quantiles.\n\n        Parameters\n        ----------\n        column1 : str\n            First statistic column to compare (e.g., \"mean\").\n        column2 : str\n            Second statistic column to compare (e.g., \"median\").\n        name : str, optional\n            Name for the comparison result. If empty, uses f\"c({column1},{column2})\".\n            Default is \"\".\n\n        Examples\n        --------\n        &gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Compare mean vs median\n        &gt;&gt;&gt; comp = ComparisonItem.Column(\n        ...     column1=\"mean\",\n        ...     column2=\"median (not 0)\",\n        ...     name=\"median_mean_diff\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; comparison = sc.compare(\n        ...     sc,\n        ...     ratio=False,\n        ...     compare_list_columns=[comp]\n        ... )[\"difference\"]\n        \"\"\"\n        def __init__(self, column1: str, column2: str, name: str = \"\"):\n            self.column1 = column1\n            self.column2 = column2\n\n            if name == \"\":\n                name = f\"c({column1},{column2})\"\n            self.name = name\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.ComparisonItem.Column","title":"Column","text":"<p>Specify a comparison between two statistics/columns.</p> <p>Used to compare different statistics for the same variable, such as comparing mean vs median or comparing different quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>column1</code> <code>str</code> <p>First statistic column to compare (e.g., \"mean\").</p> required <code>column2</code> <code>str</code> <p>Second statistic column to compare (e.g., \"median\").</p> required <code>name</code> <code>str</code> <p>Name for the comparison result. If empty, uses f\"c({column1},{column2})\". Default is \"\".</p> <code>''</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Compare mean vs median\n&gt;&gt;&gt; comp = ComparisonItem.Column(\n...     column1=\"mean\",\n...     column2=\"median (not 0)\",\n...     name=\"median_mean_diff\"\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; comparison = sc.compare(\n...     sc,\n...     ratio=False,\n...     compare_list_columns=[comp]\n... )[\"difference\"]\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\comparisons.py</code> <pre><code>class Column:\n    \"\"\"\n    Specify a comparison between two statistics/columns.\n\n    Used to compare different statistics for the same variable, such as\n    comparing mean vs median or comparing different quantiles.\n\n    Parameters\n    ----------\n    column1 : str\n        First statistic column to compare (e.g., \"mean\").\n    column2 : str\n        Second statistic column to compare (e.g., \"median\").\n    name : str, optional\n        Name for the comparison result. If empty, uses f\"c({column1},{column2})\".\n        Default is \"\".\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Compare mean vs median\n    &gt;&gt;&gt; comp = ComparisonItem.Column(\n    ...     column1=\"mean\",\n    ...     column2=\"median (not 0)\",\n    ...     name=\"median_mean_diff\"\n    ... )\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; comparison = sc.compare(\n    ...     sc,\n    ...     ratio=False,\n    ...     compare_list_columns=[comp]\n    ... )[\"difference\"]\n    \"\"\"\n    def __init__(self, column1: str, column2: str, name: str = \"\"):\n        self.column1 = column1\n        self.column2 = column2\n\n        if name == \"\":\n            name = f\"c({column1},{column2})\"\n        self.name = name\n</code></pre>"},{"location":"api/basic_standard_errors/#survey_kit.statistics.calculator.ComparisonItem.Variable","title":"Variable","text":"<p>Specify a comparison between two variables.</p> <p>Used to compare different variables within the same dataset, such as comparing income from two sources or comparing outcomes across groups.</p> <p>Parameters:</p> Name Type Description Default <code>value1</code> <code>str</code> <p>First variable value to compare (e.g., \"income\").</p> required <code>value2</code> <code>str</code> <p>Second variable value to compare (e.g., \"income_2\").</p> required <code>column</code> <code>str</code> <p>Name of the column containing variable names in the estimates dataframe. Default is \"Variable\".</p> <code>'Variable'</code> <code>name</code> <code>str</code> <p>Name for the comparison result. If empty, uses f\"{value1}vs\". Default is \"\".</p> <code>''</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Compare two income variables\n&gt;&gt;&gt; comp = ComparisonItem.Variable(\n...     value1=\"wage_income\",\n...     value2=\"self_employment_income\",\n...     name=\"wage_vs_self_employment\"\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; comparison = sc.compare(\n...     sc,\n...     difference=False,\n...     compare_list_variables=[comp]\n... )[\"ratio\"]\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\comparisons.py</code> <pre><code>class Variable:\n    \"\"\"\n    Specify a comparison between two variables.\n\n    Used to compare different variables within the same dataset, such as\n    comparing income from two sources or comparing outcomes across groups.\n\n    Parameters\n    ----------\n    value1 : str\n        First variable value to compare (e.g., \"income\").\n    value2 : str\n        Second variable value to compare (e.g., \"income_2\").\n    column : str, optional\n        Name of the column containing variable names in the estimates dataframe.\n        Default is \"Variable\".\n    name : str, optional\n        Name for the comparison result. If empty, uses f\"{value1}_vs_{value2}\".\n        Default is \"\".\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.statistics.calculator import ComparisonItem\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Compare two income variables\n    &gt;&gt;&gt; comp = ComparisonItem.Variable(\n    ...     value1=\"wage_income\",\n    ...     value2=\"self_employment_income\",\n    ...     name=\"wage_vs_self_employment\"\n    ... )\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; comparison = sc.compare(\n    ...     sc,\n    ...     difference=False,\n    ...     compare_list_variables=[comp]\n    ... )[\"ratio\"]\n    \"\"\"\n    def __init__(self, value1: str, value2: str, column: str=\"Variable\", name: str = \"\"):\n        self.column = column\n        self.value1 = value1\n        self.value2 = value2\n        self.name = name\n</code></pre>"},{"location":"api/calibration/","title":"Calibration","text":""},{"location":"api/calibration/#survey_kit.calibration.moment.Moment","title":"Moment","text":"<p>               Bases: <code>Serializable</code></p> <p>Statistical moments for survey calibration.</p> <p>A Moment represents target statistics (means, proportions) that survey  weights should be calibrated to match. Moments can be simple (e.g., overall mean) interactions (i.e. share in a and b).  They can also be stratified by groups, and can include submoments for more complex constraints.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Dataframe containing the data.</p> <code>None</code> <code>formula</code> <code>str</code> <p>Formulaic formula specifying variables for the moment (e.g., \"C(gender) + age\"). Default is \"\".</p> <code>''</code> <code>weight</code> <code>str</code> <p>Column name for weights. If empty, uniform weights are created. Default is \"\".</p> <code>''</code> <code>index</code> <code>str | list[str] | None</code> <p>Column name(s) for unique observation identifiers. Default is None.</p> <code>None</code> <code>sort_by</code> <code>str | list[str] | None</code> <p>Column name(s) to sort by when creating row index. Default is None.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to rescale model matrix by dividing by target values. This implies a  tradeoff between the tolerated miss (i.e. how close do we need to get  in the calibration) and the target of this moment. Default is True.</p> <code>True</code> <code>by</code> <code>list[str] | str | None</code> <p>Variables or formula to stratify moment by, creating submoments for each i.e. if the formula is race and gender and by is state, the moments would race x gender x state group. Default is None.</p> <code>None</code> <code>missing_to_zero</code> <code>bool</code> <p>Fill missing values with zero. Default is True.</p> <code>True</code> <code>keep_full_group</code> <code>bool</code> <p>When using 'by', whether to keep the overall moment in addition to submoments. Default is False.</p> <code>False</code> <code>equalize_by</code> <code>bool</code> <p>Weight submoments equally rather than by their sample proportions. Default is False.</p> <code>False</code> <code>equalize_by_obs_share</code> <code>bool</code> <p>Weight submoments (within by) by their observation counts. Default is False.</p> <code>False</code> <code>equalize_by_weight_share</code> <code>bool</code> <p>Weight submoments (within by) by their weight sums. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>model_matrix</code> <code>IntoFrameT | None</code> <p>Design matrix for calibration (X in the moment equations).</p> <code>targets</code> <code>IntoFrameT | None</code> <p>Target values to calibrate to.</p> <code>non_zero</code> <code>IntoFrameT | None</code> <p>Count of non-zero observations for each moment variable.</p> <code>sub_moments</code> <code>list[Moment]</code> <p>List of submoments when stratifying by groups.</p> <code>columns</code> <code>list[str]</code> <p>Column names used in calibration after any restrictions.</p> <code>n_observations</code> <code>int</code> <p>Number of observations in this moment.</p> <p>Examples:</p> <p>Simple moment for categorical variable:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from survey_kit.calibration.moment import Moment\n&gt;&gt;&gt; \n&gt;&gt;&gt; df = pl.DataFrame({\n&gt;&gt;&gt;     \"id\": range(100),\n&gt;&gt;&gt;     \"gender\": [\"M\", \"F\"] * 50,\n&gt;&gt;&gt;     \"weight\": [1.0] * 100\n&gt;&gt;&gt; })\n&gt;&gt;&gt; \n&gt;&gt;&gt; moment = Moment(df=df, formula=\"C(gender)\", weight=\"weight\")\n</code></pre> <p>Moment with continuous and categorical variables:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\n&gt;&gt;&gt;     \"id\": range(100),\n&gt;&gt;&gt;     \"age\": range(20, 120),\n&gt;&gt;&gt;     \"education\": [\"HS\", \"College\", \"Grad\"] * 33 + [\"HS\"],\n&gt;&gt;&gt;     \"weight\": [1.0] * 100\n&gt;&gt;&gt; })\n&gt;&gt;&gt; \n&gt;&gt;&gt; moment = Moment(\n&gt;&gt;&gt;     df=df, \n&gt;&gt;&gt;     formula=\"age + C(education)\", \n&gt;&gt;&gt;     weight=\"weight\"\n&gt;&gt;&gt; )\n</code></pre> <p>Stratified moment with subgroups:</p> <pre><code>&gt;&gt;&gt; moment = Moment(\n&gt;&gt;&gt;     df=df,\n&gt;&gt;&gt;     formula=\"age\",\n&gt;&gt;&gt;     by=\"C(education)\",\n&gt;&gt;&gt;     weight=\"weight\",\n&gt;&gt;&gt;     equalize_by=True\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # This creates separate age moments for each education level\n</code></pre> Notes <ul> <li>Formulas use the formulaic library syntax, with C() for categorical variables.</li> <li>When using 'by' parameter, submoments are automatically created for each group.</li> <li>The rescale option divides model matrix values by targets, which can improve convergence but changes the interpretation of calibration parameters.</li> <li>Submoments allow for complex calibration schemes like raking or post-stratification.</li> </ul> Source code in <code>src\\survey_kit\\calibration\\moment.py</code> <pre><code>class Moment(Serializable):\n    \"\"\"\n    Statistical moments for survey calibration.\n\n    A Moment represents target statistics (means, proportions) that survey \n    weights should be calibrated to match. Moments can be simple (e.g., overall mean)\n    interactions (i.e. share in a and b).  They can also be  \n    stratified by groups, and can include submoments for more complex constraints.\n\n    Parameters\n    ----------\n    df : IntoFrameT\n        Dataframe containing the data.\n    formula : str, optional\n        Formulaic formula specifying variables for the moment (e.g., \"C(gender) + age\").\n        Default is \"\".\n    weight : str, optional\n        Column name for weights. If empty, uniform weights are created. Default is \"\".\n    index : str | list[str] | None, optional\n        Column name(s) for unique observation identifiers. Default is None.\n    sort_by : str | list[str] | None, optional\n        Column name(s) to sort by when creating row index. Default is None.\n    rescale : bool, optional\n        Whether to rescale model matrix by dividing by target values. This implies a \n        tradeoff between the tolerated miss (i.e. how close do we need to get \n        in the calibration) and the target of this moment. Default is True.\n    by : list[str] | str | None, optional\n        Variables or formula to stratify moment by, creating submoments for each\n        i.e. if the formula is race and gender and by is state,\n        the moments would race x gender x state\n        group. Default is None.\n    missing_to_zero : bool, optional\n        Fill missing values with zero. Default is True.\n    keep_full_group : bool, optional\n        When using 'by', whether to keep the overall moment in addition to\n        submoments. Default is False.\n    equalize_by : bool, optional\n        Weight submoments equally rather than by their sample proportions.\n        Default is False.\n    equalize_by_obs_share : bool, optional\n        Weight submoments (within by) by their observation counts. Default is False.\n    equalize_by_weight_share : bool, optional\n        Weight submoments (within by) by their weight sums. Default is False.\n\n    Attributes\n    ----------\n    model_matrix : IntoFrameT | None\n        Design matrix for calibration (X in the moment equations).\n    targets : IntoFrameT | None\n        Target values to calibrate to.\n    non_zero : IntoFrameT | None\n        Count of non-zero observations for each moment variable.\n    sub_moments : list[Moment]\n        List of submoments when stratifying by groups.\n    columns : list[str]\n        Column names used in calibration after any restrictions.\n    n_observations : int\n        Number of observations in this moment.\n\n    Examples\n    --------\n    Simple moment for categorical variable:\n\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from survey_kit.calibration.moment import Moment\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; df = pl.DataFrame({\n    &gt;&gt;&gt;     \"id\": range(100),\n    &gt;&gt;&gt;     \"gender\": [\"M\", \"F\"] * 50,\n    &gt;&gt;&gt;     \"weight\": [1.0] * 100\n    &gt;&gt;&gt; })\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; moment = Moment(df=df, formula=\"C(gender)\", weight=\"weight\")\n\n    Moment with continuous and categorical variables:\n\n    &gt;&gt;&gt; df = pl.DataFrame({\n    &gt;&gt;&gt;     \"id\": range(100),\n    &gt;&gt;&gt;     \"age\": range(20, 120),\n    &gt;&gt;&gt;     \"education\": [\"HS\", \"College\", \"Grad\"] * 33 + [\"HS\"],\n    &gt;&gt;&gt;     \"weight\": [1.0] * 100\n    &gt;&gt;&gt; })\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; moment = Moment(\n    &gt;&gt;&gt;     df=df, \n    &gt;&gt;&gt;     formula=\"age + C(education)\", \n    &gt;&gt;&gt;     weight=\"weight\"\n    &gt;&gt;&gt; )\n\n    Stratified moment with subgroups:\n\n    &gt;&gt;&gt; moment = Moment(\n    &gt;&gt;&gt;     df=df,\n    &gt;&gt;&gt;     formula=\"age\",\n    &gt;&gt;&gt;     by=\"C(education)\",\n    &gt;&gt;&gt;     weight=\"weight\",\n    &gt;&gt;&gt;     equalize_by=True\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # This creates separate age moments for each education level\n\n    Notes\n    -----\n    - Formulas use the formulaic library syntax, with C() for categorical variables.\n    - When using 'by' parameter, submoments are automatically created for each group.\n    - The rescale option divides model matrix values by targets, which can improve\n    convergence but changes the interpretation of calibration parameters.\n    - Submoments allow for complex calibration schemes like raking or post-stratification.\n    \"\"\"\n    _save_suffix = \"moment\"\n    _save_exclude_items = [\"nw_type\"]\n\n    def __init__(\n        self,\n        df: IntoFrameT | None = None,\n        nw_type: NarwhalsType | None = None,\n        formula: str = \"\",\n        weight: str = \"\",\n        index: str | list[str] | None = None,\n        sort_by: str | list[str] | None = None,\n        rescale: bool = True,\n        by: list[str] | str | None = None,\n        missing_to_zero: bool = True,\n        keep_full_group: bool = False,\n        equalize_by: bool = False,\n        equalize_by_obs_share: bool = False,\n        equalize_by_weight_share: bool = False,\n        by_share: float = 1.0,\n        target_moment: bool = True,\n        is_sub_moment: bool = False,\n        initial_processing: bool = True,\n    ):\n        if missing_to_zero and df is not None:\n            df = fill_missing(df=df)\n\n        self.df = df\n\n        self.formula = formula\n        self.weight = weight\n        self.index = list_input(index)\n        self.rescale = rescale\n\n        if by is None:\n            by = []\n        self.by = by\n\n        self.equalize_by = equalize_by\n        self.equalize_by_obs_share = equalize_by_obs_share\n        self.equalize_by_weight_share = equalize_by_weight_share\n        self.keep_full_group = keep_full_group\n\n        # Derived variables\n        #   Any sub_moments of this?\n        self.sub_moments = []\n        #   Processed By into a list of where statements\n        self.by_where_expressions = []\n        self.by_where_strings = []\n        # By group adjustment to weights (i.e. this group's share of the total weight)\n        self.by_share = by_share\n\n        # List of variables used in by\n        self.byvars = []\n        # Columns to use in calibration\n        #   All, until restricted for full rank or minimum observation restrictions\n        self.columns = []\n\n        # Final model matrix for weighting\n        self.model_matrix = None\n        # Vector of derived moment scale factors\n        self.scale = None\n        # Vector of target values\n        self.targets = None\n        # Vector of non-zero counts\n        self.non_zero = None\n        # When weighting, carry the zero count of the target, too\n        self.non_zero_target = None\n\n        # Has the rank already been checked on this?\n        self.rank_checked = False\n\n        # Number of observations\n        self.n_observations = 0\n        # When weighting, placeholder for target nObs\n        self.n_observations_target = 0\n\n        self.nw_type = nw_type\n        #   Only do the processing, if data is passed in\n        if df is not None:\n            self.nw_type = NarwhalsType(df)\n            if len(self.index) == 0:\n                # Add an index, if there isn't one\n                #   We need one to be able to put the file back together again\n                sort_by = list_input(sort_by)\n\n                if len(sort_by) == 0:\n                    sort_by = (\n                        nw.from_native(df)\n                        .lazy_backend(self.nw_type)\n                        .collect_schema()\n                        .names()[0]\n                    )\n\n                self.index = [\"___rownumber\"]\n                self.df = (\n                    nw.from_native(self.df)\n                    .with_row_index(name=self.index[0], order_by=sort_by)\n                    .to_native()\n                )\n\n            #   Weights\n            if self.weight == \"\":\n                self.weight = \"___ones\"\n                self.df = (\n                    nw.from_native(self.df)\n                    .with_columns(nw.lit(1).alias(self.weight))\n                    .to_native()\n                )\n\n            #   Are there by groups that generate subgroup moments?\n            if type(self.by) is str and self.by != \"\":\n                self.byvars = FormulaBuilder.columns_from_formula(formula=self.by)\n            elif type(self.by) is list:\n                self.byvars = self.by\n\n            if not is_sub_moment and initial_processing:\n                self.initialize_moment(target_moment)\n\n    def initialize_moment(\n        self,\n        # Is this a target moment (i.e. a list of moment constraints)?\n        target_moment: bool = True,\n    ):\n        # Start the list of vars to keep with the vars in the formula\n        keepvars = FormulaBuilder.columns_from_formula(formula=self.formula)\n        keepvars.extend(self.index)\n        keepvars.append(self.weight)\n        keepvars.extend(self.byvars)\n\n        #   Restrict to the relevant observations and variables, as needed\n        self.df = nw.from_native(self.df).select(keepvars).to_native()\n\n        #   Do we need to calculate targets?\n        if target_moment:\n            self._get_by_wheres()\n\n            self._get_model_matrix()\n            self._get_targets()\n            self._create_sub_moments()\n\n    def _get_by_wheres(self):\n        if type(self.by) is str:\n            f_by = FormulaBuilder(df=self.df, formula=self.by)\n\n            f_by.remove_constant()\n            self.by = f_by.formula\n            df_by = nw.from_native(\n                formulaic.Formula(f_by.formula).get_model_matrix(\n                    nw.from_native(self.df)\n                    .lazy_backend(self.nw_type)\n                    .collect()\n                    .to_native()\n                )\n            )\n\n        elif type(self.by) is list:\n            if len(self.by):\n                df_by = nw.from_native(self.df).select(self.by)\n            else:\n                #   No by\n                return\n\n        #   For interactions, get the unique values of the variables\n        interactioncols = [\n            coli\n            for coli in df_by.lazy_backend(self.nw_type).collect_schema().names()\n            if coli.find(\":\") &gt;= 0\n        ]\n\n        if len(interactioncols) &gt; 0:\n            for interi in interactioncols:\n                subcols = interi.split(\":\")\n                df_inter = (\n                    df_by.select(subcols)\n                    .unique()\n                    .sort(subcols)\n                    .lazy_backend(self.nw_type)\n                    .collect()\n                )\n\n                for i, rowi in enumerate(df_inter.rows()):\n                    if i &gt; 0 or not self.keep_full_group:\n                        wherei = None\n\n                        for j, coli in enumerate(subcols):\n                            condi = nw.col(coli) == rowi[j]\n                            stringi = f\"{coli}=={rowi[j]}\"\n                            if wherei is None:\n                                wherei = condi\n                                wherei_string = stringi\n                            else:\n                                wherei = wherei &amp; condi\n                                wherei_string += f\"_{stringi}\"\n\n                        self.by_where_expressions.append(wherei)\n                        self.by_where_strings.append(wherei_string)\n\n            #   Now drop the interactions and the sub-items\n            droplist = []\n            for interi in interactioncols:\n                subcols = interi.split(\":\")\n\n                droplist.append(interi)\n                droplist.extend(subcols)\n\n            df_by = df_by.drop(list(set(droplist)))\n\n        #   Any non-interacted columns (remaining?)\n        for coli in df_by.lazy_backend(self.nw_type).collect_schema().names():\n            df_inter = (\n                df_by.select(coli)\n                .unique()\n                .sort(coli)\n                .lazy_backend(self.nw_type)\n                .collect()\n            )\n\n            for rowi in df_inter.rows():\n                condi = nw.col(coli) == rowi[0]\n                stringi = f\"{coli}=={rowi[0]}\"\n                self.by_where_expressions.append(condi)\n                self.by_where_strings.append(stringi)\n\n    def _get_model_matrix(self):\n        fb = FormulaBuilder(df=self.df, formula=self.formula)\n\n        fb.remove_constant()\n\n        model_matrix = nw.from_native(\n            formulaic.Formula(fb.formula).get_model_matrix(\n                nw.from_native(self.df).lazy_backend(self.nw_type).collect().to_native()\n            )\n        )\n\n        #   The dataframe only needs the weights and the index, now\n        keep_df = [self.weight]\n        keep_df.extend(self.index)\n        keep_df.extend(self.byvars)\n        self.df = nw.from_native(self.df).select(keep_df).to_native()\n\n        #   Append the index to the model matrix, so we can link to it\n        self.model_matrix = (\n            concat_wrapper(\n                [\n                    nw.from_native(self.df)\n                    .select(self.index)\n                    .lazy_backend(self.nw_type)\n                    .collect(),\n                    nw.from_native(model_matrix).lazy_backend(self.nw_type).collect(),\n                ],\n                how=\"horizontal\",\n            )\n            .lazy_backend(self.nw_type)\n            .to_native()\n        )\n\n    def _get_targets(self, non_zero_only: bool = False):\n        col_df = [self.weight]\n        col_df.extend(self.index)\n\n        df_targets = join_wrapper(\n            self.model_matrix,\n            nw.from_native(self.df).select(col_df),\n            on=self.index,\n            how=\"left\",\n        )\n\n        if not non_zero_only:\n            #   Get the targets themselves\n            summary_mean = column_stats_builder(\n                df=self.model_matrix, cols_exclude=self.index, stat=\"mean\"\n            )\n\n            self.targets = (\n                nw.from_native(\n                    calculate_by(\n                        df=df_targets,\n                        column_stats=summary_mean,\n                        weight=self.weight,\n                        no_suffix=True,\n                    )\n                )\n                .with_columns(nw.all() * self.by_share)\n                .to_native()\n            )\n\n        #   How many values are non-zero?\n        summary_nonzero = column_stats_builder(\n            df=self.model_matrix,\n            cols_include=\"*\",\n            cols_exclude=self.index,\n            stat=\"rawcount_not0\",\n        )\n\n        self.non_zero = calculate_by(\n            df=df_targets,\n            column_stats=summary_nonzero,\n            weight=self.weight,\n            no_suffix=True,\n        )\n\n    def _rescale_targets(self):\n        if self.rescale and self.targets is not None:\n            cols_scale = []\n            cols_targets = []\n\n            targets = nw.from_native(self.targets)\n\n            for coli in targets.lazy_backend(self.nw_type).collect_schema().names():\n                cols_scale.append((nw.col(coli) ** -1).alias(coli))\n                cols_targets.append(nw.lit(1).alias(coli))\n            self.scale = targets.with_columns(cols_scale).to_native()\n            self.targets = targets.with_columns(cols_targets).to_native()\n\n    def _create_sub_moments(self):\n        if len(self.by_where_expressions) &gt; 0:\n            # Need the ratio of by group weights to overall weights\n            if self.equalize_by:\n                if self.equalize_by_obs_share:\n                    total_obs = safe_height(self.df)\n\n                #   We need to get or load the full group targets for the subgroup, if we want to equalize them\n                if self.targets is None:\n                    # Get the targets of the parent moment without affecting the actual object (self)\n                    m_targets_only = deepcopy_with_fallback(self)\n                    m_targets_only._get_model_matrix()\n                    m_targets_only._get_targets()\n\n                    targets_equalize = m_targets_only.targets\n                    del m_targets_only\n                else:\n                    targets_equalize = self.targets\n\n        #   Make sure the weight sums do not overflow\n        self.df = safe_sum_cast(df=self.df, columns=[self.weight])\n        total_weight = (\n            nw.from_native(self.df)\n            .lazy_backend(self.nw_type)\n            .select(nw.col(self.weight).sum())\n            .collect()\n            .item(0, 0)\n        )\n\n        #   Don't recreate the model matrix/df, if we don't have to\n        if self.model_matrix is not None:\n            bykeep = []\n            bykeep.append(self.weight)\n            bykeep.extend(self.index)\n            bykeep.extend(self.byvars)\n            df_by = nw.from_native(self.df).lazy_backend(self.nw_type).select(bykeep)\n        else:\n            df_by = nw.from_native(self.df).lazy_backend(self.nw_type)\n\n        for i_by, byi in enumerate(self.by_where_expressions):\n            df_byi = df_by.filter(byi)\n\n            group_weight = (\n                df_byi.lazy_backend(self.nw_type)\n                .select(nw.col(self.weight).sum())\n                .collect()\n                .item(0, 0)\n            )\n\n            #  What is the share of the weight that should go to the\n            #      group identified by this byi\n            if self.equalize_by:\n                if self.equalize_by_obs_share:\n                    group_obs = safe_height(df_byi)\n                    by_share = group_obs / total_obs\n                elif self.equalize_by_weight_share:\n                    by_share = group_weight / total_weight\n                else:\n                    by_share = 1 / len(self.by_where_expressions)\n            else:\n                by_share = group_weight / total_weight\n\n            sub_moment = Moment(\n                df=df_byi,\n                nw_type=self.nw_type,\n                formula=self.formula,\n                weight=self.weight,\n                index=self.index,\n                rescale=self.rescale,\n                by_share=by_share,\n                is_sub_moment=True,\n            )\n\n            #  Get the model matrix by subsetting the parent model matrix\n            sub_moment.model_matrix = join_wrapper(\n                nw.from_native(sub_moment.df).select(sub_moment.index),\n                self.model_matrix,\n                how=\"left\",\n                on=self.index,\n            )\n\n            sub_moment._get_targets(non_zero_only=self.equalize_by)\n\n            if self.equalize_by:\n                sub_moment.targets = (\n                    nw.from_native(targets_equalize)\n                    .with_columns(nw.all() * by_share)\n                    .to_native()\n                )\n\n            sub_moment.by_where_expressions = [byi]\n            sub_moment.by_where_strings = [self.by_where_strings[i_by]]\n            self.sub_moments.append(sub_moment)\n\n        if len(self.by_where_expressions) == 0:\n            #   No sub_moments\n            self.n_observations = safe_height(self.df)\n\n        #   Rescale the Target moments, if necessary\n        self._rescale_targets()\n\n        for subi in self.sub_moments:\n            subi._rescale_targets()\n\n        #   Don't need df or model_matrix anymore\n        self.df = None\n        self.model_matrix = None\n        for subi in self.sub_moments:\n            subi.n_observations = safe_height(subi.df)\n            subi.df = None\n            subi.model_matrix = None\n\n        #   Get rid of the other dataframes if this isn't going to be\n        #       used as a moment to match to\n        if len(self.by_where_expressions) &gt; 0 and not self.keep_full_group:\n            self.targets = None\n            self.non_zero = None\n            self.scale = None\n\n    def rescaled_model_matrix(self, narrow: bool = False):\n        if narrow and len(self.columns) &gt; 0:\n            #   Only pass the subset of columns\n            df_out = nw.from_native(self.model_matrix).select(self.columns).to_native()\n        else:\n            df_out = nw.from_native(self.model_matrix).sort(self.index).to_native()\n\n        if self.rescale and self.scale is not None:\n            with_columns = []\n\n            cols_main = (\n                nw.from_native(df_out)\n                .lazy_backend(self.nw_type)\n                .collect_schema()\n                .names()\n            )\n            cols_scale = (\n                nw.from_native(self.scale)\n                .lazy_backend(self.nw_type)\n                .collect_schema()\n                .names()\n            )\n            for coli in set(cols_main).intersection(cols_scale):\n                if coli not in self.index:\n                    #   Get the scale adjustment from self.scale\n                    valuei = (\n                        nw.from_native(self.scale)\n                        .lazy_backend(self.nw_type)\n                        .select(coli)\n                        .collect()\n                        .item(0, 0)\n                    )\n                    with_columns.append((nw.col(coli) * valuei).alias(coli))\n\n            df_out = (\n                nw.from_native(df_out)\n                .lazy_backend(self.nw_type)\n                .with_columns(with_columns)\n                .to_native()\n            )\n\n        return df_out\n\n    #####################################################\n    #   Serializable - BEGIN\n    #####################################################\n    @classmethod\n    def _init_from_dict(cls, data: dict):\n        return super()._init_from_dict(data, initial_processing=False)\n\n    @classmethod\n    def load(\n        cls,\n        path: str = \"\",\n        delete: bool = False,\n        delete_only: bool = False,\n        **df_kwargs,\n    ) -&gt; Moment | None:\n        return super().load(\n            path=path, delete=delete, delete_only=delete_only, **df_kwargs\n        )\n</code></pre>"},{"location":"api/calibration/#survey_kit.calibration.calibration.Calibration","title":"Calibration","text":"<p>               Bases: <code>Serializable</code></p> <p>Survey calibration using entropy balancing methods.</p> <p>Calibration adjusts survey weights to match known population moments (targets) while  minimizing the distance from base weights.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Input dataframe containing the survey data to be calibrated.</p> required <code>moments</code> <code>list[Moment | str] | Moment | str | None</code> <p>Moment objects or paths to saved moments defining calibration targets. Can be a single Moment, list of Moments, or paths to saved Moment files. Default is None.</p> <code>None</code> <code>missing_to_zero</code> <code>bool</code> <p>Whether to fill missing values with zero before calibration. Default is True.</p> <code>True</code> <code>index</code> <code>list[str] | None</code> <p>Column names to use as unique identifiers for observations. If None, a row number index will be created. Default is None.</p> <code>None</code> <code>weight</code> <code>str</code> <p>Column name containing base weights. If empty string, uniform weights of 1 will be created. Default is \"\".</p> <code>''</code> <code>initial_guess</code> <code>str</code> <p>Column name for initial weight guesses to improve convergence. Default is \"\".</p> <code>''</code> <code>final_weight</code> <code>str</code> <p>Column name for output calibrated weights. If empty, defaults to \"___final_weight\". Default is \"\".</p> <code>''</code> <code>aggregation</code> <code>str</code> <p>How to combine multiple moments: \"Combined\" (all at once), \"Sequential\" (one at a time), or \"Both\" (sequential first, then combined if needed). Default is \"Combined\".</p> <code>'Combined'</code> <code>iterations</code> <code>int</code> <p>Maximum number of iterations for calibration algorithm. If -1, uses method-specific defaults (50 for aebw, 250 for legacy_ebw). Default is -1.</p> <code>-1</code> <code>iterations_loop</code> <code>int</code> <p>Maximum number of loops for sequential/trimmed calibration. Default is 20.</p> <code>20</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance for maximum difference between targets and estimates. Default is 0.00001.</p> <code>1e-05</code> <p>Attributes:</p> Name Type Description <code>df</code> <code>IntoFrameT</code> <p>Dataframe with calibrated weights in the final_weight column.</p> <code>moments</code> <code>list[Moment]</code> <p>List of Moment objects used for calibration.</p> <code>diagnostics_out</code> <code>dict | None</code> <p>Dictionary containing convergence status and diagnostic dataframe after running calibration.</p> <p>Examples:</p> <p>Basic calibration to match population totals:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from survey_kit.calibration import Calibration, Moment\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; df = pl.DataFrame({\n&gt;&gt;&gt;     \"id\": range(100),\n&gt;&gt;&gt;     \"age_group\": [\"18-34\", \"35-54\", \"55+\"] * 33 + [\"18-34\"],\n&gt;&gt;&gt;     \"region\": [\"North\", \"South\"] * 50,\n&gt;&gt;&gt;     \"base_weight\": [1.0] * 100\n&gt;&gt;&gt; })\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Define target moments\n&gt;&gt;&gt; age_moment = Moment(df=df, formula=\"C(age_group)\", weight=\"base_weight\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Run calibration\n&gt;&gt;&gt; c = Calibration(df=df, moments=[age_moment], weight=\"base_weight\")\n&gt;&gt;&gt; results = c.run()\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Get the calibrated weights (appended to the original data)\n&gt;&gt;&gt; calibrated_df = c.get_final_weights(df)\n</code></pre> <p>Multiple moments with sequential calibration:</p> <pre><code>&gt;&gt;&gt; region_moment = Moment(df=df, formula=\"C(region)\", weight=\"base_weight\")\n&gt;&gt;&gt; c = Calibration(\n&gt;&gt;&gt;     df=df, \n&gt;&gt;&gt;     moments=[age_moment, region_moment],\n&gt;&gt;&gt;     aggregation=\"Sequential\",\n&gt;&gt;&gt;     weight=\"base_weight\"\n&gt;&gt;&gt; )\n&gt;&gt;&gt; results = c.run(min_obs=10)\n</code></pre> <p>With weight trimming:</p> <pre><code>&gt;&gt;&gt; from survey_kit.calibration.trim import Trim\n&gt;&gt;&gt; trim_params = Trim(trim=True, min_val=0.2, max_val=3.0)\n&gt;&gt;&gt; results = c.run(trim=trim_params, min_obs=[0, 10, 50])\n</code></pre> Notes <ul> <li>The calibration preserves the total sum of weights while adjusting individual weight values to match target moments.</li> <li>Sequential aggregation calibrates to each moment one at a time, while combined aggregation solves for all moments simultaneously.</li> <li>The \"Both\" aggregation tries sequential first and falls back to combined if convergence criteria aren't met.</li> <li>Use min_obs parameter to exclude moments with too few non-zero observations, which can cause convergence issues.</li> </ul> Source code in <code>src\\survey_kit\\calibration\\calibration.py</code> <pre><code>class Calibration(Serializable):\n    \"\"\"\n    Survey calibration using entropy balancing methods.\n\n    Calibration adjusts survey weights to match known population moments (targets) while \n    minimizing the distance from base weights.\n\n    Parameters\n    ----------\n    df : IntoFrameT\n        Input dataframe containing the survey data to be calibrated.\n    moments : list[Moment | str] | Moment | str | None, optional\n        Moment objects or paths to saved moments defining calibration targets.\n        Can be a single Moment, list of Moments, or paths to saved Moment files.\n        Default is None.\n    missing_to_zero : bool, optional\n        Whether to fill missing values with zero before calibration. Default is True.\n    index : list[str] | None, optional\n        Column names to use as unique identifiers for observations. If None,\n        a row number index will be created. Default is None.\n    weight : str, optional\n        Column name containing base weights. If empty string, uniform weights\n        of 1 will be created. Default is \"\".\n    initial_guess : str, optional\n        Column name for initial weight guesses to improve convergence. Default is \"\".\n    final_weight : str, optional\n        Column name for output calibrated weights. If empty, defaults to\n        \"___final_weight\". Default is \"\".\n    aggregation : str, optional\n        How to combine multiple moments: \"Combined\" (all at once), \"Sequential\"\n        (one at a time), or \"Both\" (sequential first, then combined if needed).\n        Default is \"Combined\".\n    iterations : int, optional\n        Maximum number of iterations for calibration algorithm. If -1, uses\n        method-specific defaults (50 for aebw, 250 for legacy_ebw). Default is -1.\n    iterations_loop : int, optional\n        Maximum number of loops for sequential/trimmed calibration. Default is 20.\n    tolerance : float, optional\n        Convergence tolerance for maximum difference between targets and estimates.\n        Default is 0.00001.\n\n    Attributes\n    ----------\n    df : IntoFrameT\n        Dataframe with calibrated weights in the final_weight column.\n    moments : list[Moment]\n        List of Moment objects used for calibration.\n    diagnostics_out : dict | None\n        Dictionary containing convergence status and diagnostic dataframe after\n        running calibration.\n\n    Examples\n    --------\n    Basic calibration to match population totals:\n\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from survey_kit.calibration import Calibration, Moment\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Create sample data\n    &gt;&gt;&gt; df = pl.DataFrame({\n    &gt;&gt;&gt;     \"id\": range(100),\n    &gt;&gt;&gt;     \"age_group\": [\"18-34\", \"35-54\", \"55+\"] * 33 + [\"18-34\"],\n    &gt;&gt;&gt;     \"region\": [\"North\", \"South\"] * 50,\n    &gt;&gt;&gt;     \"base_weight\": [1.0] * 100\n    &gt;&gt;&gt; })\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Define target moments\n    &gt;&gt;&gt; age_moment = Moment(df=df, formula=\"C(age_group)\", weight=\"base_weight\")\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Run calibration\n    &gt;&gt;&gt; c = Calibration(df=df, moments=[age_moment], weight=\"base_weight\")\n    &gt;&gt;&gt; results = c.run()\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Get the calibrated weights (appended to the original data)\n    &gt;&gt;&gt; calibrated_df = c.get_final_weights(df)\n\n    Multiple moments with sequential calibration:\n\n    &gt;&gt;&gt; region_moment = Moment(df=df, formula=\"C(region)\", weight=\"base_weight\")\n    &gt;&gt;&gt; c = Calibration(\n    &gt;&gt;&gt;     df=df, \n    &gt;&gt;&gt;     moments=[age_moment, region_moment],\n    &gt;&gt;&gt;     aggregation=\"Sequential\",\n    &gt;&gt;&gt;     weight=\"base_weight\"\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; results = c.run(min_obs=10)\n\n    With weight trimming:\n\n    &gt;&gt;&gt; from survey_kit.calibration.trim import Trim\n    &gt;&gt;&gt; trim_params = Trim(trim=True, min_val=0.2, max_val=3.0)\n    &gt;&gt;&gt; results = c.run(trim=trim_params, min_obs=[0, 10, 50])\n\n    Notes\n    -----\n    - The calibration preserves the total sum of weights while adjusting individual\n    weight values to match target moments.\n    - Sequential aggregation calibrates to each moment one at a time, while combined\n    aggregation solves for all moments simultaneously.\n    - The \"Both\" aggregation tries sequential first and falls back to combined if\n    convergence criteria aren't met.\n    - Use min_obs parameter to exclude moments with too few non-zero observations,\n    which can cause convergence issues.\n    \"\"\"    \n\n\n    _save_suffix = \"calibration\"\n    _save_exclude_items = [\"nw_type\"]\n\n\n    def __init__(\n        self,\n        df: IntoFrameT,\n        moments: list[Moment | str] | Moment | str | None = None,\n        missing_to_zero: bool = True,\n        index: list[str] | None = None,\n        weight: str = \"\",\n        initial_guess: str = \"\",\n        final_weight: str = \"\",\n        aggregation: str = \"Combined\",\n        iterations: int = -1,\n        iterations_loop: int = 20,\n        tolerance: float = 0.00001,\n    ):\n        self.nw_type = NarwhalsType(df)\n\n        moments = list_input(moments)\n        index = list_input(index)\n\n        #   Pending other optimizations?\n        method = \"aebw\"\n\n        acceptableMethods = [\"aebw\", \"legacy_ebw\"]\n        bAcceptableMethods = method in acceptableMethods\n\n        acceptableAggregations = [\"Combined\", \"Sequential\", \"Both\"]\n        bAcceptableAggregation = aggregation in acceptableAggregations\n\n        if not bAcceptableMethods:\n            message = f\"ONLY {', '.join(acceptableMethods)} ARE ACCEPTABLE METHODS - passed({method})\"\n\n            logger.error(message)\n            raise Exception(message)\n\n        if not bAcceptableAggregation:\n            message = f\"ONLY {', '.join(acceptableAggregations)} ARE ACCEPTABLE METHODS - passed({aggregation})\"\n\n            logger.error(message)\n            raise Exception(message)\n\n        if missing_to_zero and df is not None:\n            df = fill_missing(df=df, value=0)\n\n        df = nw.from_native(df)\n        #   Check that base weight is &gt; 0\n        if weight != \"\":\n            c_weight = nw.col(weight)\n            c_invalid_weights = c_weight.le(0) | c_weight.is_null()\n\n            n_invalid = safe_height(\n                nw.from_native(df).lazy().select(weight).filter(c_invalid_weights)\n            )\n            if n_invalid:\n                logger.info(\n                    f\"  Dropping {n_invalid} observation(s) with invalid base weights (&lt;= 0 or missing)\"\n                )\n                df = df.filter(~c_invalid_weights)\n        self.df = df.lazy_backend(self.nw_type)\n\n        # Add an index, if there isn't one\n        #   We need one to be able to put the file back together again\n\n        if len(index) == 0:\n            self.index = [\"___rownumber\"]\n            self.df = (\n                self.df.collect()\n                .with_row_index(name=self.index[0])\n                .lazy_backend(self.nw_type)\n            )\n        else:\n            self.index = index\n\n        if weight == \"\":\n            weight = \"___ones\"\n            self.df = self.df.with_columns(nw.lit(1).alias(weight))\n        self.weight = weight\n        self.initial_guess = initial_guess\n\n        if final_weight == \"\":\n            final_weight = \"___final_weight\"\n        self.final_weight = final_weight\n        self.df = self.df.with_columns(nw.col(self.weight).alias(self.final_weight))\n\n        self.aggregation = aggregation\n        self.method = method\n\n        #   Default values if not passed\n        if iterations &lt; 0:\n            if self.method == \"aebw\":\n                iterations = 50\n            else:\n                iterations = 250\n\n        self.iterations = iterations\n        self.tolerance = tolerance\n        self.iterations_loop = iterations_loop\n\n        # Process each \"target\" moment passed in\n        #   and create a paired \"weight\" moment object\n        self.moments = []\n        if len(moments) &gt; 0:\n            for momenti in moments:\n                self.moments.append(self.process_single_moment(momenti))\n\n        #   Placeholder for combined moments when running both sequential and combined\n        self.c_combined = None\n\n        #   Placeholder for post-run diagnostics information\n        self.diagnostics_out = None\n\n    def process_single_moment(\n        self, m_input: Moment | str, df: IntoFrameT | None = None\n    ) -&gt; Moment:\n        # Clone the moment object - the processed one will be based on the data to calibrate,\n        #                           the input one is based on the data passed originally\n        #                           which need not be the same (something should generally differ...)\n        if type(m_input) is str:\n            m = Moment.load(m_input)\n        else:\n            m = deepcopy_with_fallback(m_input)\n\n        m.non_zero_target = m_input.non_zero\n        m.n_observations_target = m_input.n_observations\n\n        if df is None:\n            m.df = self.df\n        else:\n            m.df = df\n\n        m.weight = self.weight\n\n        m.index = self.index\n\n        m.initialize_moment(target_moment=False)\n\n        #   Does this moment have any targets?\n        if m.targets is not None:\n            m._get_model_matrix()\n            m._get_targets(non_zero_only=True)\n            m.n_observations = safe_height(m.model_matrix)\n\n        #   Process any submoments\n        if len(m.sub_moments) &gt; 0:\n            sub_moments = []\n            for subi in m.sub_moments:\n                subi = self.process_single_moment(subi, m.df)\n\n                sub_moments.append(subi)\n\n            m.sub_moments = sub_moments\n\n        self.compare_moment_variables(m_target=m_input, m_weight=m)\n\n        return m\n\n    def compare_moment_variables(self, m_target: Moment, m_weight: Moment) -&gt; None:\n        \"\"\"\n        Do the variables match across source and weighting moments?\n           Won't if a value doesn't exist in one data set that does in the other\n           If don't match, set to the intersection of variables - can only weight to the things in both samples\n\n        Parameters\n        ----------\n        m_target : Moment\n            Input moment with targets calculated\n        m_weight : Moment, optional\n            moment for calibration\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        if m_target.targets is not None and m_weight.model_matrix is not None:\n            cols_target = (\n                nw.from_native(m_target.targets).lazy().collect_schema().names()\n            )\n            cols_weight = (\n                nw.from_native(m_weight.model_matrix).lazy().collect_schema().names()\n            )\n            m_weight.columns = list(set(cols_target).intersection(cols_weight))\n\n    def censor_to_min_obs(self, min_obs: int = 0) -&gt; None:\n        \"\"\"\n        In calibration, do we want to limit to moments with greater than some\n           threshold of observations where x != 0\n\n        Parameters\n        ----------\n        min_obs : int, optional\n            The minimum number of != 0 observations.\n            The default is 0. (don't censor)\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        for momenti in self.moments:\n            self._censort_to_min_obs_by_moment(min_obs=min_obs, m=momenti)\n\n            for subi in momenti.sub_moments:\n                self._censort_to_min_obs_by_moment(min_obs=min_obs, m=subi)\n\n        if self.c_combined is not None:\n            self.c_combined.censor_to_min_obs(min_obs=min_obs)\n\n    def _censort_to_min_obs_by_moment(self, min_obs: int, m: Moment) -&gt; None:\n        \"\"\"\n        For each moment, censor, if needed\n\n        Parameters\n        ----------\n        min_obs : int\n            Minimum non-zero observations to censor at\n        m : Moment\n            Moment to censor\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        if m.non_zero_target is not None and len(m.columns) &gt; 0:\n            nw_type = NarwhalsType(m.non_zero_target)\n            nonzero_cols = (\n                nw_type.to_polars().select(m.columns).transpose(include_header=True)\n            )\n\n            m.columns = nonzero_cols.filter(\n                pl.col(nonzero_cols.lazy().collect_schema().names()[1]) &gt;= min_obs\n            )[nonzero_cols.lazy().collect_schema().names()[0]].to_list()\n\n        if m.non_zero is not None and len(m.columns) &gt; 0:\n            nw_type = NarwhalsType(m.non_zero)\n\n            nonzero_cols = (\n                nw_type.to_polars().select(m.columns).transpose(include_header=True)\n            )\n\n            m.columns = nonzero_cols.filter(\n                pl.col(nonzero_cols.lazy().collect_schema().names()[1]) &gt;= min_obs\n            )[nonzero_cols.lazy().collect_schema().names()[0]].to_list()\n\n    def combine_moments(self, all: bool = False, sub_moments: bool = True) -&gt; None:\n        \"\"\"\n        To simultaneously weight to lots of moments, we need to combine them\n            This directly edits the items in self.moments\n\n        Parameters\n        ----------\n        all : bool, optional\n            Combine all or just submoments into parent moments?\n            The default is False.\n        sub_moments : bool, optional\n            Combine submoments?\n            The default is True.\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if all or sub_moments:\n            logger.info(\"Aggregating any sub_moments\")\n            for i in range(len(self.moments)):\n                if len(self.moments[i].sub_moments) &gt; 0:\n                    self.moments[i] = self._combine_moment_with_sub_moments(\n                        self.moments[i]\n                    )\n\n        if all and len(self.moments) &gt; 0:\n            #   Check that submoments are combined\n            for momenti in self.moments:\n                if len(momenti.sub_moments) &gt; 0:\n                    message = \"Must combine sub moments before combining all moments\"\n                    logger.error(message)\n                    raise Exception(message)\n            #   Merge aggregate the moments together into one\n            self.moments = [\n                self._combine_moment_list(m_list=self.moments, with_count_prefix=True)\n            ]\n\n    def _combine_moment_with_sub_moments(self, m: Moment) -&gt; Moment:\n        \"\"\"\n        Combine this moment and its submoments (i.e. moments by state)\n            into one moment for weighting\n\n        Parameters\n        ----------\n        m : Moment\n            Input moment\n\n        Returns\n        -------\n        Moment\n            The combined moment with no submoments\n        \"\"\"\n\n        if len(m.sub_moments) == 0:\n            # No submoments\n            return m\n        else:\n            # Combine this with sub, adding by prefix to submoments only\n            m_list = [m]\n            prefix_list = [False]\n\n            for subi in m.sub_moments:\n                m_list.append(subi)\n                prefix_list.append(True)\n\n            m = self._combine_moment_list(m_list=m_list, with_by_prefix=prefix_list)\n        return m\n\n    def _combine_moment_list(\n        self,\n        m_list: list[Moment],\n        with_by_prefix: list[bool] | bool = None,\n        with_count_prefix: bool = False,\n    ) -&gt; Moment:\n        #   Combined moment\n        m_combined = None\n\n        for i_moment, m in enumerate(m_list):\n            if with_by_prefix is None:\n                with_by_prefixi = False\n            elif type(with_by_prefix) is list:\n                with_by_prefixi = with_by_prefix[i_moment]\n            else:\n                with_by_prefixi = with_by_prefix\n\n            #   Add the prefix?\n            prefix = self._combine_prefix(\n                with_by_prefix=with_by_prefixi,\n                with_count_prefix=with_count_prefix,\n                count_value=i_moment,\n                m=m,\n            )\n\n            if with_by_prefixi:\n                sub_moment_column_name = \"_in\"\n            else:\n                sub_moment_column_name = \"\"\n\n            m_combined = self._combine_moments_concatenate_data(\n                m_add=m,\n                m_out=m_combined,\n                sub_moment_column_name=sub_moment_column_name,\n                prefix=prefix,\n            )\n\n        #   Fill in the missing values left by submoment subgroups\n        m_combined.model_matrix = fill_missing(m_combined.model_matrix, value=0)\n\n        #   m_combined.df = m_combined.df.sort(self.index)\n        #   m_combined.ModelMatrix = m_combined.ModelMatrix.sort(self.index)\n\n        m_combined.by_where_expressions = []\n        m_combined.sub_moments = []\n\n        return m_combined\n\n    def _combine_prefix(\n        self,\n        with_by_prefix: bool = False,\n        with_count_prefix: bool = False,\n        count_value: int = 0,\n        m: Moment = None,\n    ):\n        prefix = \"\"\n        if with_by_prefix:\n            prefix = f\"{m.by_where_strings[0]}:\"\n        if with_count_prefix:\n            prefix = f\"m{count_value}_{prefix}\"\n\n        return prefix\n\n    #   Actually put them moments together\n    def _combine_moments_concatenate_data(\n        self,\n        m_add: Moment | None = None,\n        m_out: Moment | None = None,\n        sub_moment_column_name: str = \"\",\n        prefix: str = \"\",\n    ) -&gt; Moment:\n        #   NonZero always should exist if this is actually a moment to be estimated\n        #       Rather than just a parent to submoments,\n        if m_add.non_zero is None:\n            #   if it's null, don't add anything from this and just return the combined moment\n            return m_out\n\n        #   NonZero always should exist if this is actually a moment to be estimated\n        #       so get the list of column names from it\n        columns_to_add = nw.from_native(m_add.non_zero).lazy().collect_schema().names()\n\n        #   Nothing to add?, if so, return m_out as we're doing nothing here\n        if len(columns_to_add) == 0:\n            return m_out\n\n        # Add the dummy for being in this submoment\n        if sub_moment_column_name != \"\":\n            #   Non-zero count is the number of observations in group\n            m_add.non_zero = (\n                nw.from_native(m_add.non_zero)\n                .with_columns(\n                    nw.lit(m_add.n_observations).alias(sub_moment_column_name)\n                )\n                .to_native()\n            )\n\n            m_add.non_zero_target = (\n                nw.from_native(m_add.non_zero_target)\n                .with_columns(\n                    nw.lit(m_add.n_observations_target).alias(sub_moment_column_name)\n                )\n                .to_native()\n            )\n\n            #   Target is share in this Moment (set to 1 as it should be ByShare, which it will be multiplied by )\n            if m_add.rescale:\n                m_add.targets = (\n                    nw.from_native(m_add.targets)\n                    .with_columns(nw.lit(1).alias(sub_moment_column_name))\n                    .to_native()\n                )\n            else:\n                m_add.targets = (\n                    nw.from_native(m_add.targets)\n                    .with_columns(nw.lit(m_add.by_share).alias(sub_moment_column_name))\n                    .to_native()\n                )\n\n            #   No scaling needed\n            if m_add.scale is not None:\n                m_add.scale = nw.from_native(m_add.scale).with_columns(\n                    nw.lit(m_add.by_share).alias(sub_moment_column_name)\n                )\n\n            #   Model matrix - add dummy (1) as this\n            if m_add.model_matrix is not None:\n                if m_add.rescale:\n                    m_add.model_matrix = (\n                        nw.from_native(m_add.model_matrix)\n                        .with_columns(\n                            nw.lit(1 / (m_add.by_share**2)).alias(\n                                sub_moment_column_name\n                            )\n                        )\n                        .to_native()\n                    )\n                else:\n                    m_add.model_matrix = (\n                        nw.from_native(m_add.model_matrix)\n                        .with_columns(nw.lit(1).alias(sub_moment_column_name))\n                        .to_native()\n                    )\n\n            m_add.columns.append(sub_moment_column_name)\n\n        #   Add the prefix to all the column names\n        if prefix != \"\":\n            for itemi in [\n                \"model_matrix\",\n                \"targets\",\n                \"scale\",\n                \"non_zero\",\n                \"non_zero_target\",\n            ]:\n                dfi = getattr(m_add, itemi)\n                if dfi is not None:\n                    setattr(\n                        m_add,\n                        itemi,\n                        rename_with_prefix_suffix(\n                            df=dfi, prefix=prefix, exclude_list=m_add.index\n                        ),\n                    )\n\n        #   Also to the list of columns to be calibrated against\n        m_add.columns = [prefix + coli for coli in m_add.columns]\n        m_add.by_share = 1\n        #   If m_out is None, we're done, and m_add is the \"combined\" moment\n        if m_out is None:\n            return m_add\n\n        #   If m_out is not None, we need to combined m_out and m_add\n\n        #   Update the name of the added columns, if anything was changed above\n        columns_to_add = nw.from_native(m_add.non_zero).lazy().collect_schema().names()\n\n        #   Combine the \"data\" (df and ModelMatrix)\n        if m_out.df is None:\n            m_out.df = m_add.df\n            m_out.model_matrix = m_add.model_matrix\n        else:\n            df_add = (\n                nw.from_native(\n                    join_wrapper(\n                        m_add.df,\n                        nw.from_native(m_out.df)\n                        .select(m_add.index)\n                        .with_columns(nw.lit(1).alias(\"bExist\"))\n                        .to_native(),\n                        on=m_add.index,\n                        how=\"left\",\n                    )\n                )\n                .filter(nw.col(\"bExist\").is_missing())\n                .drop(\"bExist\")\n                .to_native()\n            )\n\n            m_out.df = concat_wrapper([m_out.df, df_add], how=\"diagonal\")\n\n            m_out.model_matrix = join_wrapper(\n                m_out.model_matrix, m_add.model_matrix, on=m_add.index, how=\"full\"\n            )\n\n        #   The summary data\n        for itemi in [\"targets\", \"scale\", \"non_zero\", \"non_zero_target\"]:\n            dfi_add = getattr(m_add, itemi)\n            dfi_out = getattr(m_out, itemi)\n            if dfi_out is None:\n                setattr(m_out, itemi, dfi_add)\n            else:\n                setattr(\n                    m_out, itemi, concat_wrapper([dfi_out, dfi_add], how=\"horizontal\")\n                )\n\n        m_out.columns.extend(m_add.columns)\n\n        return m_out\n\n    def run(\n        self,\n        min_obs: int | list[int] = 0,\n        skip_setup: bool = False,\n        print_diagnostics: bool = True,\n        skip_diagnostics: bool = False,\n        trim: Trim | None = None,\n        **additional_params,\n    ) -&gt; dict:\n        \"\"\"\n        Run the calibration to estimate the weights from the moments.\n\n        Parameters\n        ----------\n        min_obs : int | list[int], optional\n            Limit the moments to those with more than some number of\n            non-zero observations.\n            You can pass a list of increasing values so that if\n            the model doesn't converge with the lowest number\n            (the most constraints), then try again dropping\n            constraints with more non-zero observations and try again.\n            I.e. if you pass [0,10,100], it will see if the model converges\n            with min_obs = 0, if not, it will try with min_obs = 10, etc.\n            The default is 0.\n        print_diagnostics : bool, optional\n            Print the diagnostics (do the weights match the target moments)?\n            The default is True.\n        skip_diagnostics : bool, optional\n            Don't even calculate the diagnostics?\n            They can take a while to calculate, but\n            you really shouldn't skip this.\n            The default is False.\n        trim : Trim | None, optional\n            Pass a Trim object with bounds on the weights.\n            The default is None.\n        **additional_params : dict\n            Any additional params that are specific to a\n            calibration algorithm.\n\n        Returns\n        -------\n        dict\n            A dictionary with diagnostics information including:\n\n            - 'converged' : bool\n                Whether the calibration converged.\n            - 'max_diff' : float\n                Maximum difference between targets and estimates (if diagnostics calculated).\n            - 'diagnostics' : DataFrame\n                Detailed diagnostics by moment (if skip_diagnostics=False).\n\n        Examples\n        --------\n        Basic calibration run::\n\n            diagnostics = c.run(min_obs=50)\n            print(f\"Converged: {diagnostics['converged']}\")\n            print(f\"Max difference: {diagnostics['max_diff']}\")\n\n        Run with fallback min_obs levels::\n\n            # Try with no minimum first, then 10, then 100 if previous fails\n            diagnostics = c.run(min_obs=[0, 10, 100])\n\n        Run with weight trimming::\n\n            trim_params = Trim(trim=True, min_val=0.1, max_val=5.0)\n            diagnostics = c.run(trim=trim_params)\n        \"\"\"\n\n        #  Is min_obs a list of values?\n        #       If so, run this recursively on the remaining obs, if it doesn't converge on this one\n        min_obs_remaining = None\n        if type(min_obs) is list:\n            if len(min_obs) &gt; 1:\n                min_obs_remaining = min_obs[1 : len(min_obs)]\n                min_obs = min_obs[0]\n            else:\n                min_obs = min_obs[0]\n                min_obs_remaining = None\n\n        if min_obs &gt; 0:\n            self.censor_to_min_obs(min_obs=min_obs)\n\n        #   Need to loop if trimmed\n        #   bLoopNeeded = self.Trimmed\n        bLoopNeeded = False\n\n        if self.aggregation == \"Sequential\":\n            #   Sequential - need loop\n            bLoopNeeded = True\n        elif self.aggregation == \"Combined\":\n            if not skip_setup:\n                self.combine_moments(all=True, sub_moments=True)\n        elif self.aggregation == \"Both\":\n            if not skip_setup:\n                #   Clone this and create a version with the moments combined\n                self.c_combined = deepcopy_with_fallback(self)\n                self.c_combined.aggregation = \"Combined\"\n                # Do not trim within the combined calibration (handled in self)\n                #   self.c_combined.Trimmed = False\n\n                self.c_combined.combine_moments(all=True, sub_moments=True)\n\n        logger.info(\"Calibrating weights using \" + self.method)\n        if min_obs &gt; 0:\n            logger.info(f\"      min obs = {min_obs}\")\n\n        if bLoopNeeded:\n            n_loops = self.iterations_loop\n        else:\n            n_loops = 1\n\n        #   Loop counter\n        iLoop = 0\n\n        #   Within-loop completion flag\n        b_complete = False\n        diagnostics = None\n\n        while (iLoop &lt; n_loops) and not b_complete:\n            if n_loops &gt; 1:\n                logger.info(\"\\n\\n\\n\\nRunning loop #\" + str(iLoop + 1))\n\n            diagnostics = self._run_one_loop(trim=trim, **additional_params)\n            converged = diagnostics[\"converged\"]\n\n            if \"diagnostics\" not in list(diagnostics.keys()):\n                #   Not full diagnostics, set to null to load later\n                diagnostics = None\n\n            #   Default to complete unless set to False below\n            b_complete = True\n            if trim is not None:\n                #   Need trimming?\n                #       Don't trim if aebw without separately passed bounds\n                if trim.trim and (\n                    self.method != \"aebw\" and \"bounds\" in additional_params.keys()\n                ):\n                    b_complete = trim.trim_in_loop(c=self, iLoop=iLoop, n_loops=n_loops)\n\n            if self.aggregation == \"Sequential\":\n                # Check the max deviation against the tolerance\n                diagnostics = self.diagnostics()\n                max_diff = diagnostics[\"max_diff\"]\n\n                converged = max_diff &lt;= self.Tolerance_Loop\n                b_complete = converged\n\n            iLoop += 1\n\n        if not skip_diagnostics:\n            if diagnostics is None:\n                diagnostics = self.diagnostics()\n\n            if n_loops == 1:\n                b_complete = diagnostics[\"max_diff\"] &lt;= self.tolerance\n            else:\n                b_complete = diagnostics[\"max_diff\"] &lt;= self.tolerance\n\n            if not (b_complete or converged) and (min_obs_remaining is not None):\n                #   Recursively call this again, if needed\n\n                #   Reset the \"final weight\" (current weight) to the original one passed\n                #       (ignores the failed run's output)\n                self.df = (\n                    nw.from_native(self.df)\n                    .with_columns(nw.col(self.weight).alias(self.final_weight))\n                    .to_native()\n                )\n\n                diagnostics = self.run(\n                    min_obs=min_obs_remaining,\n                    skip_setup=True,\n                    print_diagnostics=print_diagnostics,\n                    skip_diagnostics=skip_diagnostics,\n                    **additional_params,\n                )\n\n            else:\n                diagnostics[\"converged\"] = converged\n\n            if \"diagnostics\" in diagnostics.keys():\n                diagnostics[\"diagnostics\"] = (\n                    nw.from_native(diagnostics[\"diagnostics\"])\n                    .lazy_backend(self.nw_type)\n                    .to_native()\n                )\n            self.diagnostics_out = diagnostics\n\n            if print_diagnostics:\n                self.print_diagnostics()\n\n        return diagnostics\n\n    def censor_to_min_obs(self, min_obs: int = 0) -&gt; None:\n        \"\"\"\n        In calibration, do we want to limit to moments with greater than some\n           threshold of observations where x != 0\n\n        Parameters\n        ----------\n        min_obs : int, optional\n            The minimum number of != 0 observations.\n            The default is 0. (don't censor)\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        for momenti in self.moments:\n            self._censor_to_min_obs_by_moment(min_obs=min_obs, m=momenti)\n\n            for subi in momenti.sub_moments:\n                self._censor_to_min_obs_by_moment(min_obs=min_obs, m=subi)\n\n        if self.c_combined is not None:\n            self.c_combined.censor_to_min_obs(min_obs=min_obs)\n\n    def _censor_to_min_obs_by_moment(self, min_obs: int, m: Moment) -&gt; None:\n        \"\"\"\n        For each moment, censor, if needed\n\n        Parameters\n        ----------\n        min_obs : int\n            Minimum non-zero observations to censor at\n        m : Moment\n            Moment to censor\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        if m.non_zero_target is not None and len(m.columns) &gt; 0:\n            nw_type = NarwhalsType(m.non_zero_target)\n            nonzero_cols = (\n                nw_type.to_polars().select(m.columns).transpose(include_header=True)\n            )\n\n            m.columns = nonzero_cols.filter(\n                pl.col(nonzero_cols.lazy().collect_schema().names()[1]) &gt;= min_obs\n            )[nonzero_cols.lazy().collect_schema().names()[0]].to_list()\n\n        if m.non_zero is not None and len(m.columns):\n            nw_type = NarwhalsType(m.non_zero)\n            nonzero_cols = (\n                nw_type.to_polars().select(m.columns).transpose(include_header=True)\n            )\n\n            m.columns = nonzero_cols.filter(\n                pl.col(nonzero_cols.lazy().collect_schema().names()[1]) &gt;= min_obs\n            )[nonzero_cols.lazy().collect_schema().names()[0]].to_list()\n\n    def _run_one_loop(self, trim: Trim | None = None, **additional_params):\n        if additional_params is None:\n            additional_params = {}\n\n        #   Delegate for actual calibration call\n        fCalibrate = None\n\n        additional_list = []\n\n        if self.method == \"legacy_ebw\":\n            fCalibrate = self._legacy_ebw\n\n        elif self.method == \"aebw\":\n            fCalibrate = self._accelerated_ebw_moment\n\n            #   Bound ratio adjustment to ebw\n            additional_list.append(\"bounds\")\n\n            #   Constraint violation penalty parameter\n            additional_list.append(\"eta\")\n\n            #   If passed, Sanders's version of Hainmueller's EBW\n            additional_list.append(\"dual_only\")\n            additional_list.append(\"dense\")\n            additional_list.append(\"scale_weights_to_n\")\n            additional_list.append(\"fallback_bounded\")\n            additional_list.append(\"fallback_iterations\")\n\n            if trim is not None:\n                if \"bounds\" in additional_params.keys():\n                    logger.info(\"Bounds passed directly, ignoring trim parameters: \")\n                    logger.info(trim)\n                else:\n                    logger.info(\"Do I want to pass trim bounds to the calibration?\")\n                    # additional_params[\"bounds\"] = (trim.min_val,\n                    #                                trim.max_val)\n\n        #   Keep only relevant parameters\n        additional_params = {\n            keyi: valuei\n            for keyi, valuei in additional_params.items()\n            if keyi in additional_list\n        }\n\n        #   Create an empty diagnostics variable - some aggregations populate this\n        diagnostics = None\n\n        if self.aggregation == \"Combined\":\n            converged = self._run_combined(fCalibrate=fCalibrate, **additional_params)\n\n        elif self.aggregation == \"Sequential\":\n            self._run_sequential(fCalibrate=fCalibrate, **additional_params)\n\n            #   This is just passed, convergence is checked later\n            #       through the diagnostics\n            converged = True\n\n        elif self.aggregation == \"Both\":\n            self._run_sequential(fCalibrate=fCalibrate, **additional_params)\n            diagnostics = self.diagnostics()\n\n            if diagnostics[\"max_diff\"] &gt; self.Tolerance:\n                logger.info(\n                    \"     Did not converge for sequential, running combined calibration\"\n                )\n\n                cols_df = []\n                cols_df.extend(self.index)\n                cols_df.append(self.final_weight)\n                self.c_combined.df = join_wrapper(\n                    nw.from_native(self.c_combined.df)\n                    .drop(self.final_weight)\n                    .to_native(),\n                    nw.from_native(self.df).select(cols_df).to_native(),\n                    on=self.index,\n                    how=\"left\",\n                )\n\n                self.c_combined.run(**additional_params)\n                #   Get the final diagnostics from the combined run\n                diagnostics = self.c_combined.diagnostics_out\n\n                # Update the weights back\n                self.df = join_wrapper(\n                    nw.from_native(self.df).drop(self.final_weight).to_native(),\n                    nw.from_native(self.c_combined.df).select(cols_df).to_native(),\n                    on=self.index,\n                    how=\"left\",\n                )\n            else:\n                logger.info(\n                    \"     Converged for sequential, skipping combined calibration\"\n                )\n\n        if diagnostics is None:\n            diagnostics = {\"converged\": converged}\n\n        return diagnostics\n\n    def _run_combined(self, fCalibrate=None, **additional_params):\n        m = self.moments[0]\n\n        if m.model_matrix is not None:\n            logger.info(\"     Calibration using combined moments\")\n            converged = fCalibrate(m=m, **additional_params)\n\n        return converged\n\n    def _run_sequential(self, fCalibrate=None, **additional_params):\n        for i_moment, m in enumerate(self.moments):\n            logger.info(f\"    Moment: {i_moment + 1}\")\n\n            if m.ModelMatrix is not None:\n                converged = fCalibrate(m=m, sequential=True, **additional_params)\n\n            for i_sub, subi in enumerate(m.sub_moments):\n                logger.info(f\"    Moment: {i_moment}.{i_sub + 1}\")\n\n                if subi.model_matrix is not None:\n                    converged = fCalibrate(m=subi, sequential=True, **additional_params)\n\n        return converged\n\n    def _legacy_ebw(self, m: Moment = None, sequential=False):\n        [initial_guess, base_weight, xi, targetsi, tol_adj] = self._moment_inputs(\n            m=m, sequential=sequential\n        )\n\n        nw_type = NarwhalsType(xi)\n\n        #   Initial weights\n        if base_weight is None:\n            base_weight = (\n                nw.from_native(initial_guess).lazy().collect().to_numpy().ravel()\n            )\n            initial_guess = None\n\n            #   aebw_options[\"initial_ratio_guess\"] = base_weight\n        else:\n            base_weight = (\n                nw.from_native(base_weight).lazy().collect().to_numpy().ravel()\n            )\n            initial_guess = (\n                nw.from_native(initial_guess).lazy().collect().to_numpy().ravel()\n            )\n\n            #   aebw_options[\"initial_ratio_guess\"] = initial_guess\n\n        xi = (\n            nw.from_native(xi)\n            .with_columns(\n                [cs.boolean().cast(nw.Float32), cs.numeric().cast(nw.Float64)]\n            )\n            .to_native()\n        )\n\n        xi = (\n            nw.from_native(xi)\n            .with_columns(nw.all().cast(nw.Float64))\n            .lazy()\n            .collect()\n            .to_numpy()\n        )\n\n        targetsi = nw.from_native(targetsi).lazy().collect().to_numpy().ravel()\n\n        b_converged = False\n\n        try:\n            results = legacy_calibration(\n                mean_population_moments=targetsi,\n                x_sample=xi,\n                weights0=base_weight,\n                penalty_fn=\"log_diff\",\n                rank_regularization_term=self.tolerance * tol_adj,\n                initial_guess=initial_guess,\n                n_iterations=self.iterations,\n            )\n\n            #   Make sure the print log goes with the code that executed it (rather than at the end of the log)\n            print(\"\", flush=True)\n\n            b_converged = results[1] == \"success\"\n\n            w_out = nw.from_native(\n                nw.from_arrow(\n                    pl.from_numpy(\n                        results[0], schema={\"column_0\": pl.Float64}\n                    ).to_arrow(),\n                    backend=backend_eager(nw_type.backend),\n                )\n            )\n\n        except Exception as e:\n            logger.error(e)\n\n        if w_out is not None:\n            self._merge_on_weights(m=m, weights=w_out, sequential=sequential)\n\n        return b_converged\n\n    def _accelerated_ebw_moment(\n        self,\n        m: Moment,\n        sequential: bool = False,\n        dense: bool = True,\n        bounds: tuple[float, float] | None = None,\n        #  eta=None,\n        fallback_bounded: bool = False,\n        fallback_iterations: int = -1,\n        dual_only: bool = False,\n        only_bounds: bool = False,\n        scale_weights_to_n: bool = True,\n    ) -&gt; bool:\n        \"\"\"\n        Call accelerated ebw calibration code\n            (https://github.t26.it.census.gov/sande440/entropy-balance-weighting)\n\n        Parameters\n        ----------\n        m : Moment\n            Moment to be calibrated to\n        sequential : TYPE, optional\n            Is this part of a sequential optimization. The default is False.\n        dense : bool, optional\n            Dense matrix (vs. sparse). The default is True.\n        bounds : tuple(float,float) | None, optional\n            tuple of lower, upper bound.\n            If bounds are set, the \"elastic\" calibration is called,\n            which tries to match as well as possible, but can\n            handle unsatisfiable constraints\n            (by trying to get as closs as possible)\n            Can be (0,None) for no bounds elastic calibration.\n            The default is None.\n        #   eta : TYPE, optional\n        #       Elastic calibration penalty parameter. The default is None.\n        fallback_bounded : bool, optional\n            If unbounded calibration fails to converge, fallback to bounded\n            for \"as close as possible\" calibration\n            with no bounds (bounds=[epsilon_weight_lower_bound,None])\n            The default is False.\n        fallback_iterations : int, optional\n            Number of iterations if falling back to elastic, bounded optimization.\n            The default is self.Iterations\n        dual_only : bool, optional\n            Use dual only (Hainmueller-like) optimization. The default is False.\n        only_bounds : bool, optional\n            If you have bounds, do  you want to start with the\n            bounds or see if the normal procedure satisfies the bounds?\n            If true, start with the bounded elastic,\n            if False, trie without the bounds and verify if they are satisfied.\n            The default is False.\n        scale_weights_to_n : bool, optional\n            Scaling weights to sum to n of sample seems\n            to run faster, so do that? The default is True.\n\n        Returns\n        -------\n        b_converged : boolean\n            Converged?  It also sets the weights in self\n\n        \"\"\"\n        epsilon_weight_lower_bound = 10e-5\n\n        if not hasattr(np, \"concat\"):\n            np.concat = np.concatenate\n\n        [initial_guess, base_weight, xi, targetsi, tol_adj] = self._moment_inputs(\n            m=m, sequential=sequential\n        )\n\n        nw_type = NarwhalsType(xi)\n\n        n_weights = safe_height(initial_guess)\n        aebw_options = dict(dense=dense, dual_only=dual_only)\n\n        # if eta is not None:\n        #     aebw_options[\"eta\"] = eta\n\n        #   If you pass in [0,None], then I'm assuming only_bounds\n        #       because that is unbounded\n        if bounds is not None:\n            if bounds[0] &lt;= epsilon_weight_lower_bound and bounds[1] is None:\n                only_bounds = True\n\n            if only_bounds:\n                aebw_options[\"bounds\"] = bounds\n\n        # WriteParquet(Xi,\n        #               \"/projects/data/NEWS/TempFiles/x_sample.parquet\")\n        # WriteParquet(initial_guess,\n        #               \"/projects/data/NEWS/TempFiles/weights0.parquet\")\n        # WriteParquet(Targetsi,\n        #               \"/projects/data/NEWS/TempFiles/mean_population_moments.parquet\")\n\n        #   Initial weights\n        if base_weight is None:\n            base_weight = (\n                nw.from_native(initial_guess).lazy().collect().to_numpy().ravel()\n            )\n            initial_guess = None\n\n            #   aebw_options[\"initial_ratio_guess\"] = base_weight\n        else:\n            base_weight = (\n                nw.from_native(base_weight).lazy().collect().to_numpy().ravel()\n            )\n            initial_guess = (\n                nw.from_native(initial_guess).lazy().collect().to_numpy().ravel()\n            )\n\n            #   aebw_options[\"initial_ratio_guess\"] = initial_guess\n\n        xi = (\n            nw.from_native(xi)\n            .with_columns(\n                [cs.boolean().cast(nw.Float32), cs.numeric().cast(nw.Float64)]\n            )\n            .to_native()\n        )\n\n        if dense:\n            xi = (\n                nw.from_native(xi)\n                .with_columns(nw.all().cast(nw.Float64))\n                .lazy()\n                .collect()\n                .to_numpy()\n            )\n\n        else:\n            xi = sp.csc_array(\n                nw.from_native(xi)\n                .with_columns(nw.all().cast(nw.Float64))\n                .lazy()\n                .collect()\n                .to_numpy()\n            )\n\n        targetsi = nw.from_native(targetsi).lazy().collect().to_numpy().ravel()\n\n        if scale_weights_to_n:\n            sum_weights = base_weight.sum()\n\n            adjustment = n_weights / sum_weights\n            base_weight = base_weight * adjustment\n            if initial_guess is not None:\n                initial_guess = initial_guess * adjustment\n\n        b_converged = False\n\n        results = None\n        w_out = None\n\n        try:\n            aebw_options[\"max_steps\"] = self.iterations\n            aebw_options[\"optimality_violation\"] = self.tolerance * tol_adj\n            #   aebw_options[\"save_problem_data\"] = \"/projects/data/NEWS/TempFiles/ebw.zip\"\n            results = ebw_routines.entropy_balance(\n                mean_population_moments=targetsi,\n                x_sample=xi,\n                weights0=base_weight,\n                options=aebw_options,\n            )\n\n            #   Make sure the print log goes with the code that executed it (rather than at the end of the log)\n            print(\"\", flush=True)\n\n            b_converged = results.converged\n        except Exception as e:\n            logger.error(\"Failure of initial calibration\")\n            logger.error(e)\n\n        check_bounds = not only_bounds and (bounds is not None)\n        rerun_with_bounds = False\n        if b_converged:\n            w_out = nw.from_native(\n                nw.from_arrow(\n                    pl.from_numpy(\n                        results.new_weights, schema={\"column_0\": pl.Float64}\n                    ).to_arrow(),\n                    backend=backend_eager(nw_type.backend),\n                )\n            )\n\n        ratio = None\n        if check_bounds:\n            #   It converged, but are we finished?\n            df_base_weight = nw.from_arrow(\n                pl.from_numpy(base_weight, schema={\"base\": pl.Float64}).to_arrow(),\n                backend=backend_eager(nw_type.backend),\n            )\n\n            c_ratio = nw.col(\"new\") / nw.col(\"base\")\n            ratio = nw.from_native(\n                concat_wrapper(\n                    [df_base_weight, w_out.rename({\"column_0\": \"new\"})],\n                    how=\"horizontal\",\n                )\n            ).select([c_ratio.min().alias(\"min\"), c_ratio.max().alias(\"max\")])\n\n            min_ratio = ratio.select(\"min\").item(0, 0)\n            max_ratio = ratio.select(\"max\").item(0, 0)\n\n            lower_bound = bounds[0]\n            upper_bound = bounds[1]\n\n            if lower_bound is not None and lower_bound &gt; 0:\n                rerun_with_bounds = lower_bound &gt; min_ratio\n            if not rerun_with_bounds and upper_bound is not None and upper_bound &gt; 0:\n                rerun_with_bounds = rerun_with_bounds or (upper_bound &lt; max_ratio)\n\n            if rerun_with_bounds:\n                logger.info(\"\\n\\nBounds satisfied:\")\n                logger.info(f\"     bounds = {bounds}\")\n                logger.info(f\"     limits = {[min_ratio, max_ratio]}\")\n\n        if (not b_converged and fallback_bounded) or rerun_with_bounds:\n            #   Failed, did you use bounds the first time?\n            #       Bounds will run the \"as close as possible\" version\n            if bounds is None:\n                bounds = [epsilon_weight_lower_bound, None]\n            aebw_options[\"bounds\"] = bounds\n            logger.info(f\"\\n\\nAttempting to calibrate with bounds: {bounds}\")\n\n            #   Set the initial guess\n            ratio = None\n            # if w_out is not None:\n            #     df_base_weight = pl.from_numpy(base_weight,\n            #                                     schema={\"base\":pl.Float64})\n\n            #     c_ratio = pl.col(\"new\")/pl.col(\"base\")\n            #     ratio = (pl.concat([df_base_weight,\n            #                         w_out.rename({\"column_0\":\"new\"})],\n            #                         how=\"horizontal\")\n            #               .select(c_ratio)\n            #               )\n\n            #   Initial guess doesn't work for bounded value: line 434 (inv_h_sqrt = sp..)\n            # if ratio is not None:\n            #     aebw_options[\"initial_ratio_guess\"] = ratio.to_numpy().ravel()\n            if fallback_iterations &gt; 0:\n                aebw_options[\"max_steps\"] = fallback_iterations\n\n            results = ebw_routines.entropy_balance_elastic(\n                mean_population_moments=targetsi,\n                x_sample=xi,\n                weights0=base_weight,\n                options=aebw_options,\n            )\n\n            b_converged = results.converged\n            w_out = nw.from_native(\n                nw.from_arrow(\n                    pl.from_numpy(\n                        results.new_weights, schema={\"column_0\": pl.Float64}\n                    ).to_arrow(),\n                    backend=backend_eager(nw_type.backend),\n                )\n            )\n            # except Exception as e:\n            #     logger.error(f\"Failure of calibration with bounds: {bounds}\")\n            #     logger.error(e)\n\n        if w_out is not None:\n            self._merge_on_weights(m=m, weights=w_out, sequential=sequential)\n\n        return b_converged\n\n    def _moment_inputs(self, m: Moment = None, sequential=False, add_intercept=True):\n        #   Initial weights\n        cols_weights = [self.final_weight]\n        if self.initial_guess != \"\":\n            cols_weights.append(self.initial_guess)\n        cols_df = self.index + cols_weights\n\n        qi = (\n            nw.from_native(\n                join_wrapper(\n                    nw.from_native(m.model_matrix).select(m.index).to_native(),\n                    nw.from_native(self.df).select(cols_df).to_native(),\n                    on=self.index,\n                    how=\"left\",\n                )\n            )\n            .sort(m.index)\n            .select(cols_weights)\n            .to_native()\n        )\n\n        qi = (\n            nw.from_native(safe_sum_cast(qi))\n            .with_columns([nw.col(coli) / nw.col(coli).sum() for coli in cols_weights])\n            .to_native()\n        )\n\n        if self.initial_guess != \"\":\n            qi_base = qi.select(self.final_weight)\n            qi = qi.select(self.initial_guess)\n\n            qi_base = (\n                nw.from_native(safe_sum_cast(df=qi_base, columns=[self.final_weight]))\n                .with_columns(\n                    (nw.col(self.final_weight) / nw.col(self.final_weight).sum()).alias(\n                        self.final_weight\n                    )\n                )\n                .to_native()\n            )\n        else:\n            qi_base = None\n        col_qi = nw.from_native(qi).lazy().collect_schema().names()[0]\n        qi = nw.from_native(qi)\n        qi = (\n            nw.from_native(safe_sum_cast(df=qi, columns=col_qi))\n            .with_columns(\n                (nw.col(col_qi) / nw.col(col_qi).sum()).alias(\n                    qi.lazy().collect_schema().names()[0]\n                )\n            )\n            .to_native()\n        )\n\n        xi = m.rescaled_model_matrix(narrow=True)\n\n        # Targets need to be adjusted for submoments\n        targetsi = (\n            nw.from_native(m.targets)\n            .select(\n                nw.col(nw.from_native(xi).lazy().collect_schema().names()) / m.by_share\n            )\n            .to_native()\n        )\n\n        #   Adjust tolerance for subgroup byshare to match actual passed tolerance\n        if sequential:\n            tol_adj = m.by_share\n        else:\n            tol_adj = 1\n\n        if add_intercept:\n            xi = (\n                nw.from_native(xi)\n                .with_columns(nw.lit(1).alias(\"___weighting_intercept___\"))\n                .to_native()\n            )\n\n            targetsi = (\n                nw.from_native(targetsi)\n                .with_columns(nw.lit(1).alias(\"___weighting_intercept___\"))\n                .to_native()\n            )\n\n        return [qi, qi_base, xi, targetsi, tol_adj]\n\n    def _merge_on_weights(\n        self, m: Moment, weights: IntoFrameT, sequential: bool = False\n    ):\n        temp_name = self.final_weight + \"___temp___\"\n        weights = nw.from_native(weights)\n        weights = safe_sum_cast(\n            weights.rename({weights.lazy().collect_schema().names()[0]: temp_name}),\n            columns=temp_name,\n        )\n\n        #   Adjust to share in this group (relative to total weight in this group vs. overall)\n        #       relative to total weights that sum to the n in the sample\n        n_obs = safe_height(weights)\n        weights = (\n            nw.from_native(weights)\n            .with_columns(\n                (nw.col(temp_name) / nw.col(temp_name).sum() * n_obs).alias(temp_name)\n            )\n            .to_native()\n        )\n        #   .rename({wOut.columns[0]:self.final_weight}\n\n        if sequential:\n            weights = nw.from_native(weights).with_columns(\n                nw.all() * m.by_share / n_obs / safe_height(self.df)\n            )\n\n            #   Merge by index\n            weights = pl.concat(\n                [\n                    nw.from_native(m.model_matrix)\n                    .select(self.index)\n                    .collect()\n                    .to_native(),\n                    weights,\n                ],\n                how=\"horizontal\",\n            )\n            self.df = (\n                nw.from_native(\n                    join_wrapper(self.df, weights, on=self.index, how=\"left\")\n                )\n                .with_columns(\n                    nw.when(nw.col(temp_name).is_not_missing())\n                    .then(nw.col(temp_name))\n                    .otherwise(nw.col(self.final_weight))\n                    .alias(self.final_weight)\n                )\n                .drop(temp_name)\n                .lazy_backend(self.nw_type)\n                .to_native()\n            )\n        else:\n            #   Concatenate, it's faster (since the data's sorted and the same length)\n            self.df = (\n                concat_wrapper(\n                    [\n                        nw.from_native(self.df)\n                        .drop(self.final_weight)\n                        .lazy()\n                        .collect(),\n                        nw.from_native(weights).rename({temp_name: self.final_weight}),\n                    ],\n                    how=\"horizontal\",\n                )\n                .lazy_backend(self.nw_type)\n                .to_native()\n            )\n\n        # diagi = self._diagnostics_moment(m=m,\n        #                                  with_by_prefix=False,\n        #                                  with_count_prefix=False,\n        #                                  count_value=False)\n\n        # logger.info(diagi)\n\n    def diagnostics(self):\n        diag = None\n\n        with_count_prefix = len(self.moments) &gt; 1\n        for i_moment, m in enumerate(self.moments):\n            if m.targets is not None:\n                diagi = self._diagnostics_moment(\n                    m=m,\n                    with_by_prefix=False,\n                    with_count_prefix=with_count_prefix,\n                    count_value=i_moment,\n                )\n\n                if diag is None and diagi is not None:\n                    diag = diagi\n                else:\n                    diag = concat_wrapper([diag, diagi], how=\"vertical\")\n\n            for subi in m.sub_moments:\n                diagi = self._diagnostics_moment(\n                    m=subi,\n                    with_by_prefix=True,\n                    with_count_prefix=with_count_prefix,\n                    count_value=i_moment,\n                )\n\n                if diag is None and diagi is not None:\n                    diag = diagi\n                else:\n                    diag = concat_wrapper([diag, diagi], how=\"vertical\")\n\n        diag = (\n            nw.from_native(diag)\n            .filter((nw.col(\"Estimates\").is_not_missing()))\n            .with_columns((nw.col(\"Estimates\") - nw.col(\"Targets\")).alias(\"diff\"))\n            .with_columns(\n                (100 * (nw.col(\"Estimates\") / nw.col(\"Targets\") - 1)).alias(\"percent\")\n            )\n            .to_native()\n        )\n\n        diag = compress_df(diag, no_boolean=True)\n        max_diff = (\n            nw.from_native(diag)\n            .filter(nw.col(\"Calibrated\") == 1)\n            .with_columns((nw.col(\"percent\").abs() / 100).alias(\"abs_diff\"))\n            .select(nw.col(\"abs_diff\").max())\n            .lazy()\n            .collect()\n            .item(0, 0)\n        )\n\n        return {\"diagnostics\": diag, \"max_diff\": max_diff}\n\n    def _diagnostics_moment(\n        self,\n        m: Moment,\n        with_by_prefix: bool = False,\n        with_count_prefix: bool = False,\n        count_value: int = 0,\n    ):\n        if m.model_matrix is None:\n            #   Do nothing\n            return None\n\n        #   Calculate the stats on model matrix\n        #       Rescale the data (if necessary)\n        cols_df = []\n        cols_df.extend(self.index)\n        cols_df.append(self.weight)\n        cols_df.append(self.final_weight)\n\n        df_statsin = join_wrapper(\n            m.rescaled_model_matrix(), self.df, on=m.index, how=\"left\"\n        )\n\n        #       What are we calculating\n        c_exclude = [m.weight, self.final_weight]\n        c_exclude.extend(m.index)\n\n        cols_in_mm = [\n            coli\n            for coli in nw.from_native(m.model_matrix).lazy().collect_schema().names()\n            if coli not in m.index\n        ]\n        column_stats = column_stats_builder(\n            cols_include=cols_in_mm,\n            df=m.model_matrix,\n            cols_exclude=c_exclude,\n            stat=\"mean\",\n        )\n        #       Final weighted estimates\n        df_estimates = (\n            nw.from_native(\n                calculate_by(\n                    df=df_statsin,\n                    column_stats=column_stats,\n                    weight=self.final_weight,\n                    no_suffix=True,\n                )\n            )\n            .with_columns(cs.numeric() * m.by_share)\n            .with_columns(nw.lit(\"Estimates\").alias(\"Column\"))\n            .to_native()\n        )\n        df_estimates = df_estimates\n\n        #       Initial values\n        df_initial = (\n            nw.from_native(\n                calculate_by(\n                    df=df_statsin,\n                    column_stats=column_stats,\n                    weight=self.weight,\n                    no_suffix=True,\n                )\n            )\n            .with_columns(cs.numeric() * m.by_share)\n            .with_columns(nw.lit(\"Initial\").alias(\"Column\"))\n            .to_native()\n        )\n\n        #   Return some basic info with the diagnostics\n        df_targets = (\n            nw.from_native(m.targets)\n            .with_columns(nw.lit(\"Targets\").alias(\"Column\"))\n            .to_native()\n        )\n\n        #   Non-zero counts in the weighted data\n        df_non_zero = (\n            nw.from_native(m.non_zero)\n            .with_columns(nw.lit(\"NonZero\").alias(\"Column\"))\n            .to_native()\n        )\n\n        #   Non-zero counts in the target data\n        df_non_zero_target = (\n            nw.from_native(m.non_zero_target)\n            .with_columns(nw.lit(\"NonZero_Target\").alias(\"Column\"))\n            .to_native()\n        )\n\n        #   What variables were actually used in the calibration?\n        #       It seems easiest to use the one row NonZero vector as the basis\n        cols_m_no_index = [ci for ci in m.columns if ci not in m.index]\n        c_ones = [nw.lit(1).alias(coli) for coli in cols_m_no_index]\n        c_zeros = [\n            nw.lit(0).alias(coli) for coli in cols_in_mm if coli not in cols_m_no_index\n        ]\n\n        df_calibrated = (\n            nw.from_native(m.non_zero)\n            .select(cols_m_no_index)\n            .with_columns(c_ones)\n            .with_columns(c_zeros)\n            .with_columns(nw.lit(\"Calibrated\").alias(\"Column\"))\n            .to_native()\n        )\n\n        #   Stack the results\n        df_diagnostics = concat_wrapper(\n            [\n                df_initial,\n                df_targets,\n                df_estimates,\n                df_calibrated,\n                df_non_zero,\n                df_non_zero_target,\n            ],\n            how=\"diagonal\",\n        )\n\n        #   Add the prefix?\n        prefix = self._combine_prefix(\n            with_by_prefix=with_by_prefix,\n            with_count_prefix=with_count_prefix,\n            count_value=count_value,\n            m=m,\n        )\n        if prefix != \"\":\n            df_diagnostics = rename_with_prefix_suffix(\n                df=df_diagnostics, prefix=prefix, ExcludeList=[\"Column\"]\n            )\n\n        #   Reorder and transpose the diagnostics to be easier to read\n        colorder = (\n            nw.from_native(df_diagnostics)\n            .drop(\"Column\")\n            .lazy()\n            .collect_schema()\n            .names()\n        )\n        nw_type = NarwhalsType(df_diagnostics)\n        df_diagnostics = nw_type.to_polars().lazy().collect()\n        df_diagnostics = nw_type.from_polars(\n            df_diagnostics.select(colorder)\n            .transpose(\n                include_header=True, column_names=df_diagnostics[\"Column\"].to_list()\n            )\n            .rename({\"column\": \"Variable\"})\n        )\n\n        return nw.from_native(df_diagnostics).lazy_backend(nw_type).to_native()\n\n    def print_diagnostics(\n        self,\n        max_columns: int = 100,\n        calibrated_only: bool = False,\n        min_non_zero: int = 10,\n        sort: list = None,\n        descending: list = None,\n    ):\n        \"\"\"\n        Print formatted calibration diagnostics to the logger.\n\n        Parameters\n        ----------\n        max_columns : int, optional\n            Maximum number of rows to display. Default is 100.\n        calibrated_only : bool, optional\n            Only show moments that were actually calibrated to. Default is False.\n        min_non_zero : int, optional\n            Minimum non-zero observations required to display a moment. Default is 10.\n        sort : list, optional\n            Column names to sort by. Default is [\"Calibrated\", \"abs\", \"NonZero\"].\n        descending : list, optional\n            Sort order for each column in sort. Default is [True, True, True].\n        \"\"\"\n\n        diag = self.diagnostics_out\n\n        if sort is None and descending is None:\n            sort = [\"Calibrated\", \"abs\", \"NonZero\"]\n            descending = [True, True, True]\n\n        #   Some polars config code to make the diagnostic readable\n        logger.info(\"Converged:          \" + str(diag[\"converged\"]))\n        logger.info(\"Maximum Difference: \" + str(diag[\"max_diff\"]))\n\n        diagnostics = diag[\"diagnostics\"]\n        diagnostics = NarwhalsType(diagnostics).to_polars()\n\n        if calibrated_only:\n            diagnostics = diagnostics.filter(pl.col.Calibrated == 1)\n\n        diagnostics = diagnostics.lazy().collect()\n        #   Only show NonZero_Target if it not identical to NonZero\n        if diagnostics.select(\"NonZero\").equals(\n            diagnostics.select(\"NonZero_Target\").rename({\"NonZero_Target\": \"NonZero\"})\n        ):\n            diagnostics = diagnostics.drop(\"NonZero_Target\")\n\n        with pl.Config(fmt_str_lengths=50) as cfg:\n            #   Basic formatting\n            cfg.set_tbl_cell_alignment(\"RIGHT\")\n            cfg.set_tbl_hide_column_data_types(True)\n            cfg.set_tbl_width_chars(600)\n            cfg.set_tbl_cols(len(diagnostics.lazy().collect_schema().names()))\n\n            #   Show all the rows (up to 100)\n            cfg.set_tbl_rows(min(max_columns, diagnostics.height))\n\n            #   Don't show tiny moments\n\n            if descending is not None:\n                d_descending = {\"descending\": descending}\n            else:\n                d_descending = {}\n\n            logger.info(\n                diagnostics.filter(pl.col.NonZero &gt;= min_non_zero)\n                .with_columns((pl.col(\"diff\").abs().alias(\"abs\")))\n                .sort(sort, **d_descending)\n                .drop(\"abs\")\n            )\n\n    def get_final_weights(self,\n                          df_merge_to:IntoFrameT | None=None,\n                          truncate_low:float|None=None,\n                          truncate_high:float|None=None) -&gt; IntoFrameT:\n        \"\"\"\n        Get the final calibrated weights, optionally merged to another dataframe.\n\n        Parameters\n        ----------\n        df_merge_to : IntoFrameT | None, optional\n            Dataframe to merge weights to. If None, returns weights with index only.\n            Default is None.\n        truncate_low : float | None, optional\n            Truncate weights below this value. Default is None.\n        truncate_high : float | None, optional\n            Truncate weights above this value. Default is None.\n\n        Returns\n        -------\n        IntoFrameT\n            Dataframe with calibrated weights. If df_merge_to provided, returns that\n            dataframe with weights merged on index. Otherwise returns index columns\n            and final weight column only.\n        \"\"\"\n\n        col_weights = []\n        col_weights.extend(self.index)\n        col_weights.append(self.final_weight)\n        df_weights = (\n            nw.from_native(self.df)\n            .select(col_weights)\n            .to_native()\n        )\n\n        c_final_weight = nw.col(self.final_weight)\n        truncate = None\n        if truncate_low is not None:\n            #   N to truncate?\n            expr_low = c_final_weight.lt(truncate_low)\n            n_truncate_low = safe_height(\n                nw.from_native(df_weights)\n                .filter(expr_low)\n                .to_native()\n            )\n            logger.info(f\"     n truncated at {truncate_low} = {n_truncate_low}\")\n\n            if n_truncate_low:\n                truncate = (\n                    nw.when(expr_low)\n                    .then(nw.lit(truncate_low))\n                )\n        if truncate_high is not None:\n            #   N to truncate?\n            expr_high = c_final_weight.gt(truncate_high)\n            n_truncate_high = safe_height(\n                nw.from_native(df_weights)\n                .filter(expr_high)\n                .to_native()\n            )\n\n            logger.info(f\"     n truncated at {truncate_high} = {n_truncate_high}\")\n\n            if n_truncate_high:\n                if truncate is not None:\n                    base = truncate\n                else:\n                    base = nw\n\n                truncate = (\n                    base.when(expr_high)\n                    .then(nw.lit(truncate_high))\n                )\n\n        if truncate is not None:\n            truncate = truncate.otherwise(nw.col(self.final_weight)).alias(self.final_weight)\n\n            df_weights = (\n                nw.from_native(df_weights)\n                .with_columns(truncate)\n                .to_native()\n            )\n\n\n        if df_merge_to is not None:\n            if self.index == [\"___rownumber\"]:\n                #   Just concatenate, we made the index internally\n                df_weights = concat_wrapper(\n                    [\n                        df_merge_to,\n                        (\n                            nw.from_native(df_weights)\n                            .sort(self.index)\n                            .select(self.final_weight)\n                            .to_native()\n                        )\n                    ],\n                    how=\"horizontal\"\n                )\n            else:\n                #   Merge on the index (safer - doesn't require assumption neither have changed)\n                df_weights = join_wrapper(\n                    df_merge_to,\n                    df_weights,\n                    on=self.index,\n                    how=\"left\"\n                )\n\n        return df_weights\n</code></pre>"},{"location":"api/calibration/#survey_kit.calibration.calibration.Calibration.run","title":"run","text":"<pre><code>run(\n    min_obs: int | list[int] = 0,\n    skip_setup: bool = False,\n    print_diagnostics: bool = True,\n    skip_diagnostics: bool = False,\n    trim: Trim | None = None,\n    **additional_params,\n) -&gt; dict\n</code></pre> <p>Run the calibration to estimate the weights from the moments.</p> <p>Parameters:</p> Name Type Description Default <code>min_obs</code> <code>int | list[int]</code> <p>Limit the moments to those with more than some number of non-zero observations. You can pass a list of increasing values so that if the model doesn't converge with the lowest number (the most constraints), then try again dropping constraints with more non-zero observations and try again. I.e. if you pass [0,10,100], it will see if the model converges with min_obs = 0, if not, it will try with min_obs = 10, etc. The default is 0.</p> <code>0</code> <code>print_diagnostics</code> <code>bool</code> <p>Print the diagnostics (do the weights match the target moments)? The default is True.</p> <code>True</code> <code>skip_diagnostics</code> <code>bool</code> <p>Don't even calculate the diagnostics? They can take a while to calculate, but you really shouldn't skip this. The default is False.</p> <code>False</code> <code>trim</code> <code>Trim | None</code> <p>Pass a Trim object with bounds on the weights. The default is None.</p> <code>None</code> <code>**additional_params</code> <code>dict</code> <p>Any additional params that are specific to a calibration algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with diagnostics information including:</p> <ul> <li>'converged' : bool     Whether the calibration converged.</li> <li>'max_diff' : float     Maximum difference between targets and estimates (if diagnostics calculated).</li> <li>'diagnostics' : DataFrame     Detailed diagnostics by moment (if skip_diagnostics=False).</li> </ul> <p>Examples:</p> <p>Basic calibration run::</p> <pre><code>diagnostics = c.run(min_obs=50)\nprint(f\"Converged: {diagnostics['converged']}\")\nprint(f\"Max difference: {diagnostics['max_diff']}\")\n</code></pre> <p>Run with fallback min_obs levels::</p> <pre><code># Try with no minimum first, then 10, then 100 if previous fails\ndiagnostics = c.run(min_obs=[0, 10, 100])\n</code></pre> <p>Run with weight trimming::</p> <pre><code>trim_params = Trim(trim=True, min_val=0.1, max_val=5.0)\ndiagnostics = c.run(trim=trim_params)\n</code></pre> Source code in <code>src\\survey_kit\\calibration\\calibration.py</code> <pre><code>def run(\n    self,\n    min_obs: int | list[int] = 0,\n    skip_setup: bool = False,\n    print_diagnostics: bool = True,\n    skip_diagnostics: bool = False,\n    trim: Trim | None = None,\n    **additional_params,\n) -&gt; dict:\n    \"\"\"\n    Run the calibration to estimate the weights from the moments.\n\n    Parameters\n    ----------\n    min_obs : int | list[int], optional\n        Limit the moments to those with more than some number of\n        non-zero observations.\n        You can pass a list of increasing values so that if\n        the model doesn't converge with the lowest number\n        (the most constraints), then try again dropping\n        constraints with more non-zero observations and try again.\n        I.e. if you pass [0,10,100], it will see if the model converges\n        with min_obs = 0, if not, it will try with min_obs = 10, etc.\n        The default is 0.\n    print_diagnostics : bool, optional\n        Print the diagnostics (do the weights match the target moments)?\n        The default is True.\n    skip_diagnostics : bool, optional\n        Don't even calculate the diagnostics?\n        They can take a while to calculate, but\n        you really shouldn't skip this.\n        The default is False.\n    trim : Trim | None, optional\n        Pass a Trim object with bounds on the weights.\n        The default is None.\n    **additional_params : dict\n        Any additional params that are specific to a\n        calibration algorithm.\n\n    Returns\n    -------\n    dict\n        A dictionary with diagnostics information including:\n\n        - 'converged' : bool\n            Whether the calibration converged.\n        - 'max_diff' : float\n            Maximum difference between targets and estimates (if diagnostics calculated).\n        - 'diagnostics' : DataFrame\n            Detailed diagnostics by moment (if skip_diagnostics=False).\n\n    Examples\n    --------\n    Basic calibration run::\n\n        diagnostics = c.run(min_obs=50)\n        print(f\"Converged: {diagnostics['converged']}\")\n        print(f\"Max difference: {diagnostics['max_diff']}\")\n\n    Run with fallback min_obs levels::\n\n        # Try with no minimum first, then 10, then 100 if previous fails\n        diagnostics = c.run(min_obs=[0, 10, 100])\n\n    Run with weight trimming::\n\n        trim_params = Trim(trim=True, min_val=0.1, max_val=5.0)\n        diagnostics = c.run(trim=trim_params)\n    \"\"\"\n\n    #  Is min_obs a list of values?\n    #       If so, run this recursively on the remaining obs, if it doesn't converge on this one\n    min_obs_remaining = None\n    if type(min_obs) is list:\n        if len(min_obs) &gt; 1:\n            min_obs_remaining = min_obs[1 : len(min_obs)]\n            min_obs = min_obs[0]\n        else:\n            min_obs = min_obs[0]\n            min_obs_remaining = None\n\n    if min_obs &gt; 0:\n        self.censor_to_min_obs(min_obs=min_obs)\n\n    #   Need to loop if trimmed\n    #   bLoopNeeded = self.Trimmed\n    bLoopNeeded = False\n\n    if self.aggregation == \"Sequential\":\n        #   Sequential - need loop\n        bLoopNeeded = True\n    elif self.aggregation == \"Combined\":\n        if not skip_setup:\n            self.combine_moments(all=True, sub_moments=True)\n    elif self.aggregation == \"Both\":\n        if not skip_setup:\n            #   Clone this and create a version with the moments combined\n            self.c_combined = deepcopy_with_fallback(self)\n            self.c_combined.aggregation = \"Combined\"\n            # Do not trim within the combined calibration (handled in self)\n            #   self.c_combined.Trimmed = False\n\n            self.c_combined.combine_moments(all=True, sub_moments=True)\n\n    logger.info(\"Calibrating weights using \" + self.method)\n    if min_obs &gt; 0:\n        logger.info(f\"      min obs = {min_obs}\")\n\n    if bLoopNeeded:\n        n_loops = self.iterations_loop\n    else:\n        n_loops = 1\n\n    #   Loop counter\n    iLoop = 0\n\n    #   Within-loop completion flag\n    b_complete = False\n    diagnostics = None\n\n    while (iLoop &lt; n_loops) and not b_complete:\n        if n_loops &gt; 1:\n            logger.info(\"\\n\\n\\n\\nRunning loop #\" + str(iLoop + 1))\n\n        diagnostics = self._run_one_loop(trim=trim, **additional_params)\n        converged = diagnostics[\"converged\"]\n\n        if \"diagnostics\" not in list(diagnostics.keys()):\n            #   Not full diagnostics, set to null to load later\n            diagnostics = None\n\n        #   Default to complete unless set to False below\n        b_complete = True\n        if trim is not None:\n            #   Need trimming?\n            #       Don't trim if aebw without separately passed bounds\n            if trim.trim and (\n                self.method != \"aebw\" and \"bounds\" in additional_params.keys()\n            ):\n                b_complete = trim.trim_in_loop(c=self, iLoop=iLoop, n_loops=n_loops)\n\n        if self.aggregation == \"Sequential\":\n            # Check the max deviation against the tolerance\n            diagnostics = self.diagnostics()\n            max_diff = diagnostics[\"max_diff\"]\n\n            converged = max_diff &lt;= self.Tolerance_Loop\n            b_complete = converged\n\n        iLoop += 1\n\n    if not skip_diagnostics:\n        if diagnostics is None:\n            diagnostics = self.diagnostics()\n\n        if n_loops == 1:\n            b_complete = diagnostics[\"max_diff\"] &lt;= self.tolerance\n        else:\n            b_complete = diagnostics[\"max_diff\"] &lt;= self.tolerance\n\n        if not (b_complete or converged) and (min_obs_remaining is not None):\n            #   Recursively call this again, if needed\n\n            #   Reset the \"final weight\" (current weight) to the original one passed\n            #       (ignores the failed run's output)\n            self.df = (\n                nw.from_native(self.df)\n                .with_columns(nw.col(self.weight).alias(self.final_weight))\n                .to_native()\n            )\n\n            diagnostics = self.run(\n                min_obs=min_obs_remaining,\n                skip_setup=True,\n                print_diagnostics=print_diagnostics,\n                skip_diagnostics=skip_diagnostics,\n                **additional_params,\n            )\n\n        else:\n            diagnostics[\"converged\"] = converged\n\n        if \"diagnostics\" in diagnostics.keys():\n            diagnostics[\"diagnostics\"] = (\n                nw.from_native(diagnostics[\"diagnostics\"])\n                .lazy_backend(self.nw_type)\n                .to_native()\n            )\n        self.diagnostics_out = diagnostics\n\n        if print_diagnostics:\n            self.print_diagnostics()\n\n    return diagnostics\n</code></pre>"},{"location":"api/calibration/#survey_kit.calibration.calibration.Calibration.print_diagnostics","title":"print_diagnostics","text":"<pre><code>print_diagnostics(\n    max_columns: int = 100,\n    calibrated_only: bool = False,\n    min_non_zero: int = 10,\n    sort: list = None,\n    descending: list = None,\n)\n</code></pre> <p>Print formatted calibration diagnostics to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>max_columns</code> <code>int</code> <p>Maximum number of rows to display. Default is 100.</p> <code>100</code> <code>calibrated_only</code> <code>bool</code> <p>Only show moments that were actually calibrated to. Default is False.</p> <code>False</code> <code>min_non_zero</code> <code>int</code> <p>Minimum non-zero observations required to display a moment. Default is 10.</p> <code>10</code> <code>sort</code> <code>list</code> <p>Column names to sort by. Default is [\"Calibrated\", \"abs\", \"NonZero\"].</p> <code>None</code> <code>descending</code> <code>list</code> <p>Sort order for each column in sort. Default is [True, True, True].</p> <code>None</code> Source code in <code>src\\survey_kit\\calibration\\calibration.py</code> <pre><code>def print_diagnostics(\n    self,\n    max_columns: int = 100,\n    calibrated_only: bool = False,\n    min_non_zero: int = 10,\n    sort: list = None,\n    descending: list = None,\n):\n    \"\"\"\n    Print formatted calibration diagnostics to the logger.\n\n    Parameters\n    ----------\n    max_columns : int, optional\n        Maximum number of rows to display. Default is 100.\n    calibrated_only : bool, optional\n        Only show moments that were actually calibrated to. Default is False.\n    min_non_zero : int, optional\n        Minimum non-zero observations required to display a moment. Default is 10.\n    sort : list, optional\n        Column names to sort by. Default is [\"Calibrated\", \"abs\", \"NonZero\"].\n    descending : list, optional\n        Sort order for each column in sort. Default is [True, True, True].\n    \"\"\"\n\n    diag = self.diagnostics_out\n\n    if sort is None and descending is None:\n        sort = [\"Calibrated\", \"abs\", \"NonZero\"]\n        descending = [True, True, True]\n\n    #   Some polars config code to make the diagnostic readable\n    logger.info(\"Converged:          \" + str(diag[\"converged\"]))\n    logger.info(\"Maximum Difference: \" + str(diag[\"max_diff\"]))\n\n    diagnostics = diag[\"diagnostics\"]\n    diagnostics = NarwhalsType(diagnostics).to_polars()\n\n    if calibrated_only:\n        diagnostics = diagnostics.filter(pl.col.Calibrated == 1)\n\n    diagnostics = diagnostics.lazy().collect()\n    #   Only show NonZero_Target if it not identical to NonZero\n    if diagnostics.select(\"NonZero\").equals(\n        diagnostics.select(\"NonZero_Target\").rename({\"NonZero_Target\": \"NonZero\"})\n    ):\n        diagnostics = diagnostics.drop(\"NonZero_Target\")\n\n    with pl.Config(fmt_str_lengths=50) as cfg:\n        #   Basic formatting\n        cfg.set_tbl_cell_alignment(\"RIGHT\")\n        cfg.set_tbl_hide_column_data_types(True)\n        cfg.set_tbl_width_chars(600)\n        cfg.set_tbl_cols(len(diagnostics.lazy().collect_schema().names()))\n\n        #   Show all the rows (up to 100)\n        cfg.set_tbl_rows(min(max_columns, diagnostics.height))\n\n        #   Don't show tiny moments\n\n        if descending is not None:\n            d_descending = {\"descending\": descending}\n        else:\n            d_descending = {}\n\n        logger.info(\n            diagnostics.filter(pl.col.NonZero &gt;= min_non_zero)\n            .with_columns((pl.col(\"diff\").abs().alias(\"abs\")))\n            .sort(sort, **d_descending)\n            .drop(\"abs\")\n        )\n</code></pre>"},{"location":"api/calibration/#survey_kit.calibration.calibration.Calibration.get_final_weights","title":"get_final_weights","text":"<pre><code>get_final_weights(\n    df_merge_to: IntoFrameT | None = None,\n    truncate_low: float | None = None,\n    truncate_high: float | None = None,\n) -&gt; IntoFrameT\n</code></pre> <p>Get the final calibrated weights, optionally merged to another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df_merge_to</code> <code>IntoFrameT | None</code> <p>Dataframe to merge weights to. If None, returns weights with index only. Default is None.</p> <code>None</code> <code>truncate_low</code> <code>float | None</code> <p>Truncate weights below this value. Default is None.</p> <code>None</code> <code>truncate_high</code> <code>float | None</code> <p>Truncate weights above this value. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>Dataframe with calibrated weights. If df_merge_to provided, returns that dataframe with weights merged on index. Otherwise returns index columns and final weight column only.</p> Source code in <code>src\\survey_kit\\calibration\\calibration.py</code> <pre><code>def get_final_weights(self,\n                      df_merge_to:IntoFrameT | None=None,\n                      truncate_low:float|None=None,\n                      truncate_high:float|None=None) -&gt; IntoFrameT:\n    \"\"\"\n    Get the final calibrated weights, optionally merged to another dataframe.\n\n    Parameters\n    ----------\n    df_merge_to : IntoFrameT | None, optional\n        Dataframe to merge weights to. If None, returns weights with index only.\n        Default is None.\n    truncate_low : float | None, optional\n        Truncate weights below this value. Default is None.\n    truncate_high : float | None, optional\n        Truncate weights above this value. Default is None.\n\n    Returns\n    -------\n    IntoFrameT\n        Dataframe with calibrated weights. If df_merge_to provided, returns that\n        dataframe with weights merged on index. Otherwise returns index columns\n        and final weight column only.\n    \"\"\"\n\n    col_weights = []\n    col_weights.extend(self.index)\n    col_weights.append(self.final_weight)\n    df_weights = (\n        nw.from_native(self.df)\n        .select(col_weights)\n        .to_native()\n    )\n\n    c_final_weight = nw.col(self.final_weight)\n    truncate = None\n    if truncate_low is not None:\n        #   N to truncate?\n        expr_low = c_final_weight.lt(truncate_low)\n        n_truncate_low = safe_height(\n            nw.from_native(df_weights)\n            .filter(expr_low)\n            .to_native()\n        )\n        logger.info(f\"     n truncated at {truncate_low} = {n_truncate_low}\")\n\n        if n_truncate_low:\n            truncate = (\n                nw.when(expr_low)\n                .then(nw.lit(truncate_low))\n            )\n    if truncate_high is not None:\n        #   N to truncate?\n        expr_high = c_final_weight.gt(truncate_high)\n        n_truncate_high = safe_height(\n            nw.from_native(df_weights)\n            .filter(expr_high)\n            .to_native()\n        )\n\n        logger.info(f\"     n truncated at {truncate_high} = {n_truncate_high}\")\n\n        if n_truncate_high:\n            if truncate is not None:\n                base = truncate\n            else:\n                base = nw\n\n            truncate = (\n                base.when(expr_high)\n                .then(nw.lit(truncate_high))\n            )\n\n    if truncate is not None:\n        truncate = truncate.otherwise(nw.col(self.final_weight)).alias(self.final_weight)\n\n        df_weights = (\n            nw.from_native(df_weights)\n            .with_columns(truncate)\n            .to_native()\n        )\n\n\n    if df_merge_to is not None:\n        if self.index == [\"___rownumber\"]:\n            #   Just concatenate, we made the index internally\n            df_weights = concat_wrapper(\n                [\n                    df_merge_to,\n                    (\n                        nw.from_native(df_weights)\n                        .sort(self.index)\n                        .select(self.final_weight)\n                        .to_native()\n                    )\n                ],\n                how=\"horizontal\"\n            )\n        else:\n            #   Merge on the index (safer - doesn't require assumption neither have changed)\n            df_weights = join_wrapper(\n                df_merge_to,\n                df_weights,\n                on=self.index,\n                how=\"left\"\n            )\n\n    return df_weights\n</code></pre>"},{"location":"api/calibration/#survey_kit.calibration.trim.Trim","title":"Trim","text":"<p>               Bases: <code>Serializable</code></p> <p>Weight trimming parameters for iterative calibration.</p> <p>Controls how calibrated weights are trimmed (bounded) during iterative calibration to prevent extreme weight values. Trimming can improve practical usability of weights at the cost of some bias in meeting calibration targets exactly.</p> <p>Parameters:</p> Name Type Description Default <code>trim</code> <code>bool</code> <p>Whether to apply trimming. Default is False.</p> <code>False</code> <code>min_val</code> <code>float</code> <p>Minimum allowed weight ratio (weight/base_weight). Default is 0.05.</p> <code>0.05</code> <code>max_val</code> <code>float</code> <p>Maximum allowed weight ratio (weight/base_weight). Default is 5.0.</p> <code>5.0</code> <code>step</code> <code>float</code> <p>How much to relax trim bounds on each iteration (multiplied by iteration number). Default is 0.02.</p> <code>0.02</code> <code>tolerance_step</code> <code>float</code> <p>How much to relax trim tolerance on each iteration. Default is 0.01.</p> <code>0.01</code> <code>ignore_n</code> <code>int</code> <p>Don't trim if fewer than this many observations exceed bounds. This prevents trimming for a small number of outliers. Default is 0.</p> <code>0</code> <p>Examples:</p> <p>Basic trimming with standard bounds:</p> <pre><code>&gt;&gt;&gt; from survey_kit.calibration.trim import Trim\n&gt;&gt;&gt; trim = Trim(trim=True, min_val=0.2, max_val=4.0)\n</code></pre> <p>Relaxed trimming that ignores small violations:</p> <pre><code>&gt;&gt;&gt; trim = Trim(\n&gt;&gt;&gt;     trim=True,\n&gt;&gt;&gt;     min_val=0.1,\n&gt;&gt;&gt;     max_val=5.0,\n&gt;&gt;&gt;     ignore_n=5,\n&gt;&gt;&gt;     step=0.03\n&gt;&gt;&gt; )\n</code></pre> <p>Usage in calibration:</p> <pre><code>&gt;&gt;&gt; from survey_kit.calibration import Calibration\n&gt;&gt;&gt; cal = Calibration(df=df, moments=moments, weight=\"base_weight\")\n&gt;&gt;&gt; results = cal.run(trim=trim, iterations_loop=10)\n</code></pre> Notes <ul> <li>Trimming occurs between calibration iterations in an iterative loop.</li> <li>Bounds are relaxed on each iteration: on iteration i, the max bound becomes max_val * (1 + i * tolerance_step) and the trim point becomes  max_val * (1 - i * step).</li> <li>Setting ignore_n &gt; 0 prevents trimming when only a few observations exceed bounds, which can be useful for preserving calibration accuracy.</li> <li>Trimming creates a trade-off: tighter bounds reduce variance of weighted estimates but may prevent exact calibration to targets.</li> <li>The iterative trimming approach allows the algorithm to gradually find a balance between meeting targets and respecting weight bounds.</li> </ul> Source code in <code>src\\survey_kit\\calibration\\trim.py</code> <pre><code>class Trim(Serializable):\n    \"\"\"\n    Weight trimming parameters for iterative calibration.\n\n    Controls how calibrated weights are trimmed (bounded) during iterative calibration\n    to prevent extreme weight values. Trimming can improve practical usability of\n    weights at the cost of some bias in meeting calibration targets exactly.\n\n    Parameters\n    ----------\n    trim : bool, optional\n        Whether to apply trimming. Default is False.\n    min_val : float, optional\n        Minimum allowed weight ratio (weight/base_weight). Default is 0.05.\n    max_val : float, optional\n        Maximum allowed weight ratio (weight/base_weight). Default is 5.0.\n    step : float, optional\n        How much to relax trim bounds on each iteration (multiplied by iteration\n        number). Default is 0.02.\n    tolerance_step : float, optional\n        How much to relax trim tolerance on each iteration. Default is 0.01.\n    ignore_n : int, optional\n        Don't trim if fewer than this many observations exceed bounds. This prevents\n        trimming for a small number of outliers. Default is 0.\n\n    Examples\n    --------\n    Basic trimming with standard bounds:\n\n    &gt;&gt;&gt; from survey_kit.calibration.trim import Trim\n    &gt;&gt;&gt; trim = Trim(trim=True, min_val=0.2, max_val=4.0)\n\n    Relaxed trimming that ignores small violations:\n\n    &gt;&gt;&gt; trim = Trim(\n    &gt;&gt;&gt;     trim=True,\n    &gt;&gt;&gt;     min_val=0.1,\n    &gt;&gt;&gt;     max_val=5.0,\n    &gt;&gt;&gt;     ignore_n=5,\n    &gt;&gt;&gt;     step=0.03\n    &gt;&gt;&gt; )\n\n    Usage in calibration:\n\n    &gt;&gt;&gt; from survey_kit.calibration import Calibration\n    &gt;&gt;&gt; cal = Calibration(df=df, moments=moments, weight=\"base_weight\")\n    &gt;&gt;&gt; results = cal.run(trim=trim, iterations_loop=10)\n\n    Notes\n    -----\n    - Trimming occurs between calibration iterations in an iterative loop.\n    - Bounds are relaxed on each iteration: on iteration i, the max bound becomes\n    max_val * (1 + i * tolerance_step) and the trim point becomes \n    max_val * (1 - i * step).\n    - Setting ignore_n &gt; 0 prevents trimming when only a few observations exceed\n    bounds, which can be useful for preserving calibration accuracy.\n    - Trimming creates a trade-off: tighter bounds reduce variance of weighted\n    estimates but may prevent exact calibration to targets.\n    - The iterative trimming approach allows the algorithm to gradually find a\n    balance between meeting targets and respecting weight bounds.\n    \"\"\"\n    _save_suffix = \"calibration_trim\"\n\n    def __init__(\n        self,\n        trim: bool = False,\n        min_val: float = 0.05,\n        max_val: float = 5.0,\n        step: float = 0.02,\n        tolerance_step: float = 0.01,\n        ignore_n: int = 0,\n    ):\n        self.trim = trim\n        self.min_val = min_val\n        self.max_val = max_val\n        self.step = step\n        self.tolerance_step = tolerance_step\n        self.ignore_n = ignore_n\n\n    def trim_in_loop(self, c: Calibration, iLoop: int, nLoops: int):\n        # Complete unless set to false later\n        bComplete = True\n\n        #   Check if the weights need trimming (not on last loop)\n        if self.trim and (iLoop - 1) &lt; nLoops:\n            # Check min and max weight\n            max_weight = (\n                nw.from_native(c.df)\n                .select(nw.col(c.final_weight).max())\n                .collect()\n                .item(0, 0)\n            )\n            min_weight = (\n                nw.from_native(c.df)\n                .select(nw.col(c.final_weight).min())\n                .collect()\n                .item(0, 0)\n            )\n\n            trim_limit_max = self.max_val * (1 + iLoop * self.tolerance_step)\n            trim_limit_min = self.min_val * (1 - iLoop * self.tolerance_step)\n            btrim_max = False\n            btrim_min = False\n            n_to_trim_max = 0\n            n_to_trim_min = 0\n\n            if max_weight &gt; trim_limit_max:\n                if self.ignore_n &gt; 0:\n                    n_to_trim_max = safe_height(\n                        nw.from_native(c.df).filter(\n                            nw.col(c.final_weight) &gt; trim_limit_max\n                        )\n                    )\n\n                    btrim_max = n_to_trim_max &gt; self.ignore_n\n\n                    if not btrim_max:\n                        logger.info(\n                            f\"Max weight ({max_weight}) is greater than trim_max({trim_limit_max}), but NOT TRIMMING because only {n_to_trim_max}  need trimming against a passed limit of {self.ignore_n}\"\n                        )\n            else:\n                btrim_max = True\n\n            if min_weight &lt; trim_limit_min:\n                if self.ignore_n &gt; 0:\n                    n_to_trim_max = safe_height(\n                        nw.from_native(c.df).filter(\n                            nw.col(c.final_weight) &lt; trim_limit_min\n                        )\n                    )\n\n                    btrim_min = n_to_trim_min &gt; self.ignore_n\n\n                    if not btrim_min:\n                        logger.info(\n                            f\"Min weight ({min_weight}) is greater than trim_max({trim_limit_min}), but NOT TRIMMING because only {n_to_trim_min}  need trimming against a passed limit of {self.ignore_n}\"\n                        )\n                    else:\n                        btrim_min = True\n\n            if btrim_max:\n                bComplete = False\n\n                trim_at = self.max_val * (1 - iLoop * self.step)\n\n                logger.info(\n                    f\"Max weight ({max_weight}) is greater than trim_max({trim_limit_max}), trimming to {trim_at})\"\n                )\n\n                if n_to_trim_max &gt; 0:\n                    logger.info(f\"     {n_to_trim_max} need trimming\")\n\n                c.df = (\n                    nw.from_native(c.df)\n                    .with_columns(\n                        (\n                            nw.when(nw.col(c.final_weight) &gt; trim_at)\n                            .then(nw.lit(trim_at))\n                            .otherwise(nw.col(c.final_weight))\n                            .alias(c.final_weight)\n                        )\n                    )\n                    .to_native()\n                )\n\n            if btrim_min:\n                bComplete = False\n                trim_at = self.min_val * (1 + iLoop * self.step)\n\n                logger.info(\n                    f\"Min weight ({min_weight}) is greater than trim_max({trim_limit_min}), trimming to {trim_at})\"\n                )\n\n                if n_to_trim_min &gt; 0:\n                    logger.info(f\"     {n_to_trim_min} need trimming\")\n\n                c.df = (\n                    nw.from_native(c.df)\n                    .with_columns(\n                        (\n                            nw.when(nw.col(c.final_weight) &lt; trim_at)\n                            .then(nw.lit(trim_at))\n                            .otherwise(nw.col(c.final_weight))\n                            .alias(c.final_weight)\n                        )\n                    )\n                    .to_native()\n                )\n\n        return bComplete\n\n    def _str__(self):\n        return (\n            f\"trim           = {self.trim}\"\n            + \"\\n\"\n            + f\"min_val        = {self.min_val}\"\n            + \"\\n\"\n            + f\"min_val        = {self.min_val}\"\n            + \"\\n\"\n            + f\"max_val        = {self.max_val}\"\n            + \"\\n\"\n            + f\"step           = {self.step}\"\n            + \"\\n\"\n            + f\"tolerance_step = {self.tolerance_step}\"\n            + \"\\n\"\n            + f\"ignore_n       = {self.ignore_n}\"\n        )\n</code></pre>"},{"location":"api/miscellaneous/","title":"Miscellaneous","text":""},{"location":"api/miscellaneous/#survey_kit.serializable.Serializable","title":"Serializable","text":"Source code in <code>src\\survey_kit\\serializable.py</code> <pre><code>class Serializable:\n    _save_exclude_items = []\n    _save_suffix = \"serial\"\n    _registry = {}\n\n    try:\n        import numpy as np\n\n        _BASE_TYPES = (int, float, str, bool, np.float64)\n    except ImportError:\n        _BASE_TYPES = (int, float, str, bool)\n\n    def __init__(self):\n        self.__fully_serializable__ = True\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Automatically register subclasses when they're defined\"\"\"\n        super().__init_subclass__(**kwargs)\n        if hasattr(cls, \"_save_suffix\") and cls._save_suffix:\n            Serializable._registry[cls._save_suffix] = cls\n            logger.debug(f\"Registered {cls.__name__} with suffix '{cls._save_suffix}'\")\n\n    @classmethod\n    def _init_from_dict(\n        cls, data: dict, init_kwargs: dict | None = None, **additional_kwargs\n    ):\n        init_keys = list(inspect.signature(cls.__init__).parameters.keys())\n        init_keys.remove(\"self\")\n        if init_kwargs is None:\n            init_kwargs = {}\n        remaining_kwargs = {}\n        for key, value in data.items():\n            if key in init_keys:\n                init_kwargs[key] = value\n            else:\n                remaining_kwargs[key] = value\n\n        obj = cls(**init_kwargs, **additional_kwargs)\n\n        for keyi, valuei in remaining_kwargs.items():\n            if (keyi not in obj._reserved_keys()) and (\n                keyi not in obj._save_exclude_items\n            ):\n                setattr(obj, keyi, valuei)\n\n        return obj\n\n    def __hash__(self):\n        d_save = self.to_dict()\n        if self.__fully_serializable__:\n            save_caller = json\n        else:\n            save_caller = pickle\n\n        return hash(save_caller.dumps(d_save))\n\n    def save(self, path: str, quietly: bool = True) -&gt; None:\n        \"\"\"\n        Save a serializable object to\n\n        Parameters\n        ----------\n        path : str, optional\n            Path of object.  Can exclude the suffix.\n        quietly: bool, optional\n            No message to console/log\n        \"\"\"\n\n        self.__serializable_n_dfs = 0\n\n        d_save = self.to_dict()\n\n        if type(path) is not str:\n            path = str(path)\n        if not path.endswith(f\".{self._save_suffix}\"):\n            folder_path = f\"{path}.{self._save_suffix}\"\n        else:\n            folder_path = path\n\n        #   logger.info(folder_path)\n\n        if os.path.isdir(folder_path):\n            logger.info(\"Removing existing directory \" + folder_path)\n            shutil.rmtree(folder_path)\n\n        #   Make the path to save everything\n        create_folders_if_needed([folder_path])\n        #   os.makedirs(folder_path)\n\n        #   Save the data\n        dfs = d_save[\"__serialized_dfs__\"]\n        if len(dfs):\n            self._save_dfs(folder_path=folder_path, dfs=dfs, quietly=quietly)\n\n        if self.__fully_serializable__:\n            suffix = \"json\"\n            save_caller = json\n            w = \"w\"\n        else:\n            suffix = \"pkl\"\n            save_caller = pickle\n            w = \"wb\"\n\n        path_object = f\"{folder_path}/{self._save_suffix}.{suffix}\"\n        with open(path_object, w) as f:\n            save_caller.dump(d_save, f)\n\n    def _save_dfs(self, folder_path: str, dfs: dict, quietly: bool = True) -&gt; None:\n        for valuei in dfs.values():\n            dfi = valuei[\"df\"]\n            pathi = valuei[\"path\"]\n\n            path_save = os.path.normpath(f\"{folder_path}/{pathi}\")\n            create_folders_if_needed([os.path.dirname(path_save)])\n\n            dfi_nw = nw.from_native(dfi)\n            d_metadata = dict(engine=nw.get_native_namespace(dfi_nw).__package__)\n            SerializableDictionary(d_metadata).save(path_save)\n            if isinstance(dfi_nw, nw.LazyFrame):\n                dfi_nw.sink_parquet(path_save)\n            else:\n                dfi_nw.write_parquet(path_save)\n\n            del valuei[\"df\"]\n\n    @classmethod\n    def delete(cls, path: str):\n        cls.load(path=path, delete=True, delete_only=True)\n\n    @classmethod\n    def load(\n        cls,\n        path: str = \"\",\n        delete: bool = False,\n        delete_only: bool = False,\n        init_kwargs: dict | None = None,\n        **df_kwargs,\n    ) -&gt; Serializable | None:\n        \"\"\"\n        Load a serializable object from disk\n\n        Parameters\n        ----------\n        path : str, optional\n            Path of object.  Can exclude the suffix.\n        delete : bool, optional\n            Delete after load? The default is False.\n        delete_only : bool, optional\n            Don't load, just delete. The default is False.\n        init_kwargs : dict, optional\n            Arguments to pass to init function\n        df_kwargs : dict, optional\n            Arguments to pass to narwhals scan_parquet for dataframe loading\n        Returns\n        -------\n        Serializable\n        \"\"\"\n\n        if type(path) is not str:\n            path = str(path)\n\n        # If called on Serializable base class, auto-detect the subclass\n        if cls == Serializable:\n            # Find which suffix matches\n            detected_class = None\n            for suffix, registered_cls in Serializable._registry.items():\n                if path.endswith(f\".{suffix}\") or os.path.isdir(f\"{path}.{suffix}\"):\n                    detected_class = registered_cls\n                    break\n\n            if detected_class is None:\n                message = f\"Could not detect serializable type for path: {path}\"\n                logger.error(message)\n                raise Exception(message)\n\n            # Delegate to the correct class\n            return detected_class.load(\n                path=path, delete=delete, delete_only=delete_only, **df_kwargs\n            )\n\n        if not path.endswith(f\".{cls._save_suffix}\"):\n            folder_path = f\"{path}.{cls._save_suffix}\"\n        else:\n            folder_path = path\n\n        if not delete_only:\n            path_dict = os.path.normpath(f\"{folder_path}/{cls._save_suffix}.json\")\n            path_found = os.path.isfile(path_dict)\n\n            if path_found:\n                load_caller = json\n            else:\n                path_dict = os.path.normpath(f\"{folder_path}/{cls._save_suffix}.pkl\")\n                load_caller = pickle\n                path_found = os.path.isfile(path_dict)\n\n            if not path_found:\n                message = f\"{folder_path} isn't a valid Serializable object\"\n                logger.error(message)\n                raise Exception(message)\n\n            with open(path_dict, \"rb\") as f:\n                d_loaded = load_caller.load(f)\n\n            if len(d_loaded[\"__serialized_dfs__\"]):\n                cls._load_dfs(\n                    folder_path=folder_path,\n                    dfs=d_loaded[\"__serialized_dfs__\"],\n                    delete=delete,\n                    **df_kwargs,\n                )\n\n        if delete or delete_only:\n            if os.path.isdir(folder_path):\n                logger.info(\"Removing existing directory \" + folder_path)\n                shutil.rmtree(folder_path)\n\n        if delete_only:\n            return None\n\n        obj = cls.from_dict(d_loaded, init_kwargs=init_kwargs)\n        return obj\n\n    @classmethod\n    def load_any(cls, path: str = \"\", delete: bool = False, delete_only: bool = False):\n        \"\"\"\n        Pass the root path of a serializable object\n            and this will figure out what it is (from the suffix)\n            and call that object's loader function.\n\n            For cases where multiple objects work, and I don't want to\n            have to do if/else logic by suffix\n\n        Parameters\n        ----------\n        path : str, optional\n            Path of object.  Can exclude the suffix.\n        delete : bool, optional\n            Delete after load? The default is False.\n        delete_only : bool, optional\n            Don't load, just delete. The default is False.\n\n        Returns\n        -------\n        Serializable object of any type\n\n        \"\"\"\n\n        items = []\n\n        for classi in items:\n            suffixi = classi._save_suffix\n            if path.endswith(suffixi) or os.path.isdir(f\"{path}.{suffixi}\"):\n                return classi.load(path=path, delete=delete, delete_only=delete_only)\n\n    @classmethod\n    def _load_dfs(\n        cls, folder_path: str, dfs: dict, delete: bool = False, **df_kwargs\n    ) -&gt; None:\n        #   Avoid circular import\n        from .utilities.dataframe import NarwhalsType\n\n        if df_kwargs is None:\n            df_kwargs = {}\n        for keyi, valuei in dfs.items():\n            pathi = valuei[\"path\"]\n\n            path_load = os.path.normpath(f\"{folder_path}/{pathi}\")\n            d_metadata = SerializableDictionary.load(path_load)\n            df_kwargsi = df_kwargs.copy()\n            if \"backend\" not in df_kwargsi:\n                df_kwargsi[\"backend\"] = d_metadata[\"engine\"]\n                backend = d_metadata[\"engine\"]\n            else:\n                backend = df_kwargsi[\"backend\"]\n            dfi = nw.scan_parquet(path_load, **df_kwargsi)\n            if delete:\n                dfi = dfi.lazy().collect().lazy_backend(NarwhalsType(backend=backend))\n\n            dfs[keyi][\"df\"] = dfi.to_native()\n\n    def to_dict(self, dfs: dict | None = None, key_path: str = \"\") -&gt; dict[str, object]:\n        candidate_items = vars(self)\n\n        self.__fully_serializable__ = True\n        items = {}\n        if dfs is None:\n            add_dfs = True\n            dfs = {}\n        else:\n            add_dfs = False\n\n        for keyi, valuei in candidate_items.items():\n            self._to_dict_item(\n                items=items, dfs=dfs, key_path=key_path, key=keyi, value=valuei\n            )\n\n        items[\"__fully_serializable__\"] = self.__fully_serializable__\n        items[\"__class__\"] = self.__class__.__qualname__\n        if add_dfs:\n            items[\"__serialized_dfs__\"] = dfs\n        else:\n            items[\"__serialized_dfs__\"] = {}\n\n        namespace = vars(inspect.getmodule(self))[\"__name__\"]\n        items[\"__namespace__\"] = namespace\n\n        #   logger.info(items)\n        return items\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: dict,\n        init_kwargs: dict | None = None,\n    ) -&gt; object:\n        data = cls._from_dict_unpack_item(data, dfs=None, init_kwargs=init_kwargs)\n\n        return data\n\n    @classmethod\n    def _from_dict_unpack_item(\n        cls,\n        item: object,\n        dfs: dict | None,\n        init_kwargs: dict | None = None,\n    ) -&gt; object:\n        if dfs is None:\n            if \"__serialized_dfs__\" in item.keys():\n                dfs = item[\"__serialized_dfs__\"]\n                del item[\"__serialized_dfs__\"]\n            else:\n                dfs = {}\n\n        typei = type(item)\n\n        if typei is dict:\n            if \"__enumeration__\" in item.keys():\n                item = cls._from_dict_unpack_enum(item, dfs)\n            elif \"__pickled__\" in item.keys():\n                item = cls._from_dict_unpack_pickled(item, dfs)\n            # elif \"__polars_expression__\" in item.keys():\n            #     item = cls._from_dict_unpack_polars_expression(item,\n            #                                                    dfs)\n            elif cls._is_serializable(item):\n                #   Unpack as a separate item\n                item = cls._from_dict_unpack_dict(item, dfs, init_kwargs=init_kwargs)\n\n                #   Then turn it into a class\n                _namespace = importlib.import_module(item[\"__namespace__\"])\n                _class_name = item[\"__class__\"]\n                _parent = _namespace\n                for classi in _class_name.split(\".\"):\n                    # #   Fix for any refactor class name changes\n                    # d_refactor = {\"MultipleImputationStats\":\"MultipleImputation\"}\n\n                    # classi = d_refactor.get(classi,\n                    #                         classi)\n\n                    _class = getattr(_parent, classi)\n                    _parent = _class\n\n                #   _class = getattr(_namespace, item['__class__'])\n\n                init_sig = inspect.signature(_class._init_from_dict)\n                if \"init_kwargs\" in init_sig.parameters.keys():\n                    item = _class._init_from_dict(item, init_kwargs=init_kwargs)\n                else:\n                    if init_kwargs is None:\n                        init_kwargs = {}\n                    item = _class._init_from_dict(item, **init_kwargs)\n\n            else:\n                item = cls._from_dict_unpack_dict(item, dfs)\n        elif typei is list:\n            item = cls._from_dict_unpack_list(item, dfs)\n        elif typei is str and item.startswith(\"__serialized_df_\"):\n            item = cls._from_dict_assign_df(item, dfs)\n        elif typei in [int, float, str, bool]:\n            #   Do nothing - don't unpack\n            pass\n\n        return item\n\n    @classmethod\n    def _from_dict_unpack_dict(\n        cls, item: dict, dfs: dict, init_kwargs: dict | None = None\n    ) -&gt; object:\n        for key, value in item.items():\n            if key not in cls._reserved_keys():\n                item[key] = cls._from_dict_unpack_item(value, dfs, init_kwargs)\n\n        return item\n\n    @classmethod\n    def _from_dict_unpack_list(cls, item: list, dfs: dict):\n        for i in range(0, len(item)):\n            item[i] = cls._from_dict_unpack_item(item[i], dfs)\n\n        return item\n\n    @classmethod\n    def _from_dict_unpack_enum(cls, item: dict, dfs: dict):\n        #   Then turn it into a class\n        _namespace = importlib.import_module(item[\"__enum_namespace__\"])\n        _class_name = item[\"__enum_class__\"]\n        _parent = _namespace\n        for classi in _class_name.split(\".\"):\n            _class = getattr(_parent, classi)\n            _parent = _class\n        value = item[\"__enumeration__\"]\n\n        return _class(value)\n\n    @classmethod\n    def _from_dict_unpack_polars_expression(cls, item: dict, dfs: dict):\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\"Polars is required for unpacking a polars Expression. \")\n\n        return pl.Expr.deserialize(io.StringIO(item[\"__polars_expression__\"]))\n\n    @classmethod\n    def _from_dict_unpack_pickled(cls, item: dict, dfs: dict):\n        pickled_data = bytes(item[\"__pickled__\"])\n        buffer = io.BytesIO(pickled_data)\n        try:\n            return pickle.load(buffer)\n        except:\n            logger.info(\"Pickled item failed to load\")\n\n    @classmethod\n    def _from_dict_assign_df(cls, item: str, dfs: dict):\n        return dfs[item][\"df\"]\n\n    def _to_dict_item(\n        self,\n        items: dict[str, object],\n        dfs: dict[str, dict],\n        key_path: str,\n        key: str,\n        value: object,\n    ) -&gt; None:\n        # logger.info(key)\n        # logger.info(value)\n        # logger.info(items)\n        # logger.info(key_path)\n\n        if (key not in self._reserved_keys()) and (key not in self._save_exclude_items):\n            typei = type(value)\n\n            if typei is list:\n                d_item = []\n\n                for i in range(0, len(value)):\n                    self._to_dict_item(\n                        items=d_item,\n                        dfs=dfs,\n                        key_path=f\"{key_path}/{key}/{i}\",\n                        key=i,\n                        value=value[i],\n                    )\n\n                self._add_to_items(key=key, value=d_item, items=items)\n            elif typei is dict:\n                d_item = {}\n\n                for ki, vi in value.items():\n                    self._to_dict_item(\n                        items=d_item,\n                        dfs=dfs,\n                        key_path=f\"{key_path}/{key}\",\n                        key=ki,\n                        value=vi,\n                    )\n\n                self._add_to_items(key=key, value=d_item, items=items)\n            elif is_narwhals_compatible(value):\n                n_dfs = len(dfs.keys())\n\n                df_name = f\"__serialized_df_{n_dfs}\"\n                dfs[df_name] = {\n                    \"df\": value,\n                    \"path\": f\"{key_path}/{key}/{df_name}.parquet\",\n                }\n\n                self._add_to_items(key=key, value=df_name, items=items)\n            elif typei in self._BASE_TYPES:\n                self._add_to_items(key=key, value=value, items=items)\n            # elif isinstance(value,pl.Expr):\n            #     self._add_to_items(key=key,\n            #                        value=self._add_to_items_polars_expression(value),\n            #                        items=items)\n            elif isinstance(value, Serializable):\n                value_add = value.to_dict(dfs=dfs, key_path=key_path)\n                self.__fully_serializable__ = (\n                    self.__fully_serializable__ and value.__fully_serializable__\n                )\n                self._add_to_items(key=key, value=value_add, items=items)\n            elif isinstance(value, Enum):\n                value_add = self._add_to_items_enumeration(value)\n                self._add_to_items(key=key, value=value_add, items=items)\n            elif value is None:\n                self._add_to_items(key=key, value=value, items=items)\n            else:\n                # logger.info(f\"{key} not serializable\")\n                # logger.info(value)\n                #   self.__fully_serializable__ = False\n                self._add_to_items(\n                    key=key, value=self._add_to_items_pickled(value), items=items\n                )\n\n    def _add_to_items_enumeration(self, value: Enum) -&gt; dict:\n        namespace = value.__module__\n        # vars(inspect.getmodule(self))[\"__name__\"]\n\n        # logger.info({\n        #     \"__enumeration__\": value.value,\n        #     \"__enum_class__\": value.__class__.__qualname__,\n        #     \"__enum_namespace__\": namespace,\n        # })\n        # logger.info(\"\")\n        return {\n            \"__enumeration__\": value.value,\n            \"__enum_class__\": value.__class__.__qualname__,\n            \"__enum_namespace__\": namespace,\n        }\n\n    def _add_to_items_polars_expression(self, value: pl.Expr) -&gt; dict:\n        return {\"__polars_expression__\": value.meta.serialize()}\n\n    def _add_to_items_pickled(self, value: pl.Expr) -&gt; dict:\n        buffer = io.BytesIO()\n        pickle.dump(value, buffer)\n        value_save = list(buffer.getvalue())\n        return {\"__pickled__\": value_save}\n\n    @staticmethod\n    def _add_to_items(key: str, value: object, items: dict | list):\n        if type(items) is dict:\n            items[key] = value\n        elif type(items) is list:\n            items.append(value)\n\n    @staticmethod\n    def _reserved_keys() -&gt; list[str]:\n        return [\n            \"__class__\",\n            \"__namespace__\",\n            \"__fully_serializable__\",\n            \"__dfs__\",\n            \"__dfpath__\",\n        ]\n\n    @staticmethod\n    def _is_serializable(data: dict):\n        return (\n            (\"__fully_serializable__\" in data.keys())\n            and (\"__class__\" in data.keys())\n            and (\"__namespace__\" in data.keys())\n        )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.serializable.Serializable.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(\n    path: str = \"\",\n    delete: bool = False,\n    delete_only: bool = False,\n    init_kwargs: dict | None = None,\n    **df_kwargs,\n) -&gt; Serializable | None\n</code></pre> <p>Load a serializable object from disk</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of object.  Can exclude the suffix.</p> <code>''</code> <code>delete</code> <code>bool</code> <p>Delete after load? The default is False.</p> <code>False</code> <code>delete_only</code> <code>bool</code> <p>Don't load, just delete. The default is False.</p> <code>False</code> <code>init_kwargs</code> <code>dict</code> <p>Arguments to pass to init function</p> <code>None</code> <code>df_kwargs</code> <code>dict</code> <p>Arguments to pass to narwhals scan_parquet for dataframe loading</p> <code>{}</code> <p>Returns:</p> Type Description <code>Serializable</code> Source code in <code>src\\survey_kit\\serializable.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    path: str = \"\",\n    delete: bool = False,\n    delete_only: bool = False,\n    init_kwargs: dict | None = None,\n    **df_kwargs,\n) -&gt; Serializable | None:\n    \"\"\"\n    Load a serializable object from disk\n\n    Parameters\n    ----------\n    path : str, optional\n        Path of object.  Can exclude the suffix.\n    delete : bool, optional\n        Delete after load? The default is False.\n    delete_only : bool, optional\n        Don't load, just delete. The default is False.\n    init_kwargs : dict, optional\n        Arguments to pass to init function\n    df_kwargs : dict, optional\n        Arguments to pass to narwhals scan_parquet for dataframe loading\n    Returns\n    -------\n    Serializable\n    \"\"\"\n\n    if type(path) is not str:\n        path = str(path)\n\n    # If called on Serializable base class, auto-detect the subclass\n    if cls == Serializable:\n        # Find which suffix matches\n        detected_class = None\n        for suffix, registered_cls in Serializable._registry.items():\n            if path.endswith(f\".{suffix}\") or os.path.isdir(f\"{path}.{suffix}\"):\n                detected_class = registered_cls\n                break\n\n        if detected_class is None:\n            message = f\"Could not detect serializable type for path: {path}\"\n            logger.error(message)\n            raise Exception(message)\n\n        # Delegate to the correct class\n        return detected_class.load(\n            path=path, delete=delete, delete_only=delete_only, **df_kwargs\n        )\n\n    if not path.endswith(f\".{cls._save_suffix}\"):\n        folder_path = f\"{path}.{cls._save_suffix}\"\n    else:\n        folder_path = path\n\n    if not delete_only:\n        path_dict = os.path.normpath(f\"{folder_path}/{cls._save_suffix}.json\")\n        path_found = os.path.isfile(path_dict)\n\n        if path_found:\n            load_caller = json\n        else:\n            path_dict = os.path.normpath(f\"{folder_path}/{cls._save_suffix}.pkl\")\n            load_caller = pickle\n            path_found = os.path.isfile(path_dict)\n\n        if not path_found:\n            message = f\"{folder_path} isn't a valid Serializable object\"\n            logger.error(message)\n            raise Exception(message)\n\n        with open(path_dict, \"rb\") as f:\n            d_loaded = load_caller.load(f)\n\n        if len(d_loaded[\"__serialized_dfs__\"]):\n            cls._load_dfs(\n                folder_path=folder_path,\n                dfs=d_loaded[\"__serialized_dfs__\"],\n                delete=delete,\n                **df_kwargs,\n            )\n\n    if delete or delete_only:\n        if os.path.isdir(folder_path):\n            logger.info(\"Removing existing directory \" + folder_path)\n            shutil.rmtree(folder_path)\n\n    if delete_only:\n        return None\n\n    obj = cls.from_dict(d_loaded, init_kwargs=init_kwargs)\n    return obj\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.serializable.Serializable.load_any","title":"load_any  <code>classmethod</code>","text":"<pre><code>load_any(\n    path: str = \"\",\n    delete: bool = False,\n    delete_only: bool = False,\n)\n</code></pre> <p>Pass the root path of a serializable object     and this will figure out what it is (from the suffix)     and call that object's loader function.</p> <pre><code>For cases where multiple objects work, and I don't want to\nhave to do if/else logic by suffix\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of object.  Can exclude the suffix.</p> <code>''</code> <code>delete</code> <code>bool</code> <p>Delete after load? The default is False.</p> <code>False</code> <code>delete_only</code> <code>bool</code> <p>Don't load, just delete. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Serializable object of any type</code> Source code in <code>src\\survey_kit\\serializable.py</code> <pre><code>@classmethod\ndef load_any(cls, path: str = \"\", delete: bool = False, delete_only: bool = False):\n    \"\"\"\n    Pass the root path of a serializable object\n        and this will figure out what it is (from the suffix)\n        and call that object's loader function.\n\n        For cases where multiple objects work, and I don't want to\n        have to do if/else logic by suffix\n\n    Parameters\n    ----------\n    path : str, optional\n        Path of object.  Can exclude the suffix.\n    delete : bool, optional\n        Delete after load? The default is False.\n    delete_only : bool, optional\n        Don't load, just delete. The default is False.\n\n    Returns\n    -------\n    Serializable object of any type\n\n    \"\"\"\n\n    items = []\n\n    for classi in items:\n        suffixi = classi._save_suffix\n        if path.endswith(suffixi) or os.path.isdir(f\"{path}.{suffixi}\"):\n            return classi.load(path=path, delete=delete, delete_only=delete_only)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.serializable.Serializable.save","title":"save","text":"<pre><code>save(path: str, quietly: bool = True) -&gt; None\n</code></pre> <p>Save a serializable object to</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of object.  Can exclude the suffix.</p> required <code>quietly</code> <code>bool</code> <p>No message to console/log</p> <code>True</code> Source code in <code>src\\survey_kit\\serializable.py</code> <pre><code>def save(self, path: str, quietly: bool = True) -&gt; None:\n    \"\"\"\n    Save a serializable object to\n\n    Parameters\n    ----------\n    path : str, optional\n        Path of object.  Can exclude the suffix.\n    quietly: bool, optional\n        No message to console/log\n    \"\"\"\n\n    self.__serializable_n_dfs = 0\n\n    d_save = self.to_dict()\n\n    if type(path) is not str:\n        path = str(path)\n    if not path.endswith(f\".{self._save_suffix}\"):\n        folder_path = f\"{path}.{self._save_suffix}\"\n    else:\n        folder_path = path\n\n    #   logger.info(folder_path)\n\n    if os.path.isdir(folder_path):\n        logger.info(\"Removing existing directory \" + folder_path)\n        shutil.rmtree(folder_path)\n\n    #   Make the path to save everything\n    create_folders_if_needed([folder_path])\n    #   os.makedirs(folder_path)\n\n    #   Save the data\n    dfs = d_save[\"__serialized_dfs__\"]\n    if len(dfs):\n        self._save_dfs(folder_path=folder_path, dfs=dfs, quietly=quietly)\n\n    if self.__fully_serializable__:\n        suffix = \"json\"\n        save_caller = json\n        w = \"w\"\n    else:\n        suffix = \"pkl\"\n        save_caller = pickle\n        w = \"wb\"\n\n    path_object = f\"{folder_path}/{self._save_suffix}.{suffix}\"\n    with open(path_object, w) as f:\n        save_caller.dump(d_save, f)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.dataframe_list.DataFrameList","title":"DataFrameList","text":"<p>               Bases: <code>Serializable</code></p> <p>A light wrapper around Lazy/DataFrames that can be used like a list, mostly, (append, extend, +) and also can be used like the underlying LazyFrame or DataFrame where a narwhals operation is applied to all items in the list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df_list = DataFrameList([df1, df2])\n&gt;&gt;&gt; df_list.filter(nw.col(\"a\") == 1)\n</code></pre> <p>This would apply the filter to df1 and df2 and return a DataFrameList with the filtered data.</p> Source code in <code>src\\survey_kit\\utilities\\dataframe_list.py</code> <pre><code>class DataFrameList(Serializable):\n    \"\"\"\n        A light wrapper around Lazy/DataFrames\n        that can be used like a list, mostly, (append, extend, +)\n        and also can be used like the underlying LazyFrame or DataFrame\n        where a narwhals operation is applied to all items in the list.\n\n        Examples\n        --------\n        &gt;&gt;&gt; df_list = DataFrameList([df1, df2])\n        &gt;&gt;&gt; df_list.filter(nw.col(\"a\") == 1)\n\n        This would apply the filter to df1 and df2 and return a DataFrameList with\n        the filtered data.\n\n    \"\"\"\n\n    _save_suffix = \"df_list\"\n\n    def __init__(self, df_list: list[IntoFrameT]):\n        self._df_list = df_list\n\n    def __getitem__(self, index):\n        return self._df_list[index]\n\n    def __setitem__(self, index, value):\n        self._df_list[index] = value\n\n    def __len__(self):\n        return len(self._df_list)\n\n    def append(self, df: IntoFrameT):\n        self._df_list.append(df)\n\n    def extend(self, iterable: DataFrameList | list[IntoFrameT]):\n        if type(iterable) is DataFrameList:\n            self._df_list.extend(iterable._df_list)\n        else:\n            self._df_list.extend(iterable)\n\n    def __add__(self, other: DataFrameList | list[IntoFrameT]):\n        if type(other) is DataFrameList:\n            self._df_list = self._df_list + other._df_list\n            return self\n        else:\n            self._df_list = self._df_list + other\n            return self\n\n    def __repr__(self):\n        #   Mimic how lists of polars tables are displayed\n        return \",\\n\".join([dfi.__repr__() for dfi in self._df_list])\n\n    def __iter__(self):\n        return iter(self._df_list)\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Delegate attribute to polars LazyFrame | DataFrame\n            Determines lazy/data based on df_list object 0\n\n        \"\"\"\n\n        if hasattr(nw.from_native(self._df_list[0]), attr):\n            this_attr = getattr(nw.from_native(self._df_list[0]), attr)\n            if callable(this_attr):\n\n                def wrapper(*args, **kwargs):\n                    output = [\n                        getattr(nw.from_native(dfi), attr)(*args, **kwargs)\n                        for dfi in self._df_list\n                    ]\n                    if isinstance(output[0], (nw.LazyFrame, nw.DataFrame)):\n                        return DataFrameList([dfi.to_native() for dfi in output])\n                    else:\n                        return DataFrameList([itemi for itemi in output])\n\n                return wrapper\n            else:\n                output = [getattr(nw.from_native(dfi), attr) for dfi in self._df_list]\n                if isinstance(output[0], (nw.LazyFrame, nw.DataFrame)):\n                    return DataFrameList([dfi.to_native() for dfi in output])\n                else:\n                    return DataFrameList([itemi for itemi in output])\n\n        else:\n            raise AttributeError(\n                f\"{type(nw.from_native(self._df_list[0]))} has no attribute '{attr}'\"\n            )\n\n    def join_to_list(\n        self,\n        df_join: list[IntoFrameT],\n        on: list[str],\n        how: str,\n        suffixes: list[str] | None = None,\n        prefixes: list[str] | None = None,\n    ) -&gt; DataFrameList:\n        prefixes = list_input(prefixes)\n        suffixes = list_input(suffixes)\n\n        if len(prefixes) and len(prefixes) == len(df_join):\n            prefixes = [\"\"] + prefixes\n        if len(suffixes) and len(suffixes) == len(df_join):\n            suffixes = [\"\"] + suffixes\n\n        for i in range(0, len(self._df_list)):\n            self._df_list[i] = join_list(\n                [self._df_list[i]] + df_join,\n                on=on,\n                how=how,\n                prefixes=prefixes,\n                suffixes=suffixes,\n            )\n        return self\n\n    def append_list(self, df_append: list[IntoFrameT]) -&gt; DataFrameList:\n        for i in range(0, len(self._df_list)):\n            self._df_list[i] = concat_wrapper(\n                df_list=[self._df_list[i]] + df_append, how=\"diagonal\"\n            )\n\n        return self\n\n    def calculate_stats(\n        self,\n        statistics: list[Statistics] | Statistics | None = None,\n        weight: str = \"\",\n        scale_wgts_to: float = 0.0,\n        summarize_by: dict[str, list[str]] | None = None,\n        display: bool = True,\n        display_all_vars: bool = True,\n        display_max_vars: int = 20,\n        round_output: bool | int = True,\n    ) -&gt; DataFrameList:\n        def stats_for_one(df: IntoFrameT):\n            sc = StatCalculator(\n                df=df,\n                statistics=statistics,\n                weight=weight,\n                scale_wgts_to=scale_wgts_to,\n                summarize_by=summarize_by,\n                display=display,\n                display_all_vars=display_all_vars,\n                display_max_vars=display_max_vars,\n                round_output=round_output,\n            )\n\n            return sc.df_estimates\n\n        return self.pipe(stats_for_one)\n\n    def calculate_stats_average(\n        self,\n        statistics: list[Statistics] | Statistics | None = None,\n        weight: str = \"\",\n        scale_wgts_to: float = 0.0,\n        summarize_by: dict[str, list[str]] | None = None,\n        display: bool = True,\n        display_all_vars: bool = True,\n        display_max_vars: int = 20,\n        round_output: bool | int = True,\n    ) -&gt; StatCalculator:\n        df_list = self.calculate_stats(\n            statistics=statistics,\n            weight=weight,\n            scale_wgts_to=scale_wgts_to,\n            summarize_by=summarize_by,\n            display=False,\n            display_all_vars=display_all_vars,\n            display_max_vars=display_max_vars,\n            round_output=round_output,\n        )\n\n        sc = StatCalculator(\n            df=self[0],\n            statistics=statistics,\n            weight=weight,\n            scale_wgts_to=scale_wgts_to,\n            summarize_by=summarize_by,\n            display=False,\n            display_all_vars=display_all_vars,\n            display_max_vars=display_max_vars,\n            round_output=round_output,\n            calculate=False,\n        )\n        sc.df_estimates = df_list.average()\n\n        if display:\n            sc.print()\n\n        return sc\n\n    def average(self, order_by: list[str] | str | None = None) -&gt; IntoFrameT:\n        index = \"__df_average_row_index__\"\n        order_by = list_input(order_by)\n        if len(order_by) == 0:\n            df_list = [\n                nw.from_native(dfi).lazy().collect().with_row_index(name=index)\n                for dfi in self._df_list\n            ]\n        else:\n            df_list = [\n                nw.from_native(dfi).lazy().with_row_index(name=index, order_by=order_by)\n                for dfi in self._df_list\n            ]\n        df = concat_wrapper(df_list, how=\"diagonal\")\n\n        all_cols = df.collect_schema().names()\n\n        order_by_full = order_by + [index]\n        cols_numeric = (\n            df.select(cs.numeric() | cs.boolean())\n            .drop(order_by + [index])\n            .collect_schema()\n            .names()\n        )\n        cols_first = columns_from_list(\n            df, columns=\"*\", exclude=cols_numeric + order_by_full\n        )\n\n        with_agg = []\n        if len(cols_first):\n            with_agg.append(nw.col(cols_first).first())\n        with_agg.append(nw.col(cols_numeric).mean())\n        df_out = nw.from_native(df).lazy().group_by(order_by_full).agg(with_agg)\n\n        return df_out.select(all_cols).sort(order_by_full).drop(index).to_native()\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.compress.compress_df","title":"compress_df","text":"<pre><code>compress_df(\n    df: IntoFrameT,\n    cols: list[str] | str | None = None,\n    check_string: bool = False,\n    check_string_only: bool = False,\n    cast_all_null_to_boolean: bool = True,\n    check_date_time: bool = True,\n    no_boolean: bool = False,\n) -&gt; IntoFrameT\n</code></pre> <p>Optimize DataFrame by downcasting numeric types to smallest possible representation.</p> <p>Analyzes numeric columns and casts them to the smallest data type that can accommodate all values, reducing memory usage and file sizes. Has some optional parameters to handle figuring out the optimal compress for stata</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Input data</p> required <code>cols</code> <code>list[str]</code> <p>Specific columns to compress (default: all)</p> <code>None</code> <code>check_string</code> <code>bool</code> <p>Attempt to convert string columns to numeric</p> <code>False</code> <code>check_string_only</code> <code>bool</code> <p>Only check string conversions</p> <code>False</code> <code>cast_all_null_to_boolean</code> <code>bool</code> <p>Cast all-null columns to boolean</p> <code>True</code> <code>check_date_time</code> <code>bool</code> <p>Optimize datetime columns</p> <code>True</code> <code>no_boolean</code> <code>bool</code> <p>Skip boolean type casting (and leave as int8)</p> <code>False</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>Compressed DataFrame with optimized data types</p> <p>Examples:</p> <p>Basic compression:</p> <pre><code>&gt;&gt;&gt; compressed_df = compress_df(df)\n</code></pre> <p>String to numeric conversion:</p> <pre><code>&gt;&gt;&gt; compressed_df = compress_df(df, check_string=True)\n</code></pre> Notes <p>Automatically detects the smallest integer type that can hold all values in each column, considering ranges like Int8 (-128 to 127), Int16, etc.</p> Source code in <code>src\\survey_kit\\utilities\\compress.py</code> <pre><code>def compress_df(\n    df: IntoFrameT,\n    cols: list[str] | str | None = None,\n    check_string: bool = False,\n    check_string_only: bool = False,\n    cast_all_null_to_boolean: bool = True,\n    check_date_time: bool = True,\n    no_boolean: bool = False,\n) -&gt; IntoFrameT:\n    \"\"\"\n    Optimize DataFrame by downcasting numeric types to smallest possible representation.\n\n    Analyzes numeric columns and casts them to the smallest data type that can\n    accommodate all values, reducing memory usage and file sizes.\n    Has some optional parameters to handle figuring out the optimal compress\n    for stata\n\n    Parameters\n    ----------\n    df : IntoFrameT\n        Input data\n    cols : list[str], optional\n        Specific columns to compress (default: all)\n    check_string : bool\n        Attempt to convert string columns to numeric\n    check_string_only : bool\n        Only check string conversions\n    cast_all_null_to_boolean : bool\n        Cast all-null columns to boolean\n    check_date_time : bool\n        Optimize datetime columns\n    no_boolean : bool\n        Skip boolean type casting (and leave as int8)\n\n    Returns\n    -------\n    IntoFrameT\n        Compressed DataFrame with optimized data types\n\n    Examples\n    --------\n    Basic compression:\n\n    &gt;&gt;&gt; compressed_df = compress_df(df)\n\n    String to numeric conversion:\n\n    &gt;&gt;&gt; compressed_df = compress_df(df, check_string=True)\n\n    Notes\n    -----\n    Automatically detects the smallest integer type that can hold all values\n    in each column, considering ranges like Int8 (-128 to 127), Int16, etc.\n    \"\"\"\n\n    nw_type = NarwhalsType(df)\n    df = nw_type.to_polars()\n\n    if check_date_time:\n        df = _compress_datetime(df)\n\n    #   Convert numerics\n    intlist = {}\n    if not no_boolean:\n        intlist[pl.Boolean] = [0, 1]\n    intlist[pl.Int8] = [-(2**7), 2**7 - 1]\n    intlist[pl.Int16] = [-(2**15), 2**15 - 1]\n    intlist[pl.Int32] = [-(2**31), 2**31 - 1]\n    intlist[pl.Int64] = [-(2**63), 2**63 - 1]\n\n    if cols is None:\n        cols = df.lazy().collect_schema().names()\n\n    for columni in cols:\n        cast_complete = False\n        plType = df.lazy().collect_schema()[columni]\n\n        df_col = df.select(pl.col(columni))\n\n        check_integers = False\n        check_float32 = False\n\n        maxValue = None\n        minValue = None\n\n        if check_string and (plType == pl.Utf8 or plType == pl.String):\n            numeric_string = False\n            try:\n                df_col = (\n                    df_col.select(\n                        pl.col(columni).str.strip_chars().cast(pl.Float64, strict=True)\n                    )\n                    .lazy()\n                    .collect()\n                )\n                numeric_string = True\n            except:\n                pass\n\n            if numeric_string:\n                plType = pl.Float64\n                df = df.with_columns(df_col[columni].cast(plType).alias(columni))\n\n        if plType == pl.Float64:\n            check_integers = True\n            check_float32 = False\n\n            plType_intsize = 65\n        elif plType == pl.Float32:\n            check_integers = True\n            check_float32 = False\n\n            plType_intsize = 65\n\n        elif plType == pl.Int64 or plType == pl.UInt64:\n            check_integers = True\n\n            plType_intsize = 64\n        elif plType == pl.Int32 or plType == pl.UInt32:\n            check_integers = True\n\n            plType_intsize = 32\n        elif plType == pl.Int16 or plType == pl.UInt16:\n            check_integers = True\n\n            plType_intsize = 16\n        elif plType == pl.Int8 or plType == pl.UInt8:\n            check_integers = True\n\n            plType_intsize = 8\n\n        #   First check integers\n        if check_integers and not check_string_only:\n            df_col = df_col.lazy().collect()\n\n            dfCastCheck = df_col.filter(pl.col(columni).is_not_null())\n\n            if dfCastCheck.height == 0 and df_col.height != 0:\n                #   All missing, cast to bool (for later combinations to ignore)?\n                if cast_all_null_to_boolean:\n                    try:\n                        #   Try casting on the non-null values\n                        dfCastCheck = dfCastCheck.select(\n                            pl.col(columni).cast(pl.Boolean, strict=True)\n                        )\n                        dfCastCheck = None\n                        #   Worked - then we're good to do on all of them\n                        dfcast = df_col.select(\n                            pl.col(columni).cast(pl.Boolean, strict=True)\n                        )\n                        # logger.info(\"     Cast \" + columni + \" as \" + str(inti))\n                        cast_complete = True\n                    except:\n                        pass\n                        #   logger.warning(\"     Cannot cast \" + columni + \" as \" + str(pl.Boolean))\n            else:\n                #   All integers?\n                if plType == pl.Float32 or plType == pl.Float64:\n                    bAllIntegers = (\n                        dfCastCheck.with_columns(pl.col(columni).mod(1) == 0).sum()[\n                            0, 0\n                        ]\n                        == dfCastCheck.height\n                    )\n                else:\n                    bAllIntegers = True\n\n                #   Only downcast to an integer if all are integers\n                if bAllIntegers:\n                    maxValue = dfCastCheck.max().row(0)[0]\n                    minValue = dfCastCheck.min().row(0)[0]\n\n                    if minValue is not None:\n                        for inti in intlist:\n                            if inti == pl.Boolean:\n                                intSize = 1\n                            else:\n                                intSize = int(str(inti).replace(\"Int\", \"\"))\n\n                            if plType_intsize &gt; intSize:\n                                #   logger.info(intlist[inti])\n                                lowerbound = intlist[inti][0]\n                                upperbound = intlist[inti][1]\n                                #   logger.info(lowerbound)\n                                #   logger.info(upperbound)\n\n                                if maxValue &lt;= upperbound and minValue &gt;= lowerbound:\n                                    #   logger.info(\"in range for \" + str(inti))\n\n                                    try:\n                                        #   Try casting on the non-null values\n                                        dfCastCheck = dfCastCheck.select(\n                                            pl.col(columni).cast(inti, strict=True)\n                                        )\n                                        dfCastCheck = None\n                                        #   Worked - then we're good to do on all of them\n                                        dfcast = df_col.select(\n                                            pl.col(columni).cast(inti, strict=True)\n                                        )\n                                        # logger.info(\"     Cast \" + columni + \" as \" + str(inti))\n                                        cast_complete = True\n\n                                    except:\n                                        logger.warning(\n                                            \"     Cannot cast \"\n                                            + columni\n                                            + \" as \"\n                                            + str(inti)\n                                        )\n\n                                    if cast_complete:\n                                        break\n\n        if not cast_complete and check_float32 and minValue is not None:\n            df_col = df_col.lazy().collect()\n            try:\n                dfcast = df_col.select(pl.col(columni).cast(pl.Float32, strict=True))\n                # logger.info(\"     Cast \" + columni + \" as \" + str(pl.Float32))\n                cast_complete = True\n            except:\n                logger.warning(\"     Cannot cast \" + columni + \" as \" + str(pl.Float32))\n\n        if cast_complete:\n            df = df.with_columns(dfcast[columni].alias(columni))\n\n    df = nw_type.from_polars(df)\n\n    return NarwhalsType.return_df(df, nw_type)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random","title":"random","text":""},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData","title":"RandomData","text":"<p>Generate random data, in a slightly easier way.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>Number of rows in the data to be generated</p> required <code>seed</code> <code>int</code> <p>For replicability, set the seed. Default is 0 (which does not set the seed)</p> <code>0</code> <p>Examples:</p> <p>Create a dataframe with random variables:</p> <pre><code>&gt;&gt;&gt; nRows = 10000\n&gt;&gt;&gt; df = (RandomData(n_rows=nRows, seed=89465551)\n...       .index(\"index\")\n...       .integer(\"year\", lower=2015, upper=2020)\n...       .float(\"var1\", 0, 100000)\n...       .integer(\"var2\", 1, 100000)\n...       .integer(\"var3\", 1, 50)\n...       .float(\"var4\", 0, 1)\n...       .date(\"date\", date(2020, 1, 1), date(2025, 12, 31))\n...       .datetime(\"datetime\", date(2020, 1, 1), date(2025, 12, 31))\n...       .np_distribution(\"v_normal\", \"normal\", dict(loc=1, scale=2))\n...       .np_distribution(\"v_lognormal\", \"lognormal\", dict(mean=1, sigma=2))\n...       .to_df()\n... )\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>class RandomData:\n    \"\"\"\n    Generate random data, in a slightly easier way.\n\n    Parameters\n    ----------\n    n_rows : int\n        Number of rows in the data to be generated\n    seed : int, optional\n        For replicability, set the seed.\n        Default is 0 (which does not set the seed)\n\n    Examples\n    --------\n    Create a dataframe with random variables:\n\n    &gt;&gt;&gt; nRows = 10000\n    &gt;&gt;&gt; df = (RandomData(n_rows=nRows, seed=89465551)\n    ...       .index(\"index\")\n    ...       .integer(\"year\", lower=2015, upper=2020)\n    ...       .float(\"var1\", 0, 100000)\n    ...       .integer(\"var2\", 1, 100000)\n    ...       .integer(\"var3\", 1, 50)\n    ...       .float(\"var4\", 0, 1)\n    ...       .date(\"date\", date(2020, 1, 1), date(2025, 12, 31))\n    ...       .datetime(\"datetime\", date(2020, 1, 1), date(2025, 12, 31))\n    ...       .np_distribution(\"v_normal\", \"normal\", dict(loc=1, scale=2))\n    ...       .np_distribution(\"v_lognormal\", \"lognormal\", dict(mean=1, sigma=2))\n    ...       .to_df()\n    ... )\n    \"\"\"\n\n    def __init__(self, n_rows: int, seed: int = 0):\n        if seed &gt; 0:\n            set_seed(seed)\n\n        self.rng = RandomNumberGenerator()\n        self.n_rows = n_rows\n        self._data = {}\n\n    def index(self, name: str) -&gt; RandomData:\n        \"\"\"\n        Create an index column (i.e. 0-n_rows-1)\n\n        Parameters\n        ----------\n        name : str\n            column name\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n        self._data[name] = range(0, self.n_rows)\n\n        return self\n\n    def boolean(self, name: str) -&gt; RandomData:\n        \"\"\"\n        Create an boolean column\n\n        Parameters\n        ----------\n        name : str\n            column name\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n\n        self._data[name] = np.ceil(self.rng.uniform(low=-1, high=1, size=self.n_rows))\n\n        return self\n\n    def integer(self, name: str, lower: int, upper: int) -&gt; RandomData:\n        \"\"\"\n        Create an integer column in [lower,upper]\n\n        Parameters\n        ----------\n        name : str\n            column name\n        lower : int\n            lower bound (inclusive)\n        upper : int\n            upper bound (inclusive)\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n\n        self._data[name] = np.ceil(\n            self.rng.uniform(low=lower - 1, high=upper, size=self.n_rows)\n        )\n\n        return self\n\n    def float(self, name: str, lower: float, upper: float) -&gt; RandomData:\n        \"\"\"\n        Create a float64 column in (lower,upper)\n\n        Parameters\n        ----------\n        name : str\n            column name\n        lower : float\n            lower bound\n        upper : float\n            upper bound\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n\n        self._data[name] = self.rng.uniform(low=lower, high=upper, size=self.n_rows)\n\n        return self\n\n    def date(self, name: str, start: date, end: date) -&gt; RandomData:\n        \"\"\"\n        Create a date column in [start,end]\n\n        Parameters\n        ----------\n        name : str\n            column name\n        start : date\n            lower bound\n        end : date\n            upper bound\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n        self._data[name] = pl.date_range(start, end, \"1d\", eager=True).sample(\n            n=self.n_rows, with_replacement=True\n        )\n\n        return self\n\n    def datetime(\n        self, name: str, start: datetime | date, end: datetime | date\n    ) -&gt; RandomData:\n        \"\"\"\n        Create a datetime column in [start,end]\n\n        Parameters\n        ----------\n        name : str\n            column name\n        start : datetime | date\n            lower bound\n        end : datetime | date\n            upper bound\n\n        Returns\n        -------\n        RandomData object (so you can chain 's)\n        \"\"\"\n\n        self._data[name] = pl.datetime_range(start, end, \"1m\", eager=True).sample(\n            n=self.n_rows, with_replacement=True\n        )\n\n        return self\n\n    def np_distribution(self, name: str, distribution: str, **kwargs) -&gt; RandomData:\n        \"\"\"\n        Create a column from a numpy distribution.\n\n        Parameters\n        ----------\n        name : str\n            Column name.\n        distribution : str\n            Name of numpy random distribution (e.g., \"normal\", \"lognormal\", \"exponential\").\n            Must be a valid method of np.random.Generator.\n        **kwargs\n            Keyword arguments passed to the distribution function (e.g., loc, scale for normal).\n\n        Returns\n        -------\n        RandomData\n            Self for method chaining.\n\n        Raises\n        ------\n        Exception\n            If distribution is not a valid numpy random distribution.\n\n        Examples\n        --------\n        &gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n        ...       .np_distribution(\"normal_var\", \"normal\", loc=0, scale=1)\n        ...       .np_distribution(\"lognormal_var\", \"lognormal\", mean=1, sigma=2)\n        ...       .np_distribution(\"exponential_var\", \"exponential\", scale=1.5)\n        ...       .to_df())\n        \"\"\"\n\n        if hasattr(self.rng, distribution):\n            generator = getattr(self.rng, distribution)\n            self._data[name] = generator(size=self.n_rows, **kwargs)\n        else:\n            message = f\"Numpy generator does not have the function '{distribution}'\"\n            logger.error(message)\n            raise Exception(message)\n\n        return self\n\n    def to_df(self, compress: bool = True) -&gt; pl.DataFrame:\n        \"\"\"\n        Convert accumulated random data to a Polars DataFrame.\n\n        Parameters\n        ----------\n        compress : bool, optional\n            Apply compression to reduce memory usage by downcasting numeric types.\n            Default is True.\n\n        Returns\n        -------\n        pl.DataFrame\n            DataFrame containing all generated columns.\n\n        Examples\n        --------\n        &gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n        ...       .integer(\"id\", 1, 1000)\n        ...       .float(\"value\", 0, 100)\n        ...       .to_df())\n        &gt;&gt;&gt; print(df)\n\n        &gt;&gt;&gt; # Without compression\n        &gt;&gt;&gt; df_uncompressed = (RandomData(n_rows=1000, seed=123)\n        ...                    .integer(\"id\", 1, 1000)\n        ...                    .to_df(compress=False))\n        \"\"\"\n\n        df = pl.DataFrame(self._data)\n        if compress:\n            return compress_df(df)\n        else:\n            return df\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.boolean","title":"boolean","text":"<pre><code>boolean(name: str) -&gt; RandomData\n</code></pre> <p>Create an boolean column</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def boolean(self, name: str) -&gt; RandomData:\n    \"\"\"\n    Create an boolean column\n\n    Parameters\n    ----------\n    name : str\n        column name\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n\n    self._data[name] = np.ceil(self.rng.uniform(low=-1, high=1, size=self.n_rows))\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.date","title":"date","text":"<pre><code>date(name: str, start: date, end: date) -&gt; RandomData\n</code></pre> <p>Create a date column in [start,end]</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <code>start</code> <code>date</code> <p>lower bound</p> required <code>end</code> <code>date</code> <p>upper bound</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def date(self, name: str, start: date, end: date) -&gt; RandomData:\n    \"\"\"\n    Create a date column in [start,end]\n\n    Parameters\n    ----------\n    name : str\n        column name\n    start : date\n        lower bound\n    end : date\n        upper bound\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n    self._data[name] = pl.date_range(start, end, \"1d\", eager=True).sample(\n        n=self.n_rows, with_replacement=True\n    )\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.datetime","title":"datetime","text":"<pre><code>datetime(\n    name: str, start: datetime | date, end: datetime | date\n) -&gt; RandomData\n</code></pre> <p>Create a datetime column in [start,end]</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <code>start</code> <code>datetime | date</code> <p>lower bound</p> required <code>end</code> <code>datetime | date</code> <p>upper bound</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def datetime(\n    self, name: str, start: datetime | date, end: datetime | date\n) -&gt; RandomData:\n    \"\"\"\n    Create a datetime column in [start,end]\n\n    Parameters\n    ----------\n    name : str\n        column name\n    start : datetime | date\n        lower bound\n    end : datetime | date\n        upper bound\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n\n    self._data[name] = pl.datetime_range(start, end, \"1m\", eager=True).sample(\n        n=self.n_rows, with_replacement=True\n    )\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.float","title":"float","text":"<pre><code>float(name: str, lower: float, upper: float) -&gt; RandomData\n</code></pre> <p>Create a float64 column in (lower,upper)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <code>lower</code> <code>float</code> <p>lower bound</p> required <code>upper</code> <code>float</code> <p>upper bound</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def float(self, name: str, lower: float, upper: float) -&gt; RandomData:\n    \"\"\"\n    Create a float64 column in (lower,upper)\n\n    Parameters\n    ----------\n    name : str\n        column name\n    lower : float\n        lower bound\n    upper : float\n        upper bound\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n\n    self._data[name] = self.rng.uniform(low=lower, high=upper, size=self.n_rows)\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.index","title":"index","text":"<pre><code>index(name: str) -&gt; RandomData\n</code></pre> <p>Create an index column (i.e. 0-n_rows-1)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def index(self, name: str) -&gt; RandomData:\n    \"\"\"\n    Create an index column (i.e. 0-n_rows-1)\n\n    Parameters\n    ----------\n    name : str\n        column name\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n    self._data[name] = range(0, self.n_rows)\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.integer","title":"integer","text":"<pre><code>integer(name: str, lower: int, upper: int) -&gt; RandomData\n</code></pre> <p>Create an integer column in [lower,upper]</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>column name</p> required <code>lower</code> <code>int</code> <p>lower bound (inclusive)</p> required <code>upper</code> <code>int</code> <p>upper bound (inclusive)</p> required <p>Returns:</p> Type Description <code>RandomData object (so you can chain 's)</code> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def integer(self, name: str, lower: int, upper: int) -&gt; RandomData:\n    \"\"\"\n    Create an integer column in [lower,upper]\n\n    Parameters\n    ----------\n    name : str\n        column name\n    lower : int\n        lower bound (inclusive)\n    upper : int\n        upper bound (inclusive)\n\n    Returns\n    -------\n    RandomData object (so you can chain 's)\n    \"\"\"\n\n    self._data[name] = np.ceil(\n        self.rng.uniform(low=lower - 1, high=upper, size=self.n_rows)\n    )\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.np_distribution","title":"np_distribution","text":"<pre><code>np_distribution(\n    name: str, distribution: str, **kwargs\n) -&gt; RandomData\n</code></pre> <p>Create a column from a numpy distribution.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Column name.</p> required <code>distribution</code> <code>str</code> <p>Name of numpy random distribution (e.g., \"normal\", \"lognormal\", \"exponential\"). Must be a valid method of np.random.Generator.</p> required <code>**kwargs</code> <p>Keyword arguments passed to the distribution function (e.g., loc, scale for normal).</p> <code>{}</code> <p>Returns:</p> Type Description <code>RandomData</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If distribution is not a valid numpy random distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n...       .np_distribution(\"normal_var\", \"normal\", loc=0, scale=1)\n...       .np_distribution(\"lognormal_var\", \"lognormal\", mean=1, sigma=2)\n...       .np_distribution(\"exponential_var\", \"exponential\", scale=1.5)\n...       .to_df())\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def np_distribution(self, name: str, distribution: str, **kwargs) -&gt; RandomData:\n    \"\"\"\n    Create a column from a numpy distribution.\n\n    Parameters\n    ----------\n    name : str\n        Column name.\n    distribution : str\n        Name of numpy random distribution (e.g., \"normal\", \"lognormal\", \"exponential\").\n        Must be a valid method of np.random.Generator.\n    **kwargs\n        Keyword arguments passed to the distribution function (e.g., loc, scale for normal).\n\n    Returns\n    -------\n    RandomData\n        Self for method chaining.\n\n    Raises\n    ------\n    Exception\n        If distribution is not a valid numpy random distribution.\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n    ...       .np_distribution(\"normal_var\", \"normal\", loc=0, scale=1)\n    ...       .np_distribution(\"lognormal_var\", \"lognormal\", mean=1, sigma=2)\n    ...       .np_distribution(\"exponential_var\", \"exponential\", scale=1.5)\n    ...       .to_df())\n    \"\"\"\n\n    if hasattr(self.rng, distribution):\n        generator = getattr(self.rng, distribution)\n        self._data[name] = generator(size=self.n_rows, **kwargs)\n    else:\n        message = f\"Numpy generator does not have the function '{distribution}'\"\n        logger.error(message)\n        raise Exception(message)\n\n    return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomData.to_df","title":"to_df","text":"<pre><code>to_df(compress: bool = True) -&gt; pl.DataFrame\n</code></pre> <p>Convert accumulated random data to a Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>compress</code> <code>bool</code> <p>Apply compression to reduce memory usage by downcasting numeric types. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing all generated columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n...       .integer(\"id\", 1, 1000)\n...       .float(\"value\", 0, 100)\n...       .to_df())\n&gt;&gt;&gt; print(df)\n</code></pre> <pre><code>&gt;&gt;&gt; # Without compression\n&gt;&gt;&gt; df_uncompressed = (RandomData(n_rows=1000, seed=123)\n...                    .integer(\"id\", 1, 1000)\n...                    .to_df(compress=False))\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def to_df(self, compress: bool = True) -&gt; pl.DataFrame:\n    \"\"\"\n    Convert accumulated random data to a Polars DataFrame.\n\n    Parameters\n    ----------\n    compress : bool, optional\n        Apply compression to reduce memory usage by downcasting numeric types.\n        Default is True.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame containing all generated columns.\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = (RandomData(n_rows=1000, seed=123)\n    ...       .integer(\"id\", 1, 1000)\n    ...       .float(\"value\", 0, 100)\n    ...       .to_df())\n    &gt;&gt;&gt; print(df)\n\n    &gt;&gt;&gt; # Without compression\n    &gt;&gt;&gt; df_uncompressed = (RandomData(n_rows=1000, seed=123)\n    ...                    .integer(\"id\", 1, 1000)\n    ...                    .to_df(compress=False))\n    \"\"\"\n\n    df = pl.DataFrame(self._data)\n    if compress:\n        return compress_df(df)\n    else:\n        return df\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.RandomNumberGenerator","title":"RandomNumberGenerator","text":"<pre><code>RandomNumberGenerator() -&gt; np.random.Generator\n</code></pre> <p>Create a new numpy random number generator with random seed.</p> <p>Returns:</p> Type Description <code>Generator</code> <p>Numpy random number generator initialized with a random seed from Python's random module.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.random import RandomNumberGenerator\n&gt;&gt;&gt; rng = RandomNumberGenerator()\n&gt;&gt;&gt; random_values = rng.normal(loc=0, scale=1, size=100)\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def RandomNumberGenerator() -&gt; np.random.Generator:\n    \"\"\"\n    Create a new numpy random number generator with random seed.\n\n    Returns\n    -------\n    np.random.Generator\n        Numpy random number generator initialized with a random seed\n        from Python's random module.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.utilities.random import RandomNumberGenerator\n    &gt;&gt;&gt; rng = RandomNumberGenerator()\n    &gt;&gt;&gt; random_values = rng.normal(loc=0, scale=1, size=100)\n    \"\"\"\n    return np.random.default_rng(random.randint(1, 2**63 - 1))\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.generate_seed","title":"generate_seed","text":"<pre><code>generate_seed(power_of_2_limit: int = 32)\n</code></pre> <p>Generate a random seed value.</p> <p>Parameters:</p> Name Type Description Default <code>power_of_2_limit</code> <code>int</code> <p>Upper bound is 2^power_of_2_limit. Default is 32.</p> <code>32</code> <p>Returns:</p> Type Description <code>int</code> <p>Random integer between 1 and 2^power_of_2_limit - 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.random import generate_seed, set_seed\n&gt;&gt;&gt; seed = generate_seed()\n&gt;&gt;&gt; set_seed(seed)  # Use for reproducibility\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def generate_seed(power_of_2_limit: int = 32):\n    \"\"\"\n    Generate a random seed value.\n\n    Parameters\n    ----------\n    power_of_2_limit : int, optional\n        Upper bound is 2^power_of_2_limit. Default is 32.\n\n    Returns\n    -------\n    int\n        Random integer between 1 and 2^power_of_2_limit - 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.utilities.random import generate_seed, set_seed\n    &gt;&gt;&gt; seed = generate_seed()\n    &gt;&gt;&gt; set_seed(seed)  # Use for reproducibility\n    \"\"\"\n    rng = RandomNumberGenerator()\n    return int(rng.integers(1, 2**power_of_2_limit - 1, 1)[0])\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.get_random_state","title":"get_random_state","text":"<pre><code>get_random_state()\n</code></pre> <p>Get the current state of the random number generator.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>Internal state of Python's random module.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.random import get_random_state, set_random_state\n&gt;&gt;&gt; state = get_random_state()\n&gt;&gt;&gt; # ... do some random operations ...\n&gt;&gt;&gt; set_random_state(state)  # Restore to previous state\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def get_random_state():\n    \"\"\"\n    Get the current state of the random number generator.\n\n    Returns\n    -------\n    tuple\n        Internal state of Python's random module.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.utilities.random import get_random_state, set_random_state\n    &gt;&gt;&gt; state = get_random_state()\n    &gt;&gt;&gt; # ... do some random operations ...\n    &gt;&gt;&gt; set_random_state(state)  # Restore to previous state\n    \"\"\"\n    return random.getstate()\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.set_random_state","title":"set_random_state","text":"<pre><code>set_random_state(value)\n</code></pre> <p>Set the random number generator to a specific state.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>tuple</code> <p>State tuple from get_random_state().</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; state = get_random_state()\n&gt;&gt;&gt; # ... do some random operations ...\n&gt;&gt;&gt; set_random_state(state)  # Restore to previous state\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def set_random_state(value):\n    \"\"\"\n    Set the random number generator to a specific state.\n\n    Parameters\n    ----------\n    value : tuple\n        State tuple from get_random_state().\n\n    Examples\n    --------\n    &gt;&gt;&gt; state = get_random_state()\n    &gt;&gt;&gt; # ... do some random operations ...\n    &gt;&gt;&gt; set_random_state(state)  # Restore to previous state\n    \"\"\"\n    random.setstate(value)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.random.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int = 0)\n</code></pre> <p>Set the random seed for reproducibility.</p> <p>Sets the seed for Python's random module, which is used by the random number generator in this module.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed. If 0 or less, does not set seed. Default is 0.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.random import set_seed, generate_seed\n&gt;&gt;&gt; set_seed(12345)\n&gt;&gt;&gt; seed1 = generate_seed()\n&gt;&gt;&gt; set_seed(12345)\n&gt;&gt;&gt; seed2 = generate_seed()\n&gt;&gt;&gt; assert seed1 == seed2  # Same seed produces same sequence\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\random.py</code> <pre><code>def set_seed(seed: int = 0):\n    \"\"\"\n    Set the random seed for reproducibility.\n\n    Sets the seed for Python's random module, which is used by the\n    random number generator in this module.\n\n    Parameters\n    ----------\n    seed : int, optional\n        Random seed. If 0 or less, does not set seed. Default is 0.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from survey_kit.utilities.random import set_seed, generate_seed\n    &gt;&gt;&gt; set_seed(12345)\n    &gt;&gt;&gt; seed1 = generate_seed()\n    &gt;&gt;&gt; set_seed(12345)\n    &gt;&gt;&gt; seed2 = generate_seed()\n    &gt;&gt;&gt; assert seed1 == seed2  # Same seed produces same sequence\n    \"\"\"\n\n    if seed &gt; 0:\n        random.seed(seed)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder","title":"FormulaBuilder","text":"<p>Build and manipulate R-style formulas for statistical models.</p> <p>FormulaBuilder provides a programmatic way to construct complex model formulas using R/Patsy-style syntax. It supports formula manipulation, variable expansion, interactions, transformations, and pattern matching against dataframes.</p> <p>The class works with Formulaic to parse and expand formulas, and integrates with the dataframe utilities to resolve wildcards and column patterns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Reference dataframe for resolving column names and wildcards. Default is None.</p> <code>None</code> <code>formula</code> <code>str</code> <p>Initial formula string. If empty, constructs from lhs and constant. Default is \"\".</p> <code>''</code> <code>lhs</code> <code>str</code> <p>Left-hand side of formula (response variable). Default is \"\".</p> <code>''</code> <code>constant</code> <code>bool</code> <p>Include intercept term (1) or suppress it (0). Default is True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>formula</code> <code>str</code> <p>Current formula string.</p> <code>df</code> <code>IntoFrameT | None</code> <p>Reference dataframe.</p> <code>columns</code> <code>list[str]</code> <p>All variables required by the formula.</p> <code>columns_rhs</code> <code>list[str]</code> <p>Variables in right-hand side of formula.</p> <code>columns_lhs</code> <code>list[str]</code> <p>Variables in left-hand side of formula.</p> <p>Examples:</p> <p>Basic formula construction:</p> <pre><code>&gt;&gt;&gt; from survey_kit.utilities.formula import FormulaBuilder\n&gt;&gt;&gt; \n&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\", constant=True)\n&gt;&gt;&gt; fb += \"age + education\"\n&gt;&gt;&gt; print(fb.formula)\n'income~1+age+education'\n</code></pre> <p>Add variables using wildcards:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n&gt;&gt;&gt; fb.continuous(columns=\"demographic_*\")\n&gt;&gt;&gt; fb.factor(columns=\"region\")\n&gt;&gt;&gt; print(fb.formula)\n</code></pre> <p>Polynomial terms:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n&gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=3)\n&gt;&gt;&gt; print(fb.formula)\n'income~1+poly(age,degree=3,raw=True)'\n</code></pre> <p>Interactions:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n&gt;&gt;&gt; fb.simple_interaction(columns=[\"age\", \"education\"], order=2)\n&gt;&gt;&gt; print(fb.formula)\n'income~1+(age+education)**2'\n</code></pre> <p>Standardization:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n&gt;&gt;&gt; fb.scale(columns=[\"age\", \"experience\"])\n&gt;&gt;&gt; print(fb.formula)\n'income~1+scale(age)+scale(experience)'\n</code></pre> <p>Working with existing formula strings:</p> <pre><code>&gt;&gt;&gt; formula = \"income~age+education+age:education\"\n&gt;&gt;&gt; fb = FormulaBuilder(df=df, formula=formula)\n&gt;&gt;&gt; fb.expand()  # Expand shorthand\n&gt;&gt;&gt; print(fb.rhs())  # Get right-hand side\n'age+education+age:education'\n&gt;&gt;&gt; print(fb.columns)\n['income', 'age', 'education']\n</code></pre> Notes <p>Formula syntax follows R/Patsy conventions: - <code>~</code> separates left and right sides - <code>+</code> adds terms - <code>:</code> creates interactions - <code>*</code> creates main effects and interactions: <code>a*b</code> = <code>a+b+a:b</code> - <code>**n</code> creates all n-way interactions - <code>I()</code> for arithmetic operations - <code>C()</code> for categorical variables - Functions like <code>scale()</code>, <code>center()</code>, <code>poly()</code> for transformations</p> <p>Wildcards in column specifications: - <code>\"income_*\"</code> matches all columns starting with \"income_\" - <code>[\"age\", \"education_*\"]</code> matches age and all education columns</p> <p>The FormulaBuilder can be used in two modes: 1. Object mode: Create instance and chain methods 2. Static mode: Call methods with self=None for one-off operations</p> See Also <p>formulaic.Formula : Underlying formula parser</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>class FormulaBuilder:\n    \"\"\"\n    Build and manipulate R-style formulas for statistical models.\n\n    FormulaBuilder provides a programmatic way to construct complex model formulas\n    using R/Patsy-style syntax. It supports formula manipulation, variable expansion,\n    interactions, transformations, and pattern matching against dataframes.\n\n    The class works with Formulaic to parse and expand formulas, and integrates\n    with the dataframe utilities to resolve wildcards and column patterns.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Reference dataframe for resolving column names and wildcards.\n        Default is None.\n    formula : str, optional\n        Initial formula string. If empty, constructs from lhs and constant.\n        Default is \"\".\n    lhs : str, optional\n        Left-hand side of formula (response variable). Default is \"\".\n    constant : bool, optional\n        Include intercept term (1) or suppress it (0). Default is True.\n\n    Attributes\n    ----------\n    formula : str\n        Current formula string.\n    df : IntoFrameT | None\n        Reference dataframe.\n    columns : list[str]\n        All variables required by the formula.\n    columns_rhs : list[str]\n        Variables in right-hand side of formula.\n    columns_lhs : list[str]\n        Variables in left-hand side of formula.\n\n    Examples\n    --------\n    Basic formula construction:\n\n    &gt;&gt;&gt; from survey_kit.utilities.formula import FormulaBuilder\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\", constant=True)\n    &gt;&gt;&gt; fb += \"age + education\"\n    &gt;&gt;&gt; print(fb.formula)\n    'income~1+age+education'\n\n    Add variables using wildcards:\n\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n    &gt;&gt;&gt; fb.continuous(columns=\"demographic_*\")\n    &gt;&gt;&gt; fb.factor(columns=\"region\")\n    &gt;&gt;&gt; print(fb.formula)\n\n    Polynomial terms:\n\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n    &gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=3)\n    &gt;&gt;&gt; print(fb.formula)\n    'income~1+poly(age,degree=3,raw=True)'\n\n    Interactions:\n\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n    &gt;&gt;&gt; fb.simple_interaction(columns=[\"age\", \"education\"], order=2)\n    &gt;&gt;&gt; print(fb.formula)\n    'income~1+(age+education)**2'\n\n    Standardization:\n\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"income\")\n    &gt;&gt;&gt; fb.scale(columns=[\"age\", \"experience\"])\n    &gt;&gt;&gt; print(fb.formula)\n    'income~1+scale(age)+scale(experience)'\n\n    Working with existing formula strings:\n\n    &gt;&gt;&gt; formula = \"income~age+education+age:education\"\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, formula=formula)\n    &gt;&gt;&gt; fb.expand()  # Expand shorthand\n    &gt;&gt;&gt; print(fb.rhs())  # Get right-hand side\n    'age+education+age:education'\n    &gt;&gt;&gt; print(fb.columns)\n    ['income', 'age', 'education']\n\n    Notes\n    -----\n    Formula syntax follows R/Patsy conventions:\n    - `~` separates left and right sides\n    - `+` adds terms\n    - `:` creates interactions\n    - `*` creates main effects and interactions: `a*b` = `a+b+a:b`\n    - `**n` creates all n-way interactions\n    - `I()` for arithmetic operations\n    - `C()` for categorical variables\n    - Functions like `scale()`, `center()`, `poly()` for transformations\n\n    Wildcards in column specifications:\n    - `\"income_*\"` matches all columns starting with \"income_\"\n    - `[\"age\", \"education_*\"]` matches age and all education columns\n\n    The FormulaBuilder can be used in two modes:\n    1. Object mode: Create instance and chain methods\n    2. Static mode: Call methods with self=None for one-off operations\n\n    See Also\n    --------\n    formulaic.Formula : Underlying formula parser\n    \"\"\"\n\n    def __init__(\n        self,\n        df: IntoFrameT | None = None,\n        formula: str = \"\",\n        lhs: str = \"\",\n        constant: bool = True,\n    ):\n        if formula == \"\":\n            self.formula = f\"{lhs}~{int(constant)}\"\n        else:\n            self.formula = formula\n\n        if df is not None:\n            self.df = nw.from_native(df).head(0).to_native()\n        else:\n            self.df = None\n\n    def __str__(self):\n        return self.formula\n\n    def __add__(self, o):\n        \"\"\"Add terms to the formula using + operator.\"\"\"\n        if type(o) is FormulaBuilder:\n            o = o.rhs()\n\n        o = str(o)\n        if o.startswith(\"~\"):\n            o = o[1 : len(o)]\n\n        if o.startswith(\"1+\"):\n            o = o[2 : len(o)]\n        if o.startswith(\"0+\"):\n            o = o[2 : len(o)]\n\n        self.formula = f\"{self.formula}+{o}\"\n\n        self.expand()\n\n        return self\n\n    def add_to_formula(self, add_part: str = \"\", plus_first: bool = True) -&gt; None:\n        \"\"\"\n        Append a string to the formula.\n\n        Parameters\n        ----------\n        add_part : str, optional\n            String to append. Default is \"\".\n        plus_first : bool, optional\n            Add \"+\" before the string. Default is True.\n        \"\"\"\n        if plus_first:\n            plus = \"+\"\n        else:\n            plus = \"\"\n        self.formula += f\"{plus}{add_part}\"\n\n    def any_wrapper(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        case_insensitive: bool = False,\n        prefix: str = \"\",\n        suffix: str = \"\",\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        General wrapper for adding columns with prefix/suffix.\n\n        Used internally by other methods to add variables with transformations.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for resolving column patterns. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause (bypasses column lookup). Default is \"\".\n        case_insensitive : bool, optional\n            Case-insensitive column matching. Default is False.\n        prefix : str, optional\n            String to prepend to each column. Default is \"\".\n        suffix : str, optional\n            String to append to each column. Default is \"\".\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string if self is None, otherwise returns self for chaining.\n        \"\"\"\n        columns = list_input(columns)\n\n        #   Dataframe to look for columns in\n        if df is None:\n            df = self.df\n\n        if clause != \"\":\n            output = f\"+{prefix}{clause}{suffix}\"\n        else:\n            if df is not None:\n                columns = columns_from_list(\n                    df=df, columns=columns, case_insensitive=case_insensitive\n                )\n\n            out_list = [f\"{prefix}{coli}{suffix}\" for coli in columns]\n            output = \"+\" + \"+\".join(out_list)\n\n        if self is None:\n            return output\n        else:\n            self.formula += output\n            return self\n\n    def continuous(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        case_insensitive: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Add continuous variables to the formula.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns (e.g., \"income_*\"). Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"y\")\n        &gt;&gt;&gt; fb.continuous(columns=[\"age\", \"income\"])\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+age+income'\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        return caller.any_wrapper(\n            df=df, columns=columns, clause=clause, case_insensitive=case_insensitive\n        )\n\n    def function(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        operator_before: str = \"\",\n        operator_after: str = \"\",\n        function_item: str = \"\",\n        case_insensitive: bool = False,\n        **kwargs,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Wrap columns in a function call.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        operator_before : str, optional\n            String before function call. Default is \"\".\n        operator_after : str, optional\n            String after function call. Default is \"\".\n        function_item : str, optional\n            Function name (e.g., \"log\", \"sqrt\"). Default is \"\".\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n        **kwargs\n            Additional function arguments.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.function(columns=\"income\", function_item=\"log\")\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+log(income)'\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        if function_item != \"\":\n            operator_before = f\"{operator_before}{function_item}(\"\n\n            operator_after_final = \"\"\n            if len(kwargs):\n                for keyi, valuei in kwargs.items():\n                    operator_after_final += \",\"\n                    if type(valuei) is str:\n                        operator_after_final += f\"{keyi}='{valuei}'\"\n                    else:\n                        operator_after_final += f\"{keyi}={valuei}\"\n\n            operator_after_final += f\"{operator_after})\"\n        else:\n            operator_after_final = operator_after\n\n        return caller.any_wrapper(\n            df=df,\n            columns=columns,\n            clause=clause,\n            case_insensitive=case_insensitive,\n            prefix=f\"{{{operator_before}\",\n            suffix=f\"{operator_after_final}}}\",\n        )\n\n    def scale(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        standardize: bool = True,\n        case_insensitive: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Standardize or center variables.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        standardize : bool, optional\n            If True, standardize (mean=0, sd=1). If False, only center (mean=0).\n            Default is True.\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.scale(columns=\"age\")  # Standardize\n        &gt;&gt;&gt; fb.scale(columns=\"income\", standardize=False)  # Center only\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        if standardize:\n            function_item = \"scale\"\n        else:\n            function_item = \"center\"\n\n        return caller.any_wrapper(\n            df=df,\n            columns=columns,\n            clause=clause,\n            case_insensitive=case_insensitive,\n            prefix=f\"{function_item}(\",\n            suffix=\")\",\n        )\n\n    def center(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        case_insensitive: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Center variables (subtract mean).\n\n        Convenience method for scale(standardize=False).\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        return caller.scale(\n            df=df,\n            columns=columns,\n            clause=clause,\n            case_insensitive=case_insensitive,\n            standardize=False,\n        )\n\n    def standardize(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        case_insensitive: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Standardize variables (mean=0, sd=1).\n\n        Convenience method for scale(standardize=True).\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        return caller.scale(\n            df=df,\n            columns=columns,\n            clause=clause,\n            case_insensitive=case_insensitive,\n            standardize=True,\n        )\n\n    def polynomial(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        degree: int = 0,\n        case_insensitive: bool = False,\n        center: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Add polynomial terms.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        degree : int, optional\n            Polynomial degree. Default is 0 (returns continuous).\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n        center : bool, optional\n            Use orthogonal polynomials (centered). Default is False (raw polynomials).\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=3)\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+poly(age,degree=3,raw=True)'\n\n        &gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=2, center=True)\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+poly(age,degree=2,raw=False)'\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n            if df is None:\n                df = self.df\n\n        if degree &lt;= 1:\n            if center:\n                return caller.center(\n                    df=df,\n                    columns=columns,\n                    clause=clause,\n                    case_insensitive=case_insensitive,\n                )\n            else:\n                return caller.continuous(\n                    df=df,\n                    columns=columns,\n                    clause=clause,\n                    case_insensitive=case_insensitive,\n                )\n        else:\n            subformula = \"\"\n\n            for power in range(1, degree + 1):\n                operator_before = \"poly(\"\n                if center:\n                    operator_after = f\",degree={degree},raw=False)\"\n                else:\n                    operator_after = f\",degree={degree},raw=True)\"\n\n                subformula += FormulaBuilder.function(\n                    df=df,\n                    columns=columns,\n                    clause=clause,\n                    operator_before=operator_before,\n                    operator_after=operator_after,\n                    case_insensitive=case_insensitive,\n                )\n\n            if self is None:\n                return subformula\n            else:\n                self.add_to_formula(subformula, False)\n                return self\n\n    def simple_interaction(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        order: int = 2,\n        case_insensitive: bool = False,\n        sub_function: Callable | None = None,\n        no_base: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Create interactions between variables.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        order : int, optional\n            Interaction order. 2 = pairwise, 3 = three-way, etc. Default is 2.\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n        sub_function : Callable | None, optional\n            Function to apply to columns before interacting. Default is None.\n        no_base : bool, optional\n            Exclude main effects (only interactions). Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.simple_interaction(columns=[\"age\", \"education\"], order=2)\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+(age+education)**2'\n\n        &gt;&gt;&gt; fb.simple_interaction(columns=[\"a\", \"b\", \"c\"], order=2, no_base=True)\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+(a+b+c)**2-(a+b+c)'  # Interactions only\n        \"\"\"\n        if self is not None:\n            if df is None:\n                df = self.df\n\n        if sub_function is None:\n            sub_function = FormulaBuilder.continuous\n\n        columns = columns_from_list(\n            df=df, columns=columns, case_insensitive=case_insensitive\n        )\n\n        subformula = sub_function(\n            df=df, columns=columns, case_insensitive=case_insensitive\n        )\n        #   Remove leading plus sign\n        subformula = subformula[1 : len(subformula)]\n\n        if len(columns) &gt; 1:\n            output = f\"({subformula})**{order}\"\n        else:\n            output = subformula\n\n        if no_base:\n            output += f\"-({subformula})\"\n\n        if self is None:\n            return f\"+{output}\"\n        else:\n            self.add_to_formula(output)\n            return self\n\n    def interact_clauses(\n        self=None, clause1: str = \"\", clause2: str = \"\", no_base: bool = False\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Create interactions between two formula clauses.\n\n        Parameters\n        ----------\n        clause1 : str, optional\n            First formula clause. Default is \"\".\n        clause2 : str, optional\n            Second formula clause. Default is \"\".\n        no_base : bool, optional\n            Exclude main effects from clauses. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.interact_clauses(\"age+education\", \"region\")\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+(age+education)*(region)'\n        \"\"\"\n        if clause1.startswith(\"+\"):\n            clause1 = clause1[1:]\n        if clause2.startswith(\"+\"):\n            clause2 = clause2[1:]\n\n        clause1 = clause1.replace(\"++\", \"+\")\n        clause2 = clause2.replace(\"++\", \"+\")\n\n        output = f\"({clause1})*({clause2})\"\n\n        if no_base:\n            output += f\"-({clause1} + {clause2})\"\n\n        if self is None:\n            return f\"+{output}\"\n        else:\n            self.add_to_formula(output)\n            return self\n\n    def factor(\n        self=None,\n        df: IntoFrameT | None = None,\n        columns: str | list | None = None,\n        clause: str = \"\",\n        reference=None,\n        case_insensitive: bool = False,\n    ):\n        \"\"\"\n        Add categorical variables with treatment coding.\n\n        Parameters\n        ----------\n        df : IntoFrameT | None, optional\n            Dataframe for column lookup. Default is None.\n        columns : str | list | None, optional\n            Column names or patterns. Default is None.\n        clause : str, optional\n            Pre-constructed clause. Default is \"\".\n        reference : str | int | None, optional\n            Reference level for treatment coding. Default is None (use first level).\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb.factor(columns=\"region\")\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+C(region)'\n\n        &gt;&gt;&gt; fb.factor(columns=\"education\", reference=\"high_school\")\n        &gt;&gt;&gt; print(fb.formula)\n        \"y~1+C(education, contr.treatment('high_school'))\"\n        \"\"\"\n        if self is None:\n            caller = FormulaBuilder\n        else:\n            caller = self\n\n        prefix = \"C(\"\n        if reference is not None:\n            if type(reference) is str:\n                suffix = f\", contr.treatment('{reference}'))\"\n            else:\n                suffix = f\", contr.treatment({reference}))\"\n\n        else:\n            suffix = \")\"  #  f\", contr.treatment)\"\n\n        return caller.any_wrapper(\n            df=df, columns=columns, clause=clause, prefix=prefix, suffix=suffix\n        )\n\n    @property\n    def columns(self):\n        \"\"\"Get all variables required by the formula.\"\"\"\n        return self.columns_from_formula()\n\n    def columns_from_formula(self=None, formula: str = \"\") -&gt; list[str]:\n        \"\"\"\n        Extract variable names from a formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n\n        Returns\n        -------\n        list[str]\n            Variable names required by the formula.\n        \"\"\"\n        if type(self) is str:\n            formula = self\n            self = None\n\n        if self is not None and formula == \"\":\n            formula = self.formula\n        return list(Formula(formula).required_variables)\n\n    def lhs(self=None, formula: str = \"\") -&gt; str:\n        \"\"\"\n        Get left-hand side of formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n\n        Returns\n        -------\n        str\n            Left-hand side (response variable).\n        \"\"\"\n        if type(self) is str:\n            formula = self\n            self = None\n\n        if self is not None:\n            if formula == \"\":\n                formula = self.formula\n\n        #   Separate into subclauses\n        sides = formula.split(\"~\")\n\n        if len(sides) == 2:\n            lhs_string = sides[0]\n        else:\n            lhs_string = \"\"\n\n        return lhs_string\n\n    def rhs(self=None, formula: str = \"\") -&gt; str:\n        \"\"\"\n        Get right-hand side of formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n\n        Returns\n        -------\n        str\n            Right-hand side (predictors).\n        \"\"\"\n        if type(self) is str:\n            formula = self\n            self = None\n\n        if self is not None:\n            if formula == \"\":\n                formula = self.formula\n\n        #   Separate into subclauses\n        sides = formula.split(\"~\")\n\n        if len(sides) == 2:\n            rhs_string = sides[1]\n        else:\n            rhs_string = sides[0]\n\n        return rhs_string\n\n    @property\n    def columns_rhs(self):\n        \"\"\"Variables in right-hand side, ordered as in dataframe.\"\"\"\n        formula_rhs = self.rhs()\n        columns = FormulaBuilder.columns_from_formula(formula=formula_rhs)\n\n        if self.df is not None:\n            return _columns_original_order(\n                columns_unordered=columns,\n                columns_ordered=nw.from_native(self.df).lazy().collect_schema().names(),\n            )\n        else:\n            return columns\n\n    @property\n    def columns_lhs(self):\n        \"\"\"Variables in left-hand side, ordered as in dataframe.\"\"\"\n        formula_lhs = self.lhs()\n        if formula_lhs == \"\":\n            return []\n        else:\n            columns = FormulaBuilder.columns_from_formula(formula=formula_lhs)\n            if len(columns) &lt;= 1:\n                return columns\n\n            if self.df is not None:\n                return _columns_original_order(\n                    columns_unordered=columns,\n                    columns_ordered=nw.from_native(self.df)\n                    .lazy()\n                    .collect_schema()\n                    .names(),\n                )\n            else:\n                return columns\n\n    def has_constant(\n        self=None, formula: str = \"\", true_if_missing: bool = False\n    ) -&gt; bool:\n        \"\"\"\n        Check if formula includes an intercept.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n        true_if_missing : bool, optional\n            Return True if constant term is implicit (not specified).\n            Default is False.\n\n        Returns\n        -------\n        bool\n            True if formula includes intercept.\n        \"\"\"\n        if type(self) is str:\n            formula = self\n            self = None\n\n        if self is not None:\n            if formula == \"\":\n                formula = self.formula\n\n        #   Separate into subclauses\n        sides = formula.split(\"~\")\n        if len(sides) == 2:\n            lhs = sides[0]\n            rhs = sides[1]\n        else:\n            lhs = None\n            rhs = sides[0]\n\n        if true_if_missing:\n            return not rhs.replace(\" \", \"\").startswith(\"0\")\n        else:\n            return rhs.replace(\" \", \"\").startswith(\"1\")\n\n    def remove_constant(self):\n        \"\"\"Remove intercept from formula (change ~1+ to ~0+).\"\"\"\n        self.formula = self.formula.replace(\"~1+\", \"~\")\n        self.formula = self.formula.replace(\"~0+\", \"~\")\n        self.formula = self.formula.replace(\"~\", \"~0+\")\n\n    def expand(self=None, formula: str = \"\"):\n        \"\"\"\n        Expand formula shorthand (e.g., a*b becomes a+b+a:b).\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n\n        Returns\n        -------\n        str\n            Expanded formula string.\n        \"\"\"\n        if self is not None:\n            if formula == \"\":\n                formula = self.formula\n        else:\n            self = FormulaBuilder(formula=formula)\n\n        lhs = self.lhs()\n        rhs = self.rhs()\n\n        parser = DefaultFormulaParser()\n        parsed = parser.get_terms(rhs)\n        reconstructed_formula = \"+\".join([str(i) for i in parsed])\n\n        if lhs != \"\":\n            reconstructed_formula = f\"{lhs}~{reconstructed_formula}\"\n\n        if self is not None:\n            self.formula = reconstructed_formula\n\n        return reconstructed_formula\n\n    def exclude_interactions(\n        self=None,\n        formula: str = \"\",\n        b_exclude_powers: bool = True,\n        df: IntoFrameT | None = None,\n    ) -&gt; tuple[str, bool]:\n        \"\"\"\n        Remove interaction terms from formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n        b_exclude_powers : bool, optional\n            Also exclude polynomial terms. Default is True.\n        df : IntoFrameT | None, optional\n            Reference dataframe. Default is None.\n\n        Returns\n        -------\n        tuple[str, bool]\n            (modified formula, whether any terms were dropped)\n        \"\"\"\n        if self is not None:\n            if df is None:\n                df = self.df\n            if formula == \"\":\n                formula = self.formula\n        else:\n            self = FormulaBuilder(df=df, formula=formula)\n\n        #   It's easier with the expanded formula\n        self.expand()\n\n        #   Separate into subclauses\n        sides = self.formula.split(\"~\")\n        if len(sides) == 2:\n            lhs = sides[0]\n            rhs = sides[1]\n        else:\n            lhs = \"\"\n            rhs = sides[0]\n        subclauses = rhs.split(\"+\")\n\n        rhs = \"\"\n\n        any_dropped = False\n\n        for clausei in subclauses:\n            if b_exclude_powers and \"^\" in clausei:\n                any_dropped = True\n                logger.info(\n                    f\"Dropping {clausei} from formula for having a variable to a power\"\n                )\n            elif \":\" in clausei:\n                #   Direct interaction\n                any_dropped = True\n                logger.info(f\"Dropping {clausei} from formula\")\n            else:\n                #   include this in the final formula\n                rhs += f\"+{clausei.strip()}\"\n\n        #   Get rid of leading +\n        rhs = rhs[1 : len(rhs)]\n\n        output = f\"{lhs}~{rhs}\"\n\n        self.formula = output\n        return (output, any_dropped)\n\n    def exclude_variables(\n        self=None,\n        exclude_list: list = None,\n        formula: str = \"\",\n        df: IntoFrameT | None = None,\n        case_insensitive: bool = False,\n    ):\n        \"\"\"\n        Remove specific variables from formula.\n\n        Parameters\n        ----------\n        exclude_list : list, optional\n            Variables to exclude. Default is None.\n        formula : str, optional\n            Formula string. If empty, uses self.formula. Default is \"\".\n        df : IntoFrameT | None, optional\n            Reference dataframe. Default is None.\n        case_insensitive : bool, optional\n            Case-insensitive matching. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Modified formula string or self for chaining.\n        \"\"\"\n        if exclude_list is None:\n            exclude_list = []\n\n        if self is not None:\n            if df is None:\n                df = self.df\n            if formula == \"\":\n                formula = self.formula\n\n        if df is not None:\n            exclude_list = columns_from_list(\n                df=df, columns=exclude_list, case_insensitive=case_insensitive\n            )\n\n        #   Separate into subclauses\n        sides = formula.split(\"~\")\n        if len(sides) == 2:\n            lhs = sides[0]\n            rhs = sides[1]\n        else:\n            lhs = None\n            rhs = sides[0]\n        subclauses = rhs.split(\"+\")\n\n        rhs = \"\"\n\n        regex_list = [\n            f\"(^|([^a-zA-Z0-9_])){itemi}($|([^a-zA-Z0-9_]))\" for itemi in exclude_list\n        ]\n        regexes = \"(\" + \")|(\".join(regex_list) + \")\"\n\n        for clausei in subclauses:\n            if re.match(regexes, clausei) is None:\n                #   include this in the final formula\n                rhs += f\"+{clausei}\"\n            else:\n                logger.info(f\"Dropping {clausei} from formula\")\n\n        #   Get rid of leading +\n        rhs = rhs[1 : len(rhs)]\n\n        if lhs is not None:\n            output = f\"{lhs}~{rhs}\"\n        else:\n            output = rhs\n\n        if self is None:\n            return output\n        else:\n            self.formula = output\n            return self\n\n    def formula_with_varnames_in_brackets(\n        self=None,\n        clause: str = \"\",\n        df: pl.LazyFrame | pl.DataFrame | None = None,\n        case_insensitive: bool = False,\n        append: bool = False,\n    ) -&gt; str | FormulaBuilder:\n        \"\"\"\n        Expand {pattern} placeholders with matching column names.\n\n        Replaces {var*} with all columns matching var* pattern in the dataframe.\n        Useful for programmatically building formulas with wildcards.\n\n        Parameters\n        ----------\n        clause : str, optional\n            Formula clause with {pattern} placeholders. Default is \"\".\n        df : pl.LazyFrame | pl.DataFrame | None, optional\n            Dataframe for column lookup. Default is None.\n        case_insensitive : bool, optional\n            Case-insensitive pattern matching. Default is False.\n        append : bool, optional\n            Append to existing formula instead of replacing.\n            Only used when called on instance. Default is False.\n\n        Returns\n        -------\n        str | FormulaBuilder\n            Expanded formula string or self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; fb = FormulaBuilder(df=df)\n        &gt;&gt;&gt; fb.formula_with_varnames_in_brackets(\"{income_*} + age\")\n        &gt;&gt;&gt; print(fb.formula)\n        'y~1+income_wages+income_self_employment+age'\n        \"\"\"\n\n        call_recursively = False\n        if df is None and self is not None:\n            df = self.df\n\n        #   Separate into subclauses\n        sides = clause.split(\"~\")\n        if len(sides) == 2:\n            lhs = sides[0]\n            rhs = sides[1]\n        else:\n            lhs = None\n            rhs = sides[0]\n        subclauses = rhs.split(\"+\")\n\n        rhs = \"\"\n        for clausei in subclauses:\n            replaced_clause = \"\"\n\n            left_bracket = clausei.find(\"{\")\n            right_bracket = clausei.find(\"}\")\n\n            if left_bracket &gt;= 0 and right_bracket &gt;= 0:\n                replace_string = clausei[left_bracket : right_bracket + 1]\n                var_name = replace_string[1 : len(replace_string) - 1]\n                Columns = columns_from_list(\n                    df=df, columns=[var_name], case_insensitive=case_insensitive\n                )\n\n                for coli in Columns:\n                    if replaced_clause != \"\":\n                        replaced_clause += \"+\"\n                    replaced_clause += clausei.replace(replace_string, coli)\n\n                clausei = replaced_clause\n\n            call_recursively = clausei.find(\"{\") &gt;= 0 and clausei.find(\"}\") &gt;= 0\n            rhs += f\"+{clausei}\"\n\n        #   Get rid of leading +\n        rhs = rhs[1 : len(rhs)]\n\n        if lhs is not None:\n            output = f\"{lhs}~{rhs}\"\n        else:\n            output = rhs\n\n        if call_recursively:\n            output = FormulaBuilder.formula_with_varnames_in_brackets(\n                clause=output, df=df, case_insensitive=case_insensitive\n            )\n\n        if self is None:\n            return output\n        else:\n            if append:\n                self.add_to_formula(output)\n            else:\n                self.formula = output\n            return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.columns","title":"columns  <code>property</code>","text":"<pre><code>columns\n</code></pre> <p>Get all variables required by the formula.</p>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.columns_lhs","title":"columns_lhs  <code>property</code>","text":"<pre><code>columns_lhs\n</code></pre> <p>Variables in left-hand side, ordered as in dataframe.</p>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.columns_rhs","title":"columns_rhs  <code>property</code>","text":"<pre><code>columns_rhs\n</code></pre> <p>Variables in right-hand side, ordered as in dataframe.</p>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.add_to_formula","title":"add_to_formula","text":"<pre><code>add_to_formula(\n    add_part: str = \"\", plus_first: bool = True\n) -&gt; None\n</code></pre> <p>Append a string to the formula.</p> <p>Parameters:</p> Name Type Description Default <code>add_part</code> <code>str</code> <p>String to append. Default is \"\".</p> <code>''</code> <code>plus_first</code> <code>bool</code> <p>Add \"+\" before the string. Default is True.</p> <code>True</code> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def add_to_formula(self, add_part: str = \"\", plus_first: bool = True) -&gt; None:\n    \"\"\"\n    Append a string to the formula.\n\n    Parameters\n    ----------\n    add_part : str, optional\n        String to append. Default is \"\".\n    plus_first : bool, optional\n        Add \"+\" before the string. Default is True.\n    \"\"\"\n    if plus_first:\n        plus = \"+\"\n    else:\n        plus = \"\"\n    self.formula += f\"{plus}{add_part}\"\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.any_wrapper","title":"any_wrapper","text":"<pre><code>any_wrapper(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n    prefix: str = \"\",\n    suffix: str = \"\",\n) -&gt; str | FormulaBuilder\n</code></pre> <p>General wrapper for adding columns with prefix/suffix.</p> <p>Used internally by other methods to add variables with transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for resolving column patterns. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause (bypasses column lookup). Default is \"\".</p> <code>''</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive column matching. Default is False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>String to prepend to each column. Default is \"\".</p> <code>''</code> <code>suffix</code> <code>str</code> <p>String to append to each column. Default is \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string if self is None, otherwise returns self for chaining.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def any_wrapper(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n    prefix: str = \"\",\n    suffix: str = \"\",\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    General wrapper for adding columns with prefix/suffix.\n\n    Used internally by other methods to add variables with transformations.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for resolving column patterns. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause (bypasses column lookup). Default is \"\".\n    case_insensitive : bool, optional\n        Case-insensitive column matching. Default is False.\n    prefix : str, optional\n        String to prepend to each column. Default is \"\".\n    suffix : str, optional\n        String to append to each column. Default is \"\".\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string if self is None, otherwise returns self for chaining.\n    \"\"\"\n    columns = list_input(columns)\n\n    #   Dataframe to look for columns in\n    if df is None:\n        df = self.df\n\n    if clause != \"\":\n        output = f\"+{prefix}{clause}{suffix}\"\n    else:\n        if df is not None:\n            columns = columns_from_list(\n                df=df, columns=columns, case_insensitive=case_insensitive\n            )\n\n        out_list = [f\"{prefix}{coli}{suffix}\" for coli in columns]\n        output = \"+\" + \"+\".join(out_list)\n\n    if self is None:\n        return output\n    else:\n        self.formula += output\n        return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.center","title":"center","text":"<pre><code>center(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Center variables (subtract mean).</p> <p>Convenience method for scale(standardize=False).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def center(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Center variables (subtract mean).\n\n    Convenience method for scale(standardize=False).\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    return caller.scale(\n        df=df,\n        columns=columns,\n        clause=clause,\n        case_insensitive=case_insensitive,\n        standardize=False,\n    )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.columns_from_formula","title":"columns_from_formula","text":"<pre><code>columns_from_formula(formula: str = '') -&gt; list[str]\n</code></pre> <p>Extract variable names from a formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Variable names required by the formula.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def columns_from_formula(self=None, formula: str = \"\") -&gt; list[str]:\n    \"\"\"\n    Extract variable names from a formula.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n\n    Returns\n    -------\n    list[str]\n        Variable names required by the formula.\n    \"\"\"\n    if type(self) is str:\n        formula = self\n        self = None\n\n    if self is not None and formula == \"\":\n        formula = self.formula\n    return list(Formula(formula).required_variables)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.continuous","title":"continuous","text":"<pre><code>continuous(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Add continuous variables to the formula.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns (e.g., \"income_*\"). Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"y\")\n&gt;&gt;&gt; fb.continuous(columns=[\"age\", \"income\"])\n&gt;&gt;&gt; print(fb.formula)\n'y~1+age+income'\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def continuous(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Add continuous variables to the formula.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns (e.g., \"income_*\"). Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df, lhs=\"y\")\n    &gt;&gt;&gt; fb.continuous(columns=[\"age\", \"income\"])\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+age+income'\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    return caller.any_wrapper(\n        df=df, columns=columns, clause=clause, case_insensitive=case_insensitive\n    )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.exclude_interactions","title":"exclude_interactions","text":"<pre><code>exclude_interactions(\n    formula: str = \"\",\n    b_exclude_powers: bool = True,\n    df: IntoFrameT | None = None,\n) -&gt; tuple[str, bool]\n</code></pre> <p>Remove interaction terms from formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <code>b_exclude_powers</code> <code>bool</code> <p>Also exclude polynomial terms. Default is True.</p> <code>True</code> <code>df</code> <code>IntoFrameT | None</code> <p>Reference dataframe. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>(modified formula, whether any terms were dropped)</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def exclude_interactions(\n    self=None,\n    formula: str = \"\",\n    b_exclude_powers: bool = True,\n    df: IntoFrameT | None = None,\n) -&gt; tuple[str, bool]:\n    \"\"\"\n    Remove interaction terms from formula.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n    b_exclude_powers : bool, optional\n        Also exclude polynomial terms. Default is True.\n    df : IntoFrameT | None, optional\n        Reference dataframe. Default is None.\n\n    Returns\n    -------\n    tuple[str, bool]\n        (modified formula, whether any terms were dropped)\n    \"\"\"\n    if self is not None:\n        if df is None:\n            df = self.df\n        if formula == \"\":\n            formula = self.formula\n    else:\n        self = FormulaBuilder(df=df, formula=formula)\n\n    #   It's easier with the expanded formula\n    self.expand()\n\n    #   Separate into subclauses\n    sides = self.formula.split(\"~\")\n    if len(sides) == 2:\n        lhs = sides[0]\n        rhs = sides[1]\n    else:\n        lhs = \"\"\n        rhs = sides[0]\n    subclauses = rhs.split(\"+\")\n\n    rhs = \"\"\n\n    any_dropped = False\n\n    for clausei in subclauses:\n        if b_exclude_powers and \"^\" in clausei:\n            any_dropped = True\n            logger.info(\n                f\"Dropping {clausei} from formula for having a variable to a power\"\n            )\n        elif \":\" in clausei:\n            #   Direct interaction\n            any_dropped = True\n            logger.info(f\"Dropping {clausei} from formula\")\n        else:\n            #   include this in the final formula\n            rhs += f\"+{clausei.strip()}\"\n\n    #   Get rid of leading +\n    rhs = rhs[1 : len(rhs)]\n\n    output = f\"{lhs}~{rhs}\"\n\n    self.formula = output\n    return (output, any_dropped)\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.exclude_variables","title":"exclude_variables","text":"<pre><code>exclude_variables(\n    exclude_list: list = None,\n    formula: str = \"\",\n    df: IntoFrameT | None = None,\n    case_insensitive: bool = False,\n)\n</code></pre> <p>Remove specific variables from formula.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_list</code> <code>list</code> <p>Variables to exclude. Default is None.</p> <code>None</code> <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <code>df</code> <code>IntoFrameT | None</code> <p>Reference dataframe. Default is None.</p> <code>None</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Modified formula string or self for chaining.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def exclude_variables(\n    self=None,\n    exclude_list: list = None,\n    formula: str = \"\",\n    df: IntoFrameT | None = None,\n    case_insensitive: bool = False,\n):\n    \"\"\"\n    Remove specific variables from formula.\n\n    Parameters\n    ----------\n    exclude_list : list, optional\n        Variables to exclude. Default is None.\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n    df : IntoFrameT | None, optional\n        Reference dataframe. Default is None.\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Modified formula string or self for chaining.\n    \"\"\"\n    if exclude_list is None:\n        exclude_list = []\n\n    if self is not None:\n        if df is None:\n            df = self.df\n        if formula == \"\":\n            formula = self.formula\n\n    if df is not None:\n        exclude_list = columns_from_list(\n            df=df, columns=exclude_list, case_insensitive=case_insensitive\n        )\n\n    #   Separate into subclauses\n    sides = formula.split(\"~\")\n    if len(sides) == 2:\n        lhs = sides[0]\n        rhs = sides[1]\n    else:\n        lhs = None\n        rhs = sides[0]\n    subclauses = rhs.split(\"+\")\n\n    rhs = \"\"\n\n    regex_list = [\n        f\"(^|([^a-zA-Z0-9_])){itemi}($|([^a-zA-Z0-9_]))\" for itemi in exclude_list\n    ]\n    regexes = \"(\" + \")|(\".join(regex_list) + \")\"\n\n    for clausei in subclauses:\n        if re.match(regexes, clausei) is None:\n            #   include this in the final formula\n            rhs += f\"+{clausei}\"\n        else:\n            logger.info(f\"Dropping {clausei} from formula\")\n\n    #   Get rid of leading +\n    rhs = rhs[1 : len(rhs)]\n\n    if lhs is not None:\n        output = f\"{lhs}~{rhs}\"\n    else:\n        output = rhs\n\n    if self is None:\n        return output\n    else:\n        self.formula = output\n        return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.expand","title":"expand","text":"<pre><code>expand(formula: str = '')\n</code></pre> <p>Expand formula shorthand (e.g., a*b becomes a+b+a:b).</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>Expanded formula string.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def expand(self=None, formula: str = \"\"):\n    \"\"\"\n    Expand formula shorthand (e.g., a*b becomes a+b+a:b).\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n\n    Returns\n    -------\n    str\n        Expanded formula string.\n    \"\"\"\n    if self is not None:\n        if formula == \"\":\n            formula = self.formula\n    else:\n        self = FormulaBuilder(formula=formula)\n\n    lhs = self.lhs()\n    rhs = self.rhs()\n\n    parser = DefaultFormulaParser()\n    parsed = parser.get_terms(rhs)\n    reconstructed_formula = \"+\".join([str(i) for i in parsed])\n\n    if lhs != \"\":\n        reconstructed_formula = f\"{lhs}~{reconstructed_formula}\"\n\n    if self is not None:\n        self.formula = reconstructed_formula\n\n    return reconstructed_formula\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.factor","title":"factor","text":"<pre><code>factor(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    reference=None,\n    case_insensitive: bool = False,\n)\n</code></pre> <p>Add categorical variables with treatment coding.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>reference</code> <code>str | int | None</code> <p>Reference level for treatment coding. Default is None (use first level).</p> <code>None</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.factor(columns=\"region\")\n&gt;&gt;&gt; print(fb.formula)\n'y~1+C(region)'\n</code></pre> <pre><code>&gt;&gt;&gt; fb.factor(columns=\"education\", reference=\"high_school\")\n&gt;&gt;&gt; print(fb.formula)\n\"y~1+C(education, contr.treatment('high_school'))\"\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def factor(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    reference=None,\n    case_insensitive: bool = False,\n):\n    \"\"\"\n    Add categorical variables with treatment coding.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    reference : str | int | None, optional\n        Reference level for treatment coding. Default is None (use first level).\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.factor(columns=\"region\")\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+C(region)'\n\n    &gt;&gt;&gt; fb.factor(columns=\"education\", reference=\"high_school\")\n    &gt;&gt;&gt; print(fb.formula)\n    \"y~1+C(education, contr.treatment('high_school'))\"\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    prefix = \"C(\"\n    if reference is not None:\n        if type(reference) is str:\n            suffix = f\", contr.treatment('{reference}'))\"\n        else:\n            suffix = f\", contr.treatment({reference}))\"\n\n    else:\n        suffix = \")\"  #  f\", contr.treatment)\"\n\n    return caller.any_wrapper(\n        df=df, columns=columns, clause=clause, prefix=prefix, suffix=suffix\n    )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.formula_with_varnames_in_brackets","title":"formula_with_varnames_in_brackets","text":"<pre><code>formula_with_varnames_in_brackets(\n    clause: str = \"\",\n    df: LazyFrame | DataFrame | None = None,\n    case_insensitive: bool = False,\n    append: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Expand {pattern} placeholders with matching column names.</p> <p>Replaces {var} with all columns matching var pattern in the dataframe. Useful for programmatically building formulas with wildcards.</p> <p>Parameters:</p> Name Type Description Default <code>clause</code> <code>str</code> <p>Formula clause with {pattern} placeholders. Default is \"\".</p> <code>''</code> <code>df</code> <code>LazyFrame | DataFrame | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive pattern matching. Default is False.</p> <code>False</code> <code>append</code> <code>bool</code> <p>Append to existing formula instead of replacing. Only used when called on instance. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Expanded formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb = FormulaBuilder(df=df)\n&gt;&gt;&gt; fb.formula_with_varnames_in_brackets(\"{income_*} + age\")\n&gt;&gt;&gt; print(fb.formula)\n'y~1+income_wages+income_self_employment+age'\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def formula_with_varnames_in_brackets(\n    self=None,\n    clause: str = \"\",\n    df: pl.LazyFrame | pl.DataFrame | None = None,\n    case_insensitive: bool = False,\n    append: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Expand {pattern} placeholders with matching column names.\n\n    Replaces {var*} with all columns matching var* pattern in the dataframe.\n    Useful for programmatically building formulas with wildcards.\n\n    Parameters\n    ----------\n    clause : str, optional\n        Formula clause with {pattern} placeholders. Default is \"\".\n    df : pl.LazyFrame | pl.DataFrame | None, optional\n        Dataframe for column lookup. Default is None.\n    case_insensitive : bool, optional\n        Case-insensitive pattern matching. Default is False.\n    append : bool, optional\n        Append to existing formula instead of replacing.\n        Only used when called on instance. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Expanded formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb = FormulaBuilder(df=df)\n    &gt;&gt;&gt; fb.formula_with_varnames_in_brackets(\"{income_*} + age\")\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+income_wages+income_self_employment+age'\n    \"\"\"\n\n    call_recursively = False\n    if df is None and self is not None:\n        df = self.df\n\n    #   Separate into subclauses\n    sides = clause.split(\"~\")\n    if len(sides) == 2:\n        lhs = sides[0]\n        rhs = sides[1]\n    else:\n        lhs = None\n        rhs = sides[0]\n    subclauses = rhs.split(\"+\")\n\n    rhs = \"\"\n    for clausei in subclauses:\n        replaced_clause = \"\"\n\n        left_bracket = clausei.find(\"{\")\n        right_bracket = clausei.find(\"}\")\n\n        if left_bracket &gt;= 0 and right_bracket &gt;= 0:\n            replace_string = clausei[left_bracket : right_bracket + 1]\n            var_name = replace_string[1 : len(replace_string) - 1]\n            Columns = columns_from_list(\n                df=df, columns=[var_name], case_insensitive=case_insensitive\n            )\n\n            for coli in Columns:\n                if replaced_clause != \"\":\n                    replaced_clause += \"+\"\n                replaced_clause += clausei.replace(replace_string, coli)\n\n            clausei = replaced_clause\n\n        call_recursively = clausei.find(\"{\") &gt;= 0 and clausei.find(\"}\") &gt;= 0\n        rhs += f\"+{clausei}\"\n\n    #   Get rid of leading +\n    rhs = rhs[1 : len(rhs)]\n\n    if lhs is not None:\n        output = f\"{lhs}~{rhs}\"\n    else:\n        output = rhs\n\n    if call_recursively:\n        output = FormulaBuilder.formula_with_varnames_in_brackets(\n            clause=output, df=df, case_insensitive=case_insensitive\n        )\n\n    if self is None:\n        return output\n    else:\n        if append:\n            self.add_to_formula(output)\n        else:\n            self.formula = output\n        return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.function","title":"function","text":"<pre><code>function(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    operator_before: str = \"\",\n    operator_after: str = \"\",\n    function_item: str = \"\",\n    case_insensitive: bool = False,\n    **kwargs,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Wrap columns in a function call.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>operator_before</code> <code>str</code> <p>String before function call. Default is \"\".</p> <code>''</code> <code>operator_after</code> <code>str</code> <p>String after function call. Default is \"\".</p> <code>''</code> <code>function_item</code> <code>str</code> <p>Function name (e.g., \"log\", \"sqrt\"). Default is \"\".</p> <code>''</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional function arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.function(columns=\"income\", function_item=\"log\")\n&gt;&gt;&gt; print(fb.formula)\n'y~1+log(income)'\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def function(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    operator_before: str = \"\",\n    operator_after: str = \"\",\n    function_item: str = \"\",\n    case_insensitive: bool = False,\n    **kwargs,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Wrap columns in a function call.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    operator_before : str, optional\n        String before function call. Default is \"\".\n    operator_after : str, optional\n        String after function call. Default is \"\".\n    function_item : str, optional\n        Function name (e.g., \"log\", \"sqrt\"). Default is \"\".\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n    **kwargs\n        Additional function arguments.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.function(columns=\"income\", function_item=\"log\")\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+log(income)'\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    if function_item != \"\":\n        operator_before = f\"{operator_before}{function_item}(\"\n\n        operator_after_final = \"\"\n        if len(kwargs):\n            for keyi, valuei in kwargs.items():\n                operator_after_final += \",\"\n                if type(valuei) is str:\n                    operator_after_final += f\"{keyi}='{valuei}'\"\n                else:\n                    operator_after_final += f\"{keyi}={valuei}\"\n\n        operator_after_final += f\"{operator_after})\"\n    else:\n        operator_after_final = operator_after\n\n    return caller.any_wrapper(\n        df=df,\n        columns=columns,\n        clause=clause,\n        case_insensitive=case_insensitive,\n        prefix=f\"{{{operator_before}\",\n        suffix=f\"{operator_after_final}}}\",\n    )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.has_constant","title":"has_constant","text":"<pre><code>has_constant(\n    formula: str = \"\", true_if_missing: bool = False\n) -&gt; bool\n</code></pre> <p>Check if formula includes an intercept.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <code>true_if_missing</code> <code>bool</code> <p>Return True if constant term is implicit (not specified). Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if formula includes intercept.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def has_constant(\n    self=None, formula: str = \"\", true_if_missing: bool = False\n) -&gt; bool:\n    \"\"\"\n    Check if formula includes an intercept.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n    true_if_missing : bool, optional\n        Return True if constant term is implicit (not specified).\n        Default is False.\n\n    Returns\n    -------\n    bool\n        True if formula includes intercept.\n    \"\"\"\n    if type(self) is str:\n        formula = self\n        self = None\n\n    if self is not None:\n        if formula == \"\":\n            formula = self.formula\n\n    #   Separate into subclauses\n    sides = formula.split(\"~\")\n    if len(sides) == 2:\n        lhs = sides[0]\n        rhs = sides[1]\n    else:\n        lhs = None\n        rhs = sides[0]\n\n    if true_if_missing:\n        return not rhs.replace(\" \", \"\").startswith(\"0\")\n    else:\n        return rhs.replace(\" \", \"\").startswith(\"1\")\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.interact_clauses","title":"interact_clauses","text":"<pre><code>interact_clauses(\n    clause1: str = \"\",\n    clause2: str = \"\",\n    no_base: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Create interactions between two formula clauses.</p> <p>Parameters:</p> Name Type Description Default <code>clause1</code> <code>str</code> <p>First formula clause. Default is \"\".</p> <code>''</code> <code>clause2</code> <code>str</code> <p>Second formula clause. Default is \"\".</p> <code>''</code> <code>no_base</code> <code>bool</code> <p>Exclude main effects from clauses. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.interact_clauses(\"age+education\", \"region\")\n&gt;&gt;&gt; print(fb.formula)\n'y~1+(age+education)*(region)'\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def interact_clauses(\n    self=None, clause1: str = \"\", clause2: str = \"\", no_base: bool = False\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Create interactions between two formula clauses.\n\n    Parameters\n    ----------\n    clause1 : str, optional\n        First formula clause. Default is \"\".\n    clause2 : str, optional\n        Second formula clause. Default is \"\".\n    no_base : bool, optional\n        Exclude main effects from clauses. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.interact_clauses(\"age+education\", \"region\")\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+(age+education)*(region)'\n    \"\"\"\n    if clause1.startswith(\"+\"):\n        clause1 = clause1[1:]\n    if clause2.startswith(\"+\"):\n        clause2 = clause2[1:]\n\n    clause1 = clause1.replace(\"++\", \"+\")\n    clause2 = clause2.replace(\"++\", \"+\")\n\n    output = f\"({clause1})*({clause2})\"\n\n    if no_base:\n        output += f\"-({clause1} + {clause2})\"\n\n    if self is None:\n        return f\"+{output}\"\n    else:\n        self.add_to_formula(output)\n        return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.lhs","title":"lhs","text":"<pre><code>lhs(formula: str = '') -&gt; str\n</code></pre> <p>Get left-hand side of formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>Left-hand side (response variable).</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def lhs(self=None, formula: str = \"\") -&gt; str:\n    \"\"\"\n    Get left-hand side of formula.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n\n    Returns\n    -------\n    str\n        Left-hand side (response variable).\n    \"\"\"\n    if type(self) is str:\n        formula = self\n        self = None\n\n    if self is not None:\n        if formula == \"\":\n            formula = self.formula\n\n    #   Separate into subclauses\n    sides = formula.split(\"~\")\n\n    if len(sides) == 2:\n        lhs_string = sides[0]\n    else:\n        lhs_string = \"\"\n\n    return lhs_string\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.polynomial","title":"polynomial","text":"<pre><code>polynomial(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    degree: int = 0,\n    case_insensitive: bool = False,\n    center: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Add polynomial terms.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>degree</code> <code>int</code> <p>Polynomial degree. Default is 0 (returns continuous).</p> <code>0</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <code>center</code> <code>bool</code> <p>Use orthogonal polynomials (centered). Default is False (raw polynomials).</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=3)\n&gt;&gt;&gt; print(fb.formula)\n'y~1+poly(age,degree=3,raw=True)'\n</code></pre> <pre><code>&gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=2, center=True)\n&gt;&gt;&gt; print(fb.formula)\n'y~1+poly(age,degree=2,raw=False)'\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def polynomial(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    degree: int = 0,\n    case_insensitive: bool = False,\n    center: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Add polynomial terms.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    degree : int, optional\n        Polynomial degree. Default is 0 (returns continuous).\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n    center : bool, optional\n        Use orthogonal polynomials (centered). Default is False (raw polynomials).\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=3)\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+poly(age,degree=3,raw=True)'\n\n    &gt;&gt;&gt; fb.polynomial(columns=\"age\", degree=2, center=True)\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+poly(age,degree=2,raw=False)'\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n        if df is None:\n            df = self.df\n\n    if degree &lt;= 1:\n        if center:\n            return caller.center(\n                df=df,\n                columns=columns,\n                clause=clause,\n                case_insensitive=case_insensitive,\n            )\n        else:\n            return caller.continuous(\n                df=df,\n                columns=columns,\n                clause=clause,\n                case_insensitive=case_insensitive,\n            )\n    else:\n        subformula = \"\"\n\n        for power in range(1, degree + 1):\n            operator_before = \"poly(\"\n            if center:\n                operator_after = f\",degree={degree},raw=False)\"\n            else:\n                operator_after = f\",degree={degree},raw=True)\"\n\n            subformula += FormulaBuilder.function(\n                df=df,\n                columns=columns,\n                clause=clause,\n                operator_before=operator_before,\n                operator_after=operator_after,\n                case_insensitive=case_insensitive,\n            )\n\n        if self is None:\n            return subformula\n        else:\n            self.add_to_formula(subformula, False)\n            return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.remove_constant","title":"remove_constant","text":"<pre><code>remove_constant()\n</code></pre> <p>Remove intercept from formula (change ~1+ to ~0+).</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def remove_constant(self):\n    \"\"\"Remove intercept from formula (change ~1+ to ~0+).\"\"\"\n    self.formula = self.formula.replace(\"~1+\", \"~\")\n    self.formula = self.formula.replace(\"~0+\", \"~\")\n    self.formula = self.formula.replace(\"~\", \"~0+\")\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.rhs","title":"rhs","text":"<pre><code>rhs(formula: str = '') -&gt; str\n</code></pre> <p>Get right-hand side of formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Formula string. If empty, uses self.formula. Default is \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>Right-hand side (predictors).</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def rhs(self=None, formula: str = \"\") -&gt; str:\n    \"\"\"\n    Get right-hand side of formula.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Formula string. If empty, uses self.formula. Default is \"\".\n\n    Returns\n    -------\n    str\n        Right-hand side (predictors).\n    \"\"\"\n    if type(self) is str:\n        formula = self\n        self = None\n\n    if self is not None:\n        if formula == \"\":\n            formula = self.formula\n\n    #   Separate into subclauses\n    sides = formula.split(\"~\")\n\n    if len(sides) == 2:\n        rhs_string = sides[1]\n    else:\n        rhs_string = sides[0]\n\n    return rhs_string\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.scale","title":"scale","text":"<pre><code>scale(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    standardize: bool = True,\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Standardize or center variables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>standardize</code> <code>bool</code> <p>If True, standardize (mean=0, sd=1). If False, only center (mean=0). Default is True.</p> <code>True</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.scale(columns=\"age\")  # Standardize\n&gt;&gt;&gt; fb.scale(columns=\"income\", standardize=False)  # Center only\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def scale(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    standardize: bool = True,\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Standardize or center variables.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    standardize : bool, optional\n        If True, standardize (mean=0, sd=1). If False, only center (mean=0).\n        Default is True.\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.scale(columns=\"age\")  # Standardize\n    &gt;&gt;&gt; fb.scale(columns=\"income\", standardize=False)  # Center only\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    if standardize:\n        function_item = \"scale\"\n    else:\n        function_item = \"center\"\n\n    return caller.any_wrapper(\n        df=df,\n        columns=columns,\n        clause=clause,\n        case_insensitive=case_insensitive,\n        prefix=f\"{function_item}(\",\n        suffix=\")\",\n    )\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.simple_interaction","title":"simple_interaction","text":"<pre><code>simple_interaction(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    order: int = 2,\n    case_insensitive: bool = False,\n    sub_function: Callable | None = None,\n    no_base: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Create interactions between variables.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>order</code> <code>int</code> <p>Interaction order. 2 = pairwise, 3 = three-way, etc. Default is 2.</p> <code>2</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <code>sub_function</code> <code>Callable | None</code> <p>Function to apply to columns before interacting. Default is None.</p> <code>None</code> <code>no_base</code> <code>bool</code> <p>Exclude main effects (only interactions). Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fb.simple_interaction(columns=[\"age\", \"education\"], order=2)\n&gt;&gt;&gt; print(fb.formula)\n'y~1+(age+education)**2'\n</code></pre> <pre><code>&gt;&gt;&gt; fb.simple_interaction(columns=[\"a\", \"b\", \"c\"], order=2, no_base=True)\n&gt;&gt;&gt; print(fb.formula)\n'y~1+(a+b+c)**2-(a+b+c)'  # Interactions only\n</code></pre> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def simple_interaction(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    order: int = 2,\n    case_insensitive: bool = False,\n    sub_function: Callable | None = None,\n    no_base: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Create interactions between variables.\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    order : int, optional\n        Interaction order. 2 = pairwise, 3 = three-way, etc. Default is 2.\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n    sub_function : Callable | None, optional\n        Function to apply to columns before interacting. Default is None.\n    no_base : bool, optional\n        Exclude main effects (only interactions). Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fb.simple_interaction(columns=[\"age\", \"education\"], order=2)\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+(age+education)**2'\n\n    &gt;&gt;&gt; fb.simple_interaction(columns=[\"a\", \"b\", \"c\"], order=2, no_base=True)\n    &gt;&gt;&gt; print(fb.formula)\n    'y~1+(a+b+c)**2-(a+b+c)'  # Interactions only\n    \"\"\"\n    if self is not None:\n        if df is None:\n            df = self.df\n\n    if sub_function is None:\n        sub_function = FormulaBuilder.continuous\n\n    columns = columns_from_list(\n        df=df, columns=columns, case_insensitive=case_insensitive\n    )\n\n    subformula = sub_function(\n        df=df, columns=columns, case_insensitive=case_insensitive\n    )\n    #   Remove leading plus sign\n    subformula = subformula[1 : len(subformula)]\n\n    if len(columns) &gt; 1:\n        output = f\"({subformula})**{order}\"\n    else:\n        output = subformula\n\n    if no_base:\n        output += f\"-({subformula})\"\n\n    if self is None:\n        return f\"+{output}\"\n    else:\n        self.add_to_formula(output)\n        return self\n</code></pre>"},{"location":"api/miscellaneous/#survey_kit.utilities.formula_builder.FormulaBuilder.standardize","title":"standardize","text":"<pre><code>standardize(\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder\n</code></pre> <p>Standardize variables (mean=0, sd=1).</p> <p>Convenience method for scale(standardize=True).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT | None</code> <p>Dataframe for column lookup. Default is None.</p> <code>None</code> <code>columns</code> <code>str | list | None</code> <p>Column names or patterns. Default is None.</p> <code>None</code> <code>clause</code> <code>str</code> <p>Pre-constructed clause. Default is \"\".</p> <code>''</code> <code>case_insensitive</code> <code>bool</code> <p>Case-insensitive matching. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | FormulaBuilder</code> <p>Formula string or self for chaining.</p> Source code in <code>src\\survey_kit\\utilities\\formula_builder.py</code> <pre><code>def standardize(\n    self=None,\n    df: IntoFrameT | None = None,\n    columns: str | list | None = None,\n    clause: str = \"\",\n    case_insensitive: bool = False,\n) -&gt; str | FormulaBuilder:\n    \"\"\"\n    Standardize variables (mean=0, sd=1).\n\n    Convenience method for scale(standardize=True).\n\n    Parameters\n    ----------\n    df : IntoFrameT | None, optional\n        Dataframe for column lookup. Default is None.\n    columns : str | list | None, optional\n        Column names or patterns. Default is None.\n    clause : str, optional\n        Pre-constructed clause. Default is \"\".\n    case_insensitive : bool, optional\n        Case-insensitive matching. Default is False.\n\n    Returns\n    -------\n    str | FormulaBuilder\n        Formula string or self for chaining.\n    \"\"\"\n    if self is None:\n        caller = FormulaBuilder\n    else:\n        caller = self\n\n    return caller.scale(\n        df=df,\n        columns=columns,\n        clause=clause,\n        case_insensitive=case_insensitive,\n        standardize=True,\n    )\n</code></pre>"},{"location":"api/multiple_imputation/","title":"Multiple Imputation Standard Errors","text":""},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.mi_ses_from_function","title":"mi_ses_from_function","text":"<pre><code>mi_ses_from_function(\n    delegate: Callable,\n    join_on: list,\n    path_srmi: str = \"\",\n    df_implicates: list[IntoFrameT]\n    | DataFrameList\n    | None = None,\n    index: list | None = None,\n    df_noimputes: IntoFrameT | None = None,\n    arguments: dict | None = None,\n    df_argument_name: str = \"df\",\n    implicate_name: str = \"___implicate___\",\n    parallel: bool = False,\n    parallel_inputs: CallInputs | None = None,\n    rounding: Rounding | None = None,\n    round_output: bool = True,\n) -&gt; MultipleImputation\n</code></pre> <p>Calculate multiple imputation standard errors by applying a function to each implicate.</p> <p>This is the main function for computing MI estimates. It applies a specified function (delegate) to each imputed dataset, then combines the results using Rubin's rules to produce final estimates with proper MI standard errors.</p> <p>Parameters:</p> Name Type Description Default <code>delegate</code> <code>callable</code> <p>Function to apply to each imputed dataset. Should return estimates, standard errors, and optionally replicate estimates. Common choices: - StatCalculator class constructor - Custom analysis function</p> required <code>path_srmi</code> <code>str</code> <p>The path to a saved SRMI serialized object The default is \"\". Must pass either path_srmi or df_implicates</p> <code>''</code> <code>df_implicates</code> <code>list[IntoFrameT] | DataFrameList</code> <p>List of imputed datasets to analyze. The default is None Must pass either path_srmi or df_implicates</p> <code>None</code> <code>join_on</code> <code>list</code> <p>Column names that identify unique estimates across implicates. Used for combining results.</p> required <code>index</code> <code>list | None</code> <p>Index columns for merging df_noimputes with implicates. If None, assumes row-wise concatenation (dangerous...). Default is None.</p> <code>None</code> <code>df_noimputes</code> <code>IntoFrameT | None</code> <p>Non-imputed data (e.g., weights, design variables) to merge with each implicate. Default is None.</p> <code>None</code> <code>arguments</code> <code>dict | None</code> <p>Additional arguments passed to the delegate function. Default is None.</p> <code>None</code> <code>df_argument_name</code> <code>str</code> <p>Name of the dataframe argument in the delegate function. Default is \"df\".</p> <code>'df'</code> <code>parallel</code> <code>bool</code> <p>Run analysis on implicates in parallel. Default is False.</p> <code>False</code> <code>parallel_inputs</code> <code>CallInputs | None</code> <p>Configuration for parallel execution. Default is None.</p> <code>None</code> <code>rounding</code> <code>Rounding | None</code> <p>Rounding configuration for results. Default is None.</p> <code>None</code> <code>round_output</code> <code>bool</code> <p>Apply rounding to final output. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>MultipleImputation</code> <p>MultipleImputation object containing combined estimates, MI standard errors, degrees of freedom, and other MI diagnostics.</p> <p>Examples:</p> <p>Basic usage with StatCalculator:</p> <pre><code>&gt;&gt;&gt; from NEWS.CodeUtilities.Python.Statistics.StatCalculator import StatCalculator\n&gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Statistics, Replicates\n&gt;&gt;&gt; from NEWS.CodeUtilities.Python.SRMI.SRMI import SRMI\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Load SRMI data\n&gt;&gt;&gt; srmi = SRMI.load(path_model=\"/projects/data/NEWS/Test/py_srmi_test.srmi/\",\n...                  LazyLoad=False)\n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; # Define what to calculate\n&gt;&gt;&gt; stats = Statistics(stats=[\"mean\", \"median\"], columns=[\"income\", \"age\"])\n&gt;&gt;&gt; replicates = Replicates(weight_stub=\"replicate_\", n_replicates=80)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Arguments for StatCalculator\n&gt;&gt;&gt; arguments = {\n...     \"statistics\": stats,\n...     \"replicates\": replicates,\n...     \"weight\": \"survey_weight\",\n...     \"display\": False\n... }\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate MI estimates\n&gt;&gt;&gt; mi_results = mi_ses_from_function(\n...     delegate=StatCalculator,\n...     df_implicates=srmi.df_implicates,\n...     df_noimputes=weight_data,\n...     arguments=arguments,\n...     join_on=[\"Variable\"]\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; # View results\n&gt;&gt;&gt; mi_results.print(round_output=True)\n&gt;&gt;&gt; print(\"Degrees of freedom:\", mi_results.df_df)\n&gt;&gt;&gt; print(\"Missing information rate:\", mi_results.df_rate_of_missing_information)\n</code></pre> <p>With a custom analysis function:</p> <pre><code>&gt;&gt;&gt; def custom_analysis(df, weight=\"\", var=\"income\"):\n...     '''Custom function returning estimates'''\n...     import polars as pl\n...     \n...     if weight:\n...         mean_est = (df[var] * df[weight]).sum() / df[weight].sum()\n...     else:\n...         mean_est = df[var].mean()\n...         \n...     return pl.DataFrame({\n...         \"Variable\": [var],\n...         \"estimate\": [mean_est]\n...     })\n&gt;&gt;&gt; \n&gt;&gt;&gt; mi_custom = mi_ses_from_function(\n...     delegate=custom_analysis,\n...     df_implicates=srmi.df_implicates,\n...     arguments={\"weight\": \"survey_weight\", \"var\": \"income\"},\n...     join_on=[\"Variable\"]\n... )\n</code></pre> <p>Parallel processing for faster computation:</p> <pre><code>&gt;&gt;&gt; from NEWS.CodeUtilities.Python.Function.Utilities import CallInputs, CallTypes\n&gt;&gt;&gt; \n&gt;&gt;&gt; parallel_config = CallInputs(CallType=CallTypes.shell)\n&gt;&gt;&gt; mi_results = mi_ses_from_function(\n...     delegate=StatCalculator,\n...     df_implicates=srmi.df_implicates,\n...     arguments=arguments,\n...     join_on=[\"Variable\"],\n...     parallel=True,\n...     parallel_inputs=parallel_config\n... )\n</code></pre> Notes <p>The function implements the standard MI workflow: 1. Apply delegate function to each imputed dataset 2. Collect estimates and SEs from each implicate 3. Combine using Rubin's rules for MI inference 4. Calculate degrees of freedom and missing information rates</p> <p>For delegate functions that return tuples/lists, expects: - Item 0: estimates dataframe - Item 1: standard errors dataframe - Item 2: replicate estimates (optional) - Item 3: bootstrap flag (optional)</p> See Also <p><code>StatCalculator</code> : Main class for statistical calculations <code>MultipleImputation</code> : Class for MI results</p> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def mi_ses_from_function(delegate:Callable,\n                         join_on:list,\n                         path_srmi:str=\"\",\n                         df_implicates:list[IntoFrameT] | DataFrameList|None=None,\n                         index:list|None=None,\n                         df_noimputes:IntoFrameT | None=None,\n                         arguments:dict|None=None,\n                         df_argument_name:str=\"df\",\n                         implicate_name:str=\"___implicate___\",\n                         parallel:bool=False,\n                         parallel_inputs:CallInputs | None=None,\n                         rounding:Rounding | None=None,\n                         round_output:bool=True) -&gt; MultipleImputation:\n    \"\"\"\n    Calculate multiple imputation standard errors by applying a function to each implicate.\n\n    This is the main function for computing MI estimates. It applies a specified function\n    (delegate) to each imputed dataset, then combines the results using Rubin's rules\n    to produce final estimates with proper MI standard errors.\n\n    Parameters\n    ----------\n    delegate : callable\n        Function to apply to each imputed dataset. Should return estimates,\n        standard errors, and optionally replicate estimates. Common choices:\n        - StatCalculator class constructor\n        - Custom analysis function\n    path_srmi : str, optional\n        The path to a saved SRMI serialized object\n        The default is \"\".\n        Must pass either path_srmi or df_implicates\n    df_implicates : list[IntoFrameT] | DataFrameList, optional\n        List of imputed datasets to analyze.\n        The default is None\n        Must pass either path_srmi or df_implicates\n    join_on : list\n        Column names that identify unique estimates across implicates.\n        Used for combining results.\n    index : list|None, optional\n        Index columns for merging df_noimputes with implicates.\n        If None, assumes row-wise concatenation (dangerous...). Default is None.\n    df_noimputes : IntoFrameT | None, optional\n        Non-imputed data (e.g., weights, design variables) to merge with\n        each implicate. Default is None.\n    arguments : dict|None, optional\n        Additional arguments passed to the delegate function. Default is None.\n    df_argument_name : str, optional\n        Name of the dataframe argument in the delegate function. Default is \"df\".\n    parallel : bool, optional\n        Run analysis on implicates in parallel. Default is False.\n    parallel_inputs : CallInputs | None, optional\n        Configuration for parallel execution. Default is None.\n    rounding : Rounding | None, optional\n        Rounding configuration for results. Default is None.\n    round_output : bool, optional\n        Apply rounding to final output. Default is True.\n\n    Returns\n    -------\n    MultipleImputation\n        MultipleImputation object containing combined estimates, MI standard errors,\n        degrees of freedom, and other MI diagnostics.\n\n    Examples\n    --------\n    Basic usage with StatCalculator:\n\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.Statistics.StatCalculator import StatCalculator\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.SummaryStats import Statistics, Replicates\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.SRMI.SRMI import SRMI\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Load SRMI data\n    &gt;&gt;&gt; srmi = SRMI.load(path_model=\"/projects/data/NEWS/Test/py_srmi_test.srmi/\",\n    ...                  LazyLoad=False)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Define what to calculate\n    &gt;&gt;&gt; stats = Statistics(stats=[\"mean\", \"median\"], columns=[\"income\", \"age\"])\n    &gt;&gt;&gt; replicates = Replicates(weight_stub=\"replicate_\", n_replicates=80)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Arguments for StatCalculator\n    &gt;&gt;&gt; arguments = {\n    ...     \"statistics\": stats,\n    ...     \"replicates\": replicates,\n    ...     \"weight\": \"survey_weight\",\n    ...     \"display\": False\n    ... }\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Calculate MI estimates\n    &gt;&gt;&gt; mi_results = mi_ses_from_function(\n    ...     delegate=StatCalculator,\n    ...     df_implicates=srmi.df_implicates,\n    ...     df_noimputes=weight_data,\n    ...     arguments=arguments,\n    ...     join_on=[\"Variable\"]\n    ... )\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # View results\n    &gt;&gt;&gt; mi_results.print(round_output=True)\n    &gt;&gt;&gt; print(\"Degrees of freedom:\", mi_results.df_df)\n    &gt;&gt;&gt; print(\"Missing information rate:\", mi_results.df_rate_of_missing_information)\n\n    With a custom analysis function:\n\n    &gt;&gt;&gt; def custom_analysis(df, weight=\"\", var=\"income\"):\n    ...     '''Custom function returning estimates'''\n    ...     import polars as pl\n    ...     \n    ...     if weight:\n    ...         mean_est = (df[var] * df[weight]).sum() / df[weight].sum()\n    ...     else:\n    ...         mean_est = df[var].mean()\n    ...         \n    ...     return pl.DataFrame({\n    ...         \"Variable\": [var],\n    ...         \"estimate\": [mean_est]\n    ...     })\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; mi_custom = mi_ses_from_function(\n    ...     delegate=custom_analysis,\n    ...     df_implicates=srmi.df_implicates,\n    ...     arguments={\"weight\": \"survey_weight\", \"var\": \"income\"},\n    ...     join_on=[\"Variable\"]\n    ... )\n\n    Parallel processing for faster computation:\n\n    &gt;&gt;&gt; from NEWS.CodeUtilities.Python.Function.Utilities import CallInputs, CallTypes\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; parallel_config = CallInputs(CallType=CallTypes.shell)\n    &gt;&gt;&gt; mi_results = mi_ses_from_function(\n    ...     delegate=StatCalculator,\n    ...     df_implicates=srmi.df_implicates,\n    ...     arguments=arguments,\n    ...     join_on=[\"Variable\"],\n    ...     parallel=True,\n    ...     parallel_inputs=parallel_config\n    ... )\n\n    Notes\n    -----\n    The function implements the standard MI workflow:\n    1. Apply delegate function to each imputed dataset\n    2. Collect estimates and SEs from each implicate  \n    3. Combine using Rubin's rules for MI inference\n    4. Calculate degrees of freedom and missing information rates\n\n    For delegate functions that return tuples/lists, expects:\n    - Item 0: estimates dataframe\n    - Item 1: standard errors dataframe  \n    - Item 2: replicate estimates (optional)\n    - Item 3: bootstrap flag (optional)\n\n    See Also\n    --------\n    [`StatCalculator`][survey_kit.statistics.calculator.StatCalculator] : Main class for statistical calculations\n    [`MultipleImputation`][survey_kit.statistics.multiple_imputation.MultipleImputation] : Class for MI results\n    \"\"\"\n\n    #   Don't edit the arguments dictionary\n    arguments = copy(arguments)\n    if parallel:\n        if path_srmi != \"\":\n            n_implicates = SRMI.load(path_srmi).n_implicates\n        else:\n            n_implicates = len(df_implicates)\n        if parallel_inputs is None:\n            logger.info(\"Defaulting to shell for parallel estimation\")\n            parallel_inputs = CallInputs(\n                call_type=CallTypes.shell,\n                n_cpu=int(config.cpus/n_implicates)\n            )\n\n        del n_implicates\n        arguments = locals().copy()\n\n        implicate_stats = _mi_ses_from_function_parallel(**arguments)\n\n    else:\n        arguments = locals().copy()\n\n        del arguments[\"parallel\"]\n        del arguments[\"parallel_inputs\"]\n        implicate_stats = _mi_ses_from_function_sequential(**arguments)\n\n\n\n\n\n\n    if len(implicate_stats):\n        mi_stats = MultipleImputation(implicate_stats=implicate_stats,\n                                           join_on=join_on,\n                                           rounding=rounding)\n        mi_stats.calculate()\n\n\n        return mi_stats\n\n\n    else:\n        return None\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation","title":"MultipleImputation","text":"<p>               Bases: <code>Serializable</code></p> <p>A class for managing multiple imputation standard errors.  You should generally estimate the statistics with <code>mi_ses_from_function</code> and then use this class to do comparisons, print, or gather results from the dataframes in it.</p> <p>MultipleImputation combines estimates from multiple imputed datasets to produce final estimates with proper standard errors that account for both within-imputation and between-imputation variance. It implements Rubin's rules for multiple imputation inference, including degrees of freedom calculations and missing information rates.</p> <p>The class supports: - Combining estimates from multiple imputed datasets - Calculating proper MI standard errors using Rubin's rules - Computing degrees of freedom for t-tests with MI data - Calculating rates of missing information - Statistical comparisons between MI estimate sets - Confidence intervals and p-values adjusted for MI uncertainty - Integration with replicate weight variance estimation</p> <p>Parameters:</p> Name Type Description Default <code>implicate_stats</code> <code>list[ReplicateStats] | None</code> <p>List of ReplicateStats objects, one for each imputed dataset. Each contains estimates and potentially replicate weight SEs. Default is None.</p> <code>None</code> <code>join_on</code> <code>list | None</code> <p>Column names that identify unique estimates across implicates. Used for combining estimates. Default is None (empty list).</p> <code>None</code> <code>df_estimates</code> <code>IntoFrameT | None</code> <p>Combined estimates dataframe (calculated automatically). Default is None.</p> <code>None</code> <code>df_ses</code> <code>IntoFrameT | None</code> <p>MI standard errors dataframe (calculated automatically). Default is None.</p> <code>None</code> <code>df_df</code> <code>IntoFrameT | None</code> <p>Degrees of freedom for each estimate (calculated automatically). Default is None.</p> <code>None</code> <code>df_t</code> <code>IntoFrameT | None</code> <p>T-statistics dataframe (calculated automatically). Default is None.</p> <code>None</code> <code>df_p</code> <code>IntoFrameT | None</code> <p>P-values dataframe (calculated automatically). Default is None.</p> <code>None</code> <code>df_rate_of_missing_information</code> <code>IntoFrameT | None</code> <p>Rate of missing information for each estimate. Default is None.</p> <code>None</code> <code>rounding</code> <code>Rounding | None</code> <p>Rounding configuration for display and output. Default is None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_implicates</code> <code>int</code> <p>Number of imputed datasets used in the analysis.</p> <code>summarize_vars</code> <code>list</code> <p>List of all estimate column names (excluding join variables).</p> <p>Examples:</p> <p>Creating MI estimates from a function applied to multiple implicates:</p> <pre><code>&gt;&gt;&gt; from survey_kit.statistics.multiple_imputation import mi_ses_from_function\n&gt;&gt;&gt; from survey_kit.statistics.calculator import StatCalculator\n&gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n&gt;&gt;&gt; from survey_kit.imputation.srmi import SRMI\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Define statistics to calculate\n&gt;&gt;&gt; stats = Statistics(stats=[\"mean\"], columns=[\"var_*\"])\n&gt;&gt;&gt; replicates = Replicates(weight_stub=\"weight_\", n_replicates=20)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Arguments for the function to run on each implicate\n&gt;&gt;&gt; arguments = {\n...     \"statistics\": stats,\n...     \"replicates\": replicates,\n...     \"round_output\": False,\n...     \"display\": False\n... }\n&gt;&gt;&gt; # Load SRMI data\n&gt;&gt;&gt; srmi = SRMI.load(path_model=\"/projects/data/NEWS/Test/py_srmi_test.srmi/\",\n...                  LazyLoad=False)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Calculate MI estimates\n&gt;&gt;&gt; mi_results = mi_ses_from_function(\n...     delegate=StatCalculator,\n...     df_implicates=srmi.df_implicates,\n...     df_noimputes=weights_data,\n...     arguments=arguments,\n...     join_on=[\"Variable\"]\n... )\n&gt;&gt;&gt; mi_results.print(round_output=True)\n</code></pre> <p>Comparing two sets of MI estimates:</p> <pre><code>&gt;&gt;&gt; mi_means = mi_ses_from_function(...)  # Calculate means\n&gt;&gt;&gt; mi_medians = mi_ses_from_function(...)  # Calculate medians\n&gt;&gt;&gt; \n&gt;&gt;&gt; comparison = mi_means.compare(\n...     mi_medians,\n...     compare_list_columns=[(\"mean\", \"median\")]\n... )\n&gt;&gt;&gt; comparison[\"difference\"].print()\n&gt;&gt;&gt; comparison[\"ratio\"].print()\n</code></pre> <p>Examining MI-specific diagnostics:</p> <pre><code>&gt;&gt;&gt; print(\"Degrees of freedom:\")\n&gt;&gt;&gt; print(mi_results.df_df)\n&gt;&gt;&gt; print(\"Rate of missing information:\")\n&gt;&gt;&gt; print(mi_results.df_rate_of_missing_information)\n</code></pre> Notes <p>The class implements Rubin's (1987) rules for multiple imputation: - Combined estimate is the average across implicates - Total variance = within-imputation variance + between-imputation variance - Degrees of freedom account for finite number of implicates - Missing information rate quantifies uncertainty due to imputation</p> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>class MultipleImputation(Serializable):\n    \"\"\"\n    A class for managing multiple imputation standard errors.  You should generally estimate\n    the statistics with [`mi_ses_from_function`][survey_kit.statistics.multiple_imputation.mi_ses_from_function] and then use this class to\n    do comparisons, print, or gather results from the dataframes in it.\n\n    MultipleImputation combines estimates from multiple imputed datasets to produce\n    final estimates with proper standard errors that account for both within-imputation\n    and between-imputation variance. It implements Rubin's rules for multiple imputation\n    inference, including degrees of freedom calculations and missing information rates.\n\n    The class supports:\n    - Combining estimates from multiple imputed datasets\n    - Calculating proper MI standard errors using Rubin's rules\n    - Computing degrees of freedom for t-tests with MI data\n    - Calculating rates of missing information\n    - Statistical comparisons between MI estimate sets\n    - Confidence intervals and p-values adjusted for MI uncertainty\n    - Integration with replicate weight variance estimation\n\n    Parameters\n    ----------\n    implicate_stats : list[ReplicateStats] | None, optional\n        List of ReplicateStats objects, one for each imputed dataset.\n        Each contains estimates and potentially replicate weight SEs. Default is None.\n    join_on : list | None, optional\n        Column names that identify unique estimates across implicates.\n        Used for combining estimates. Default is None (empty list).\n    df_estimates : IntoFrameT | None, optional\n        Combined estimates dataframe (calculated automatically). Default is None.\n    df_ses : IntoFrameT | None, optional\n        MI standard errors dataframe (calculated automatically). Default is None.\n    df_df : IntoFrameT | None, optional\n        Degrees of freedom for each estimate (calculated automatically). Default is None.\n    df_t : IntoFrameT | None, optional\n        T-statistics dataframe (calculated automatically). Default is None.\n    df_p : IntoFrameT | None, optional\n        P-values dataframe (calculated automatically). Default is None.\n    df_rate_of_missing_information : IntoFrameT | None, optional\n        Rate of missing information for each estimate. Default is None.\n    rounding : Rounding | None, optional\n        Rounding configuration for display and output. Default is None.\n\n    Attributes\n    ----------\n    n_implicates : int\n        Number of imputed datasets used in the analysis.\n    summarize_vars : list\n        List of all estimate column names (excluding join variables).\n\n    Examples\n    --------\n    Creating MI estimates from a function applied to multiple implicates:\n\n    &gt;&gt;&gt; from survey_kit.statistics.multiple_imputation import mi_ses_from_function\n    &gt;&gt;&gt; from survey_kit.statistics.calculator import StatCalculator\n    &gt;&gt;&gt; from survey_kit.statistics.statistics import Statistics\n    &gt;&gt;&gt; from survey_kit.imputation.srmi import SRMI\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Define statistics to calculate\n    &gt;&gt;&gt; stats = Statistics(stats=[\"mean\"], columns=[\"var_*\"])\n    &gt;&gt;&gt; replicates = Replicates(weight_stub=\"weight_\", n_replicates=20)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Arguments for the function to run on each implicate\n    &gt;&gt;&gt; arguments = {\n    ...     \"statistics\": stats,\n    ...     \"replicates\": replicates,\n    ...     \"round_output\": False,\n    ...     \"display\": False\n    ... }\n    &gt;&gt;&gt; # Load SRMI data\n    &gt;&gt;&gt; srmi = SRMI.load(path_model=\"/projects/data/NEWS/Test/py_srmi_test.srmi/\",\n    ...                  LazyLoad=False)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # Calculate MI estimates\n    &gt;&gt;&gt; mi_results = mi_ses_from_function(\n    ...     delegate=StatCalculator,\n    ...     df_implicates=srmi.df_implicates,\n    ...     df_noimputes=weights_data,\n    ...     arguments=arguments,\n    ...     join_on=[\"Variable\"]\n    ... )\n    &gt;&gt;&gt; mi_results.print(round_output=True)\n\n    Comparing two sets of MI estimates:\n\n    &gt;&gt;&gt; mi_means = mi_ses_from_function(...)  # Calculate means\n    &gt;&gt;&gt; mi_medians = mi_ses_from_function(...)  # Calculate medians\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; comparison = mi_means.compare(\n    ...     mi_medians,\n    ...     compare_list_columns=[(\"mean\", \"median\")]\n    ... )\n    &gt;&gt;&gt; comparison[\"difference\"].print()\n    &gt;&gt;&gt; comparison[\"ratio\"].print()\n\n    Examining MI-specific diagnostics:\n\n    &gt;&gt;&gt; print(\"Degrees of freedom:\")\n    &gt;&gt;&gt; print(mi_results.df_df)\n    &gt;&gt;&gt; print(\"Rate of missing information:\")\n    &gt;&gt;&gt; print(mi_results.df_rate_of_missing_information)\n\n    Notes\n    -----\n    The class implements Rubin's (1987) rules for multiple imputation:\n    - Combined estimate is the average across implicates\n    - Total variance = within-imputation variance + between-imputation variance\n    - Degrees of freedom account for finite number of implicates\n    - Missing information rate quantifies uncertainty due to imputation\n\n    \"\"\"\n\n    _save_suffix = \"mi\"\n    def __init__(self,\n                 implicate_stats:list[ReplicateStats] | None=None,\n                 join_on:list | None=None,\n                 df_estimates:IntoFrameT | None=None,\n                 df_ses:IntoFrameT | None=None,\n                 df_df:IntoFrameT | None=None,\n                 df_t:IntoFrameT | None=None,\n                 df_p:IntoFrameT | None=None,\n                 df_rate_of_missing_information:IntoFrameT | None=None,\n                 rounding:Rounding | None=None):\n        self.implicate_stats = implicate_stats\n\n        if join_on is None:\n            join_on = []\n        self.join_on = join_on\n        self.df_estimates = df_estimates\n        self.df_ses = df_ses\n        self.df_df = df_df\n        self.df_t = df_t\n        self.df_p = df_p\n        self.df_rate_of_missing_information = df_rate_of_missing_information\n        self.rounding = rounding\n\n\n    def copy(self) -&gt; MultipleImputation:\n        return MultipleImputation(implicate_stats=[impi.copy() for impi in self.implicate_stats],\n                                  join_on=copy(self.join_on),\n                                  df_estimates=self.df_estimates,\n                                  df_ses=self.df_ses,\n                                  df_df=self.df_df,\n                                  df_t=self.df_t,\n                                  df_p=self.df_p,\n                                  df_rate_of_missing_information=self.df_rate_of_missing_information,\n                                  rounding=copy(self.rounding))\n\n\n    def calculate(self,\n                  implicate_name=\"___implicate___\"):\n        \"\"\"\n        Calculate multiple imputation estimates using Rubin's rules.\n\n        Combines estimates from individual implicates to produce final MI estimates,\n        standard errors, degrees of freedom, t-statistics, p-values, and rates of\n        missing information. Implements the standard MI combining formulas.\n\n        Parameters\n        ----------\n        implicate_name : str, optional\n            Column name identifier for imputation number. Default is \"___implicate___\".\n\n        Notes\n        -----\n        Implements Rubin's combining rules:\n        - Q\u0304 = (1/m) \u03a3 Q\u0302\u1d62  (combined estimate)\n        - U = (1/m) \u03a3 U\u1d62    (within-imputation variance)\n        - B = (1/(m-1)) \u03a3 (Q\u0302\u1d62 - Q\u0304)\u00b2  (between-imputation variance)\n        - T = U + (1 + 1/m)B  (total variance)\n        - \u03bd = (m-1)[1 + mU/((m+1)B)]\u00b2  (degrees of freedom)\n\n        where m is the number of implicates.\n        \"\"\"\n\n        df_estimates_stacked = []\n        df_ses_stacked = []\n\n\n        col_sort = \"___mi_sort___\"        \n        for indexi, imp_statsi in enumerate(self.implicate_stats):\n            if indexi == 0:\n                df_sort = (\n                    nw.from_native(imp_statsi.df_estimates)\n                    .select(self.join_on)\n                    .lazy()\n                    .collect()\n                    .with_row_index(col_sort)\n                    .lazy()\n                    .to_native()\n                )\n            df_estimates_stacked.append(imp_statsi.df_estimates)\n            df_ses_stacked.append(imp_statsi.df_ses)\n\n\n        df_estimates_stacked = (\n            nw.from_native(\n                fill_missing(\n                    concat_wrapper(\n                        df_estimates_stacked,\n                        how=\"diagonal\"\n                    ),\n                    value=float(\"nan\")\n                )\n            )\n            .lazy()\n            .collect()\n            .to_native()\n        )\n        df_ses_stacked = (\n            nw.from_native(\n                concat_wrapper(\n                    df_ses_stacked,\n                    how=\"diagonal\"\n                )\n            )\n            .with_columns(cs.boolean().cast(nw.Int8))\n            .lazy()\n            .collect()\n            .to_native()\n        )\n\n        cols_non_stats = self.join_on + [implicate_name,col_sort]\n        cols_stats = safe_columns(df_estimates_stacked)\n        cols_stats = _columns_original_order(\n            columns_unordered=list(set(cols_stats).difference(cols_non_stats)),\n            columns_ordered=cols_stats\n        )\n\n        #   Notation from Joe Schaffer MI FAQ page (downloaded from the Wayback Machine)\n        #       NEWS/Documentation/Background/MultipleImputation/MI_FAQ.htm\n        df_estimates = (\n            nw.from_native(df_estimates_stacked)\n            .group_by(self.join_on).agg(nw.all().mean())\n            .sort(self.join_on)\n        )\n\n        c_stats = nw.col(cols_stats)\n        with_between = []\n        for coli in cols_stats:\n            c_col = nw.col(coli)\n            c_bar = nw.col(f\"{coli}_bar\")\n            with_between.append(\n                (1/(self.n_implicates-1)*(c_col - c_bar)**2).alias(coli)\n                )\n\n        df_B = (\n            nw.from_native(\n                join_list(\n                    [\n                        df_estimates_stacked,\n                        (\n                            nw.from_native(df_estimates)\n                            .rename({coli:f\"{coli}_bar\" for coli in cols_stats})\n                            .to_native()\n                        )\n                    ],\n                    on=self.join_on,\n                    how=\"left\"\n                )\n            )\n            .with_columns(with_between)\n            .group_by(self.join_on).agg(c_stats.sum())\n            .to_native()\n        )\n\n        df_U = (\n            nw.from_native(df_ses_stacked)\n            .with_columns(c_stats**2)\n            .group_by(self.join_on).agg(nw.all().mean())\n            .to_native()\n        )\n\n        df_variance = (\n            nw.from_native(\n                concat_wrapper(\n                    [\n                        nw.from_native(df_B)\n                        .with_columns(c_stats*(1+1/self.n_implicates))\n                        .to_native(),\n                        df_U\n                    ],\n                    how=\"diagonal\"\n                )\n            )\n            .group_by(self.join_on).agg(nw.all().sum())\n            .to_native()\n        )\n\n        self.df_ses = (\n            nw.from_native(df_variance)\n            .with_columns(c_stats**.5)\n            .to_native()\n        )\n\n        self.df_estimates = (\n            nw.from_native(df_estimates_stacked)\n            .group_by(self.join_on).agg(nw.all().mean())\n            .sort(self.join_on)\n            .to_native()\n        )\n\n        df_B_U = join_list(\n            [\n                (\n                    nw.from_native(df_B)\n                    .rename({coli:f\"{coli}_B\" for coli in cols_stats})\n                    .to_native()\n                ),\n                (\n                    nw.from_native(df_U)\n                    .rename({coli:f\"{coli}_U\" for coli in cols_stats})\n                    .to_native()\n                )\n            ],\n            on=self.join_on,\n            how=\"left\"\n        )\n\n        m = self.n_implicates\n        with_df = []\n        for coli in cols_stats:\n            c_U = nw.col(f\"{coli}_U\")\n            c_B = nw.col(f\"{coli}_B\")\n\n            with_df.append(\n                    ((m - 1)*(1+(m*c_U)/((m+1)*c_B))**2).alias(coli)\n                )\n\n        self.df_df = (\n            nw.from_native(df_B_U)\n            .select(self.join_on + with_df)\n            .to_native()\n        )\n\n\n\n\n        with_r = []\n        with_gamma = []\n        for coli in cols_stats:\n            c_U = nw.col(f\"{coli}_U\")\n            c_B = nw.col(f\"{coli}_B\")\n            c_r = nw.col(f\"{coli}_r\")\n            c_df = nw.col(f\"{coli}_df\")\n            with_r.append(\n                    ((1 + 1/m)*c_B/c_U).alias(f\"{coli}_r\")\n                )\n\n\n            with_gamma.append(\n                    ((c_r + (2/(c_df+3)))/(c_r+1)).alias(coli)\n                )\n\n        self.df_rate_of_missing_information = (\n            nw.from_native(\n                join_list(\n                    [\n                        df_B_U,\n                        (\n                            nw.from_native(self.df_df)\n                            .rename({coli:f\"{coli}_df\" for coli in cols_stats})\n                            .to_native()\n                        )\n                    ],\n                    how=\"left\",\n                    on=self.join_on\n                )\n            )\n            .with_columns(with_r)\n            .select(self.join_on + with_gamma)\n            .to_native()\n        )\n\n        with_t = []\n        for coli in cols_stats:\n            c_est = nw.col(coli)\n            c_se = nw.col(f\"{coli}_se\")\n\n            with_t.append((c_est/c_se).abs().alias(coli))\n\n        self.df_t = (\n            nw.from_native(\n                join_list(\n                    [\n                        self.df_estimates,\n                        (\n                            nw.from_native(self.df_ses)\n                            .rename({coli:f\"{coli}_se\" for coli in cols_stats})\n                            .to_native()\n                        )\n                    ],\n                    on=self.join_on,\n                    how=\"left\"\n                )\n            )\n            .select(self.join_on + with_t)\n            .to_native()\n        )\n\n\n        df_p = join_list(\n            [\n                (\n                    nw.from_native(self.df_df)\n                    .rename({coli:f\"{coli}_df\" for coli in cols_stats})\n                    .to_native()\n                ),\n                (\n                    nw.from_native(self.df_t)\n                    .rename({coli:f\"{coli}_t\" for coli in cols_stats})\n                    .to_native()\n                )\n            ],\n            how=\"left\",\n            on=self.join_on\n        )\n\n        def p_value(t,df):\n            if t == float('inf') or t == float('nan'):\n                return 0\n            if df == float('inf') or df == float('nan'):\n                df = 1_000_000\n\n            return scipy.stats.t.sf(t,df)*2\n\n\n        def p_value_lambda(t,\n                           col_t,\n                           col_df):\n            try:\n                return p_value(t[col_t],t[col_df])\n            except:\n                return None\n\n        nw_type = NarwhalsType(df_p)\n        df_p = nw_type.to_polars().lazy().collect()\n        for coli in cols_stats:\n            index_t = df_p.columns.index(f\"{coli}_t\")\n            index_df = df_p.columns.index(f\"{coli}_df\")\n\n            df_p = (df_p.with_columns(df_p.map_rows(lambda t: p_value_lambda(t,index_t,index_df),\n                                                    return_dtype=pl.Float64))\n                        .rename({\"map\":coli})\n                        )\n\n\n        self.df_p = nw_type.from_polars(\n            df_p.select(self.join_on + cols_stats)\n        )\n\n        def _sort_and_fill_null(df:IntoFrameT) -&gt; IntoFrameT:\n            nw_type = NarwhalsType(join_list([df,df_sort],how=\"left\",on=self.join_on))\n            return nw_type.from_polars(\n                nw_type.to_polars()\n                .sort([col_sort],\n                        maintain_order=True)\n                .drop(col_sort)\n                .fill_nan(None)\n            )\n\n\n\n        self.df_estimates = _sort_and_fill_null(self.df_estimates)\n        self.df_ses = _sort_and_fill_null(self.df_ses)\n        self.df_df = _sort_and_fill_null(self.df_df)\n        self.df_t = _sort_and_fill_null(self.df_t)\n        self.df_p = _sort_and_fill_null(df_p)\n        self.df_rate_of_missing_information = _sort_and_fill_null(self.df_rate_of_missing_information)\n\n    def compare(self,\n                other:ReplicateStats | MultipleImputation | StatCalculator,\n                difference:bool=True,\n                ratio:bool=True,\n                ratio_minus_1:bool=True,\n                replicate_name:str=\"___replicate___\",\n                compare_list_variables:list[ComparisonItem.Variable] | None=None,\n                compare_list_columns:list[ComparisonItem.Column] | None=None) -&gt; dict[str,MultipleImputation]:\n        \"\"\"\n        Compare this set of MI estimates to another set of estimates.\n\n        Parameters\n        ----------\n        other : MultipleImputation | ReplicateStats | StatCalculator\n            The other object to compare against.\n        difference : bool, optional\n            Calculate and return differences. Default is True.\n        ratio : bool, optional\n            Calculate and return ratios. Default is True.\n        ratio_minus_1 : bool, optional\n            Subtract 1 from ratios (for percentage change interpretation).\n            Default is True.\n        replicate_name : str, optional\n            Column name for replicate identifier. Default is \"___replicate___\".\n        compare_list_variables : list[ComparisonItem.Variable] | None, optional\n            List of variable info to compare.\n        compare_list_columns : list[ComparisonItem.Columns] | None, optional\n            List of column info to compare.\n\n        Returns\n        -------\n        dict[str, MultipleImputation]\n            Dictionary with keys [\"difference\", \"ratio\"] containing\n            MultipleImputation objects with comparison results and proper\n            MI standard errors for the comparisons.\n\n        Examples\n        --------\n        Compare mi_news to sc_survey (StatCalculator)\n        &gt;&gt;&gt; comparison = mi_news.compare(\n        ...     sc_survey\n        ... )\n\n        Compare means vs medians:\n        &gt;&gt;&gt; comparison = mi_means.compare(\n        ...     mi_medians,\n        ...     compare_list_columns=compare_list_variables=[\n        ...     ComparisonItem.Variable(\n        ...         \"mean\",\n        ...         \"median\",\n        ...         name=\"median_mean\"\n        ...     )]\n        ... )\n        &gt;&gt;&gt; comparison[\"difference\"].print()\n\n        Compare specific variables:\n\n        &gt;&gt;&gt; comparison = mi_results1.compare(\n        ...     mi_results2,\n        ...     compare_list_variables=[ComparisonItem.Variable(\n        ...         value1=\"income\",\n        ...         value2=\"income_2\",\n        ...         name=\"income_comp\"\n        ...     )]\n        ... )\n        \"\"\"\n\n\n\n        return kit_comparisons.compare(stats1=self,\n                                stats2=other,\n                                join_on=self.join_on,\n                                rounding=self.rounding,\n                                difference=difference,\n                                ratio=ratio,\n                                ratio_minus_1=ratio_minus_1,\n                                replicate_name=replicate_name,\n                                compare_list_variables=compare_list_variables,\n                                compare_list_columns=compare_list_columns)\n\n\n\n\n\n\n\n    def round_results(self,\n                      df:IntoFrameT=None,\n                      rounding:Rounding|None=None,\n                      display_only:bool=False) -&gt; IntoFrameT:\n\n        \"\"\"\n        Apply rounding rules to MI estimates.\n\n        Parameters\n        ----------\n        df : IntoFrameT, optional\n            Table of estimates to round. Default is df_estimates.\n        rounding : Rounding|None, optional\n            Rounding configuration (True for DRB rules, int for significant digits).\n            Default is self.rounding.\n        display_only : bool, optional\n            If True, converts numbers to strings for display only. Default is False.\n\n        Returns\n        -------\n        IntoFrameT\n            The dataframe with rounding applied.\n        \"\"\"\n\n\n        if df is None:\n            df = self.df_estimates\n\n        if rounding is None:\n            rounding = self.rounding\n\n        if df is not None:\n            df = drb_round_table(df=df,\n                                 columns=rounding.cols_round,\n                                 columns_n=rounding.cols_n,\n                                 columns_exclude=rounding.cols_exclude,\n                                 round_all=rounding.round_all,\n                                 digits=rounding.round_digits,\n                                 display_only=display_only)\n\n        return df\n\n    def print(self,\n              round_output:bool|int|None=None,\n              sub_log:logging=None):\n        \"\"\"\n        Print the MI estimates and standard errors to the log.\n\n        Parameters\n        ----------\n        round_output : bool|int|None, optional\n            Rounding rule (True for DRB, int for significant digits).\n            Default is self.rounding configuration.\n        sub_log : logging, optional\n            Alternative logger to use. Default is None (use standard logging).\n\n        Examples\n        --------\n        &gt;&gt;&gt; mi_results.print(round_output=True)  # Use DRB rounding\n        &gt;&gt;&gt; mi_results.print(round_output=3)     # 3 significant digits\n        \"\"\"\n\n        if sub_log is None:\n            sub_log = logger\n\n        #   Round?\n        if round_output:\n            if self.rounding is None:\n                rounding = Rounding()\n            else:\n                rounding = deepcopy(self.rounding)\n            rounding.set_round_digits(round_output)\n\n            df_estimates = self.round_results(df=self.df_estimates,\n                                              rounding=rounding,\n                                              display_only=True)\n            df_ses = self.round_results(df=self.df_ses,\n                                        rounding=rounding,\n                                        display_only=True)\n        else:\n            df_estimates = self.df_estimates\n            df_ses = self.df_ses\n\n\n        print_se_table(df_estimates=df_estimates,\n                       df_ses=df_ses,\n                       # display_all_vars=self.display_all_vars,\n                       # display_max_vars=self.display_max_vars,\n                       sort_vars=self.join_on,\n                       round_output=False,\n                       sub_log=sub_log)\n\n\n\n    def table_of_estimates(self,\n                           round_output:bool|int|None=None,\n                           estimates_to_show:list[str] | None=None,\n                           variable_prefix:str=\"\",\n                           estimate_type_variable_name:str=\"Statistic\",\n                           ci_level:float=0.95) -&gt; IntoFrameT:\n        \"\"\"\n        Create a formatted table combining different types of estimates.\n\n        Reshapes estimates into a long format table with different statistic\n        types (estimates, SEs, t-stats, p-values, etc.) as separate rows.\n\n        Parameters\n        ----------\n        round_output : bool|int|None, optional\n            Rounding rule for display.\n        estimates_to_show : list[str] | None, optional\n            List of estimate types to include. Options:\n            - \"estimate\": Point estimates\n            - \"se\": Standard errors  \n            - \"t\": T-statistics\n            - \"p\": P-values\n            - \"ci\": Confidence intervals\n            - \"df\": Degrees of freedom\n            Default is [\"estimate\", \"se\"].\n        variable_prefix : str, optional\n            Prefix to add to variable column names. Default is \"\".\n        estimate_type_variable_name : str, optional\n            Name for column indicating statistic type. Default is \"Statistic\".\n        ci_level : float, optional\n            Confidence level for confidence intervals. Default is 0.95.\n\n        Returns\n        -------\n        IntoFrameT\n            Formatted table with estimates arranged by statistic type.\n\n        Examples\n        --------\n        &gt;&gt;&gt; table = mi_results.table_of_estimates(\n        ...     estimates_to_show=[\"estimate\", \"se\", \"p\", \"ci\"],\n        ...     round_output=True\n        ... )\n        \"\"\"\n        if estimates_to_show is None:\n            estimates_to_show = [\"estimate\",\n                                 \"se\"]\n\n\n        df_ordered = []\n        col_sort = \"__order_output_table__\"\n        for index, esti in enumerate(estimates_to_show):\n            if esti.lower() == \"estimate\":\n                df_ordered.append(\n                    nw.from_native(self.df_estimates)\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .to_native()\n                )\n            elif esti.lower() == \"se\":\n                df_ordered.append(\n                    nw.from_native(self.df_ses)\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .to_native()\n                )\n            elif esti.lower() == \"t\":\n                df_ordered.append(\n                    nw.from_native(self.df_t)\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .to_native()\n                )\n            elif esti.lower() == \"p\":\n                cols_to_keep = safe_columns(\n                    nw.from_native(self.df_estimates)\n                    .lazy()\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .to_native()\n                )\n\n\n                df_p = (\n                    nw.from_native(self.df_p)\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .select(cols_to_keep)\n                    .to_native()\n                )\n\n                when_then_censor_absurd_values = []\n                absurd_value_threshold = 1E-10\n                #   I don't want to see p-values below a ridiculously low number\n                for coli in self.df_estimates.columns:\n                    if coli not in self.join_on:\n                        ci = nw.col(coli)\n                        when_then_censor_absurd_values.append(\n                                ((nw.when(ci.lt(absurd_value_threshold))\n                                    .then(nw.lit(0.0))\n                                    .otherwise(ci)).alias(coli))\n                            )\n\n                df_ordered.append(\n                    nw.from_native(df_p)\n                    .with_columns(when_then_censor_absurd_values)\n                    .to_native()\n                )\n\n            elif esti.lower() == \"ci\":\n                df_ordered.append(\n                    nw.from_native(self._df_ci(ci_level=ci_level))\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                    .to_native()\n                )\n            elif esti.lower() == \"df\":\n                with_infinites = []\n                for coli in self.summarize_vars:\n                    c_coli = nw.col(coli)\n\n                    upper_limit = 10**7\n                    with_infinites.append(\n                                    (nw.when(c_coli.gt(upper_limit)).then(nw.lit(upper_limit))\n                                       .otherwise(c_coli).alias(coli)\n                                   )\n                            )\n\n\n                df_ordered.append(\n                    nw.from_native(self.df_df)\n                    .with_columns(with_infinites)\n                    .with_columns(\n                        [\n                            nw.lit(index).alias(col_sort),\n                            nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                        ]\n                    )\n                )\n            else:\n                message = f\"{esti} not allowed for estimates_to_show\"\n                logger.error(message)\n                raise Exception(message)\n\n        col_row_index = \"___estimate_row_count___\"\n        df_ordered[0] = (\n            nw.from_native(df_ordered[0])\n            .with_row_index(col_row_index)\n            .to_native()\n        )\n\n        df_display = concat_wrapper(\n            df_ordered,\n            how=\"diagonal\"\n        )\n\n        nw_type = NarwhalsType(df_display)\n        df_display = nw_type.to_polars()\n\n        sort_vars = self.join_on\n        df_display = (df_display.sort(sort_vars + [col_sort])\n                                .with_columns(pl.col(col_row_index).forward_fill()))\n        df_display = (df_display.sort([col_row_index] + [col_sort])\n                                .drop(col_row_index))\n\n\n\n\n        #   Clear extraneous information\n        with_clear = []\n        for coli in sort_vars:\n            c_col = pl.col(coli)\n            with_clear.append(pl.when(pl.col(col_sort) != 0)\n                                .then(pl.lit(\"\"))\n                                .otherwise(c_col.cast(pl.String))\n                                .alias(coli))\n\n\n        select_order = sort_vars + [estimate_type_variable_name]\n        remaining = []\n        rename = {}\n        for coli in df_display.columns:\n            if coli not in select_order and coli != col_sort:\n\n\n                if variable_prefix != \"\":\n                    rename[coli] = f\"{variable_prefix}{coli}\"\n\n                remaining.append(coli)\n\n\n        df_display = (df_display.with_columns(with_clear)\n                          .select(select_order + remaining))\n        #   Round?\n        if round_output:\n            if self.rounding is not None:\n                rounding = deepcopy(self.rounding)\n            else:\n                rounding = Rounding()\n            rounding.set_round_digits(round_output)\n\n            if len(rounding.cols_n) == 0 and len(rounding.cols_round) == 0 and not rounding.round_all:\n                #   Nothing set to round, round all\n                rounding.cols_round = remaining\n\n            df_display = self.round_results(df=df_display,\n                                            rounding=rounding,\n                                            display_only=True)\n\n        if len(rename):\n            df_display = df_display.rename(rename)\n        return nw_type.from_polars(df_display)\n\n\n    def _df_ci(self,\n               ci_level:float=0.95):\n\n        #   Use scipy to get the t-stat ci multiple\n        def to_ci_multiple(value) -&gt; float:\n            return scipy_stats.t.ppf(1-(1-ci_level)/2,\n                                     value)\n\n        nw_type = NarwhalsType(self.df_df)\n        dof = (nw_type.to_polars.lazy().collect()\n                    .with_columns(\n                        pl_cs.numeric().map_elements(\n                            to_ci_multiple,\n                            return_dtype=pl.Float64\n                        )\n                    )\n                )\n\n        #   Use numpy to multiply the se by the t-stat ci multiple\n        se_np = (\n            nw.from_native(self.df_ses)\n            .select(cs.numeric())\n            .lazy()\n            .collect()\n            .to_numpy()\n        )\n        dof_np = (\n            nw.from_native(dof)\n            .select(cs.numeric())\n            .lazy()\n            .collect()\n            .to_numpy()\n        )\n\n        #   return the data\n        dof = pl.concat(\n            [\n                dof.select(self.join_on),\n                pl.from_numpy(\n                    se_np*dof_np,\n                    schema={coli:pl.Float64 for coli in safe_columns(dof.select(cs.numeric()))}\n                )\n            ],\n            how=\"horizontal\"\n        )\n\n        return nw_type.from_polars(dof)\n\n    def filter(self, filter_expr: nw.Expr) -&gt; MultipleImputation:\n        #   Don't edit the underlying object\n        self = self.copy()\n\n        for dfi_name in self._df_attributes:\n            apply_as_attribute(\n                obj=self,\n                df_name=dfi_name,\n                nw_expr=filter_expr,\n                nw_method=\"filter\"\n            )\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].filter(filter_expr)\n\n        return self\n\n    def select(self,\n               select_expr:nw.Expr | str | list[str] | list[nw.Expr]\n               ) -&gt; MultipleImputation:\n\n        self = self.copy()\n        select_expr = list_input(select_expr)\n        cols_keep = columns_from_list(\n            self.df_estimates,\n            columns=select_expr\n        )\n\n        select_df_p = []\n        for coli in cols_keep:\n            cols_check = [f\"{coli}_df\",\n                          f\"{coli}_t\"]\n\n            for checki in cols_check:\n                if checki in safe_columns(self.df_p):\n                    select_df_p.append(checki)\n\n\n        add_join_on = list(set(self.join_on).difference(cols_keep))\n        cols_keep = add_join_on + cols_keep\n\n\n\n        for dfi in self._df_attributes:\n            if dfi == \"df_p\":\n                apply_as_attribute(\n                    obj=self,\n                    df_name=dfi,\n                    nw_expr=cols_keep + select_df_p,\n                    nw_method=\"select\"\n                )\n            else:\n                apply_as_attribute(\n                    obj=self,\n                    df_name=dfi,\n                    nw_expr=cols_keep,\n                    nw_method=\"select\"\n                )\n\n            for repi in range(0,len(self.implicate_stats)):\n                self.implicate_stats[repi].select(cols_keep)\n        return self\n\n    def with_columns(self,\n                     with_expr:nw.Expr | list[nw.Expr]) -&gt; MultipleImputation:\n\n        self = self.copy()\n\n        for dfi in self._df_attributes:\n            apply_as_attribute(\n                obj=self,\n                df_name=dfi,\n                nw_expr=with_expr,\n                nw_method=\"with_columns\"\n            )\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].with_columns(with_expr)\n        return self\n    def rename(self,\n               d_rename:dict[str,str]) -&gt; MultipleImputation:\n\n        self = self.copy()\n\n        for dfi in self._df_attributes:\n            apply_as_attribute(\n                obj=self,\n                df_name=dfi,\n                nw_expr=d_rename,\n                nw_method=\"rename\"\n            )\n\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].rename(d_rename)\n\n        return self\n\n\n\n    def scale_by(self,\n                 factor:float,\n                 columns:list[str] | str | None=None) -&gt; MultipleImputation:\n\n        if columns is None:\n            #   Any columns that aren't the join_on ones\n            columns = list(set(safe_columns(self.df_estimates)).difference(self.join_on))\n\n\n        return self.with_columns(with_expr=nw.col(columns)*factor)\n\n    def pipe(self,\n             function:Callable,\n             *args,\n             **kwargs) -&gt; MultipleImputation:\n        \"\"\"\n        Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)\n\n        Parameters\n        ----------\n        function : Callable\n            Function to pipe.\n        *args : TYPE\n            arguments to function\n        **kwargs : TYPE\n            keyword arguments to function\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        self = self.copy()\n\n        for dfi_name in self._df_attributes:\n            dfi = getattr(self, dfi_name)\n            if dfi is not None:\n                setattr(self, dfi_name, nw.to_native(function(nw.from_native(dfi), *args, **kwargs)))            \n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].pipe(function,\n                                            *args,\n                                            **kwargs)\n\n        return self\n\n    def concat_with(self,\n                    mi_concat:MultipleImputation,\n                    how:str=\"horizontal\") -&gt; MultipleImputation:\n        \"\"\"\n        Concatenate this with another mi object\n\n        Parameters\n        ----------\n        mi_concat : MultipleImputation\n            Other mi object to concatenate with.\n        how : str, optional\n            horizontal or vertical?\n            Horizontal will actually do a join and vertical will just stack them\n            The default is \"horizontal\".\n\n        Returns\n        -------\n        MultipleImputation\n\n        \"\"\"\n\n        self = self.copy()\n\n        def _concat_df(df:IntoFrameT,\n                       df_join:IntoFrameT) -&gt; IntoFrameT:\n            if how == \"horizontal\":\n                return join_wrapper(\n                    df=df,\n                    df_to=df_join,\n                    how=\"left\",\n                    left_on=self.join_on,\n                    right_on=mi_concat.join_on\n                )\n            elif how == \"vertical\":\n                return concat_wrapper([df,df_join],how=\"horizontal\")\n\n\n        for dfi in self._df_attributes:\n            setattr(self, \n                    dfi, \n                    _concat_df(df=getattr(self,dfi),\n                               df_join=getattr(mi_concat,dfi)))\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].concat_with(rs_concat=mi_concat.implicate_stats[repi],\n                                                   join_on_self=self.join_on,\n                                                   join_on_concat=mi_concat.join_on)\n\n        return self\n\n\n    def sort(self,\n             sort_expr:nw.Expr | list[nw.Expr] | str | list[str]) -&gt; MultipleImputation:\n\n        self = self.copy()\n        for dfi in self._df_attributes:\n            apply_as_attribute(\n                obj=self,\n                df_name=dfi,\n                nw_expr=sort_expr,\n                nw_method=\"sort\"\n            )\n\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].sort(sort_expr)\n\n        return self\n\n    def drop(self,\n             drop_expr:nw.Expr | list[nw.Expr] | str | list[str]) -&gt; MultipleImputation:\n\n        self = self.copy()\n        for dfi in self._df_attributes:\n            apply_as_attribute(\n                obj=self,\n                df_name=dfi,\n                nw_expr=drop_expr,\n                nw_method=\"drop\"\n            )\n\n        for repi in range(0,len(self.implicate_stats)):\n            self.implicate_stats[repi].drop(drop_expr)\n\n        return self\n\n    # def reshape_groups_wide_long(self,\n    #                              copy:bool=False,\n    #                              group_first:bool=True,\n    #                              group_col:str=\"Group\",\n    #                              invert_group:bool=False) -&gt; MultipleImputation:\n    #     if copy:\n    #         self = self.copy()\n\n\n\n    #     def _reshape(df:IntoFrameT,\n    #                  join_on:list[str]) -&gt; IntoFrameT:\n    #         if \"___replicate___\" in df.columns:\n    #             join_on = join_on + [\"___replicate___\"]\n\n\n    #         df_concat_by_groups = {}\n    #         for coli in df.columns:\n    #             if coli not in join_on:\n    #                 coli_group = coli.split(\":\")[0]\n    #                 coli_value = coli.split(\":\")[1]\n    #                 c_name = pl.col(join_on[0])\n    #                 rename = {coli:coli_value}\n\n    #                 if group_first:\n    #                     with_name = pl.concat_str([pl.lit(coli_group),\n    #                                                pl.lit(\":\"),\n    #                                                c_name]).alias(c_name.meta.output_name())\n    #                 else:\n    #                     with_name = pl.concat_str([c_name,\n    #                                                pl.lit(\":\"),\n    #                                                pl.lit(coli_group)]).alias(c_name.meta.output_name())\n\n    #                 with_name = with_name.alias(c_name.meta.output_name())\n\n\n\n    #                 if group_col != \"\":\n    #                     if invert_group:\n    #                         with_name = [with_name,\n    #                                      c_name.alias(group_col)]\n    #                     else:\n    #                         with_name = [with_name,\n    #                                      pl.lit(coli_group).alias(group_col)]\n\n    #                 dfi = (df.select(join_on + [coli])\n    #                          .rename(rename)\n    #                          .with_columns(with_name))\n    #                 if coli_group in df_concat_by_groups.keys():\n    #                     df_concat_by_groups[coli_group].append(dfi)\n    #                 else:\n    #                     df_concat_by_groups[coli_group] = [dfi]\n    #                 # df_concat.append((df.select(join_on + [coli])\n    #                 #                     .rename(rename)\n    #                 #                     .with_columns(with_name)))\n\n\n    #         df_concat = []\n    #         for keyi, itemi in df_concat_by_groups.items():\n    #             if len(itemi) == 1:\n    #                 df_concat.append(itemi)\n    #             else:\n    #                 if group_col != \"\":\n    #                     join_on_group = join_on + [group_col]\n    #                 else:\n    #                     join_on_group = join_on\n\n    #                 df_concat.append(JoinFileList_Simple(itemi,\n    #                                                      Join=\"outer\",\n    #                                                      JoinOn=join_on_group))\n\n    #         df_concat = pl.concat(df_concat,\n    #                               how=\"diagonal_relaxed\")\n\n\n\n\n    #         return df_concat\n\n\n    #     self = self.pipe(_reshape,\n    #                      join_on=self.join_on)\n\n    #     return self\n\n\n\n\n    def drb_round_table(self,\n                        columns:list|str|None=None,\n                        columns_n:list|str|None=None,\n                        columns_exclude:list|str|None=None,\n                        round_all:bool=True,\n                        digits:int=4,\n                        compress:bool=False) -&gt; MultipleImputation:\n\n        kwargs = copy(locals())\n        del kwargs[\"self\"]\n        self.pipe(function=drb_round_table,\n                  **kwargs)\n\n        return self\n\n    @property   \n    def _df_attributes(self) -&gt; list[str]:\n        return [\"df_estimates\",\n                 \"df_ses\",\n                 \"df_df\",\n                 \"df_t\",\n                 \"df_p\",\n                 \"df_rate_of_missing_information\"]\n\n\n    @property\n    def n_implicates(self) -&gt; int:\n        return len(self.implicate_stats)\n\n    @property\n    def summarize_vars(self) -&gt; list:\n        return self.implicate_stats[0].df_estimates.drop(self.join_on).columns\n\n\n\n    @classmethod\n    def concat(cls,\n               mi_list:list[MultipleImputation]) -&gt; MultipleImputation:\n        from .comparisons import _append_to_mi\n\n        mi_out = mi_list[0]\n\n        for i in range(1,len(mi_list)):\n            mi_out = _append_to_mi(name=\"\",\n                                   stat_item=mi_out,\n                                   stats=mi_list[i],\n                                   vertical=True,\n                                   n_implicates=mi_out.n_implicates,\n                                   vertical_drop_var_name=False,\n                                   separator=\"\")\n\n        return mi_out\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.calculate","title":"calculate","text":"<pre><code>calculate(implicate_name='___implicate___')\n</code></pre> <p>Calculate multiple imputation estimates using Rubin's rules.</p> <p>Combines estimates from individual implicates to produce final MI estimates, standard errors, degrees of freedom, t-statistics, p-values, and rates of missing information. Implements the standard MI combining formulas.</p> <p>Parameters:</p> Name Type Description Default <code>implicate_name</code> <code>str</code> <p>Column name identifier for imputation number. Default is \"implicate\".</p> <code>'___implicate___'</code> Notes <p>Implements Rubin's combining rules: - Q\u0304 = (1/m) \u03a3 Q\u0302\u1d62  (combined estimate) - U = (1/m) \u03a3 U\u1d62    (within-imputation variance) - B = (1/(m-1)) \u03a3 (Q\u0302\u1d62 - Q\u0304)\u00b2  (between-imputation variance) - T = U + (1 + 1/m)B  (total variance) - \u03bd = (m-1)[1 + mU/((m+1)B)]\u00b2  (degrees of freedom)</p> <p>where m is the number of implicates.</p> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def calculate(self,\n              implicate_name=\"___implicate___\"):\n    \"\"\"\n    Calculate multiple imputation estimates using Rubin's rules.\n\n    Combines estimates from individual implicates to produce final MI estimates,\n    standard errors, degrees of freedom, t-statistics, p-values, and rates of\n    missing information. Implements the standard MI combining formulas.\n\n    Parameters\n    ----------\n    implicate_name : str, optional\n        Column name identifier for imputation number. Default is \"___implicate___\".\n\n    Notes\n    -----\n    Implements Rubin's combining rules:\n    - Q\u0304 = (1/m) \u03a3 Q\u0302\u1d62  (combined estimate)\n    - U = (1/m) \u03a3 U\u1d62    (within-imputation variance)\n    - B = (1/(m-1)) \u03a3 (Q\u0302\u1d62 - Q\u0304)\u00b2  (between-imputation variance)\n    - T = U + (1 + 1/m)B  (total variance)\n    - \u03bd = (m-1)[1 + mU/((m+1)B)]\u00b2  (degrees of freedom)\n\n    where m is the number of implicates.\n    \"\"\"\n\n    df_estimates_stacked = []\n    df_ses_stacked = []\n\n\n    col_sort = \"___mi_sort___\"        \n    for indexi, imp_statsi in enumerate(self.implicate_stats):\n        if indexi == 0:\n            df_sort = (\n                nw.from_native(imp_statsi.df_estimates)\n                .select(self.join_on)\n                .lazy()\n                .collect()\n                .with_row_index(col_sort)\n                .lazy()\n                .to_native()\n            )\n        df_estimates_stacked.append(imp_statsi.df_estimates)\n        df_ses_stacked.append(imp_statsi.df_ses)\n\n\n    df_estimates_stacked = (\n        nw.from_native(\n            fill_missing(\n                concat_wrapper(\n                    df_estimates_stacked,\n                    how=\"diagonal\"\n                ),\n                value=float(\"nan\")\n            )\n        )\n        .lazy()\n        .collect()\n        .to_native()\n    )\n    df_ses_stacked = (\n        nw.from_native(\n            concat_wrapper(\n                df_ses_stacked,\n                how=\"diagonal\"\n            )\n        )\n        .with_columns(cs.boolean().cast(nw.Int8))\n        .lazy()\n        .collect()\n        .to_native()\n    )\n\n    cols_non_stats = self.join_on + [implicate_name,col_sort]\n    cols_stats = safe_columns(df_estimates_stacked)\n    cols_stats = _columns_original_order(\n        columns_unordered=list(set(cols_stats).difference(cols_non_stats)),\n        columns_ordered=cols_stats\n    )\n\n    #   Notation from Joe Schaffer MI FAQ page (downloaded from the Wayback Machine)\n    #       NEWS/Documentation/Background/MultipleImputation/MI_FAQ.htm\n    df_estimates = (\n        nw.from_native(df_estimates_stacked)\n        .group_by(self.join_on).agg(nw.all().mean())\n        .sort(self.join_on)\n    )\n\n    c_stats = nw.col(cols_stats)\n    with_between = []\n    for coli in cols_stats:\n        c_col = nw.col(coli)\n        c_bar = nw.col(f\"{coli}_bar\")\n        with_between.append(\n            (1/(self.n_implicates-1)*(c_col - c_bar)**2).alias(coli)\n            )\n\n    df_B = (\n        nw.from_native(\n            join_list(\n                [\n                    df_estimates_stacked,\n                    (\n                        nw.from_native(df_estimates)\n                        .rename({coli:f\"{coli}_bar\" for coli in cols_stats})\n                        .to_native()\n                    )\n                ],\n                on=self.join_on,\n                how=\"left\"\n            )\n        )\n        .with_columns(with_between)\n        .group_by(self.join_on).agg(c_stats.sum())\n        .to_native()\n    )\n\n    df_U = (\n        nw.from_native(df_ses_stacked)\n        .with_columns(c_stats**2)\n        .group_by(self.join_on).agg(nw.all().mean())\n        .to_native()\n    )\n\n    df_variance = (\n        nw.from_native(\n            concat_wrapper(\n                [\n                    nw.from_native(df_B)\n                    .with_columns(c_stats*(1+1/self.n_implicates))\n                    .to_native(),\n                    df_U\n                ],\n                how=\"diagonal\"\n            )\n        )\n        .group_by(self.join_on).agg(nw.all().sum())\n        .to_native()\n    )\n\n    self.df_ses = (\n        nw.from_native(df_variance)\n        .with_columns(c_stats**.5)\n        .to_native()\n    )\n\n    self.df_estimates = (\n        nw.from_native(df_estimates_stacked)\n        .group_by(self.join_on).agg(nw.all().mean())\n        .sort(self.join_on)\n        .to_native()\n    )\n\n    df_B_U = join_list(\n        [\n            (\n                nw.from_native(df_B)\n                .rename({coli:f\"{coli}_B\" for coli in cols_stats})\n                .to_native()\n            ),\n            (\n                nw.from_native(df_U)\n                .rename({coli:f\"{coli}_U\" for coli in cols_stats})\n                .to_native()\n            )\n        ],\n        on=self.join_on,\n        how=\"left\"\n    )\n\n    m = self.n_implicates\n    with_df = []\n    for coli in cols_stats:\n        c_U = nw.col(f\"{coli}_U\")\n        c_B = nw.col(f\"{coli}_B\")\n\n        with_df.append(\n                ((m - 1)*(1+(m*c_U)/((m+1)*c_B))**2).alias(coli)\n            )\n\n    self.df_df = (\n        nw.from_native(df_B_U)\n        .select(self.join_on + with_df)\n        .to_native()\n    )\n\n\n\n\n    with_r = []\n    with_gamma = []\n    for coli in cols_stats:\n        c_U = nw.col(f\"{coli}_U\")\n        c_B = nw.col(f\"{coli}_B\")\n        c_r = nw.col(f\"{coli}_r\")\n        c_df = nw.col(f\"{coli}_df\")\n        with_r.append(\n                ((1 + 1/m)*c_B/c_U).alias(f\"{coli}_r\")\n            )\n\n\n        with_gamma.append(\n                ((c_r + (2/(c_df+3)))/(c_r+1)).alias(coli)\n            )\n\n    self.df_rate_of_missing_information = (\n        nw.from_native(\n            join_list(\n                [\n                    df_B_U,\n                    (\n                        nw.from_native(self.df_df)\n                        .rename({coli:f\"{coli}_df\" for coli in cols_stats})\n                        .to_native()\n                    )\n                ],\n                how=\"left\",\n                on=self.join_on\n            )\n        )\n        .with_columns(with_r)\n        .select(self.join_on + with_gamma)\n        .to_native()\n    )\n\n    with_t = []\n    for coli in cols_stats:\n        c_est = nw.col(coli)\n        c_se = nw.col(f\"{coli}_se\")\n\n        with_t.append((c_est/c_se).abs().alias(coli))\n\n    self.df_t = (\n        nw.from_native(\n            join_list(\n                [\n                    self.df_estimates,\n                    (\n                        nw.from_native(self.df_ses)\n                        .rename({coli:f\"{coli}_se\" for coli in cols_stats})\n                        .to_native()\n                    )\n                ],\n                on=self.join_on,\n                how=\"left\"\n            )\n        )\n        .select(self.join_on + with_t)\n        .to_native()\n    )\n\n\n    df_p = join_list(\n        [\n            (\n                nw.from_native(self.df_df)\n                .rename({coli:f\"{coli}_df\" for coli in cols_stats})\n                .to_native()\n            ),\n            (\n                nw.from_native(self.df_t)\n                .rename({coli:f\"{coli}_t\" for coli in cols_stats})\n                .to_native()\n            )\n        ],\n        how=\"left\",\n        on=self.join_on\n    )\n\n    def p_value(t,df):\n        if t == float('inf') or t == float('nan'):\n            return 0\n        if df == float('inf') or df == float('nan'):\n            df = 1_000_000\n\n        return scipy.stats.t.sf(t,df)*2\n\n\n    def p_value_lambda(t,\n                       col_t,\n                       col_df):\n        try:\n            return p_value(t[col_t],t[col_df])\n        except:\n            return None\n\n    nw_type = NarwhalsType(df_p)\n    df_p = nw_type.to_polars().lazy().collect()\n    for coli in cols_stats:\n        index_t = df_p.columns.index(f\"{coli}_t\")\n        index_df = df_p.columns.index(f\"{coli}_df\")\n\n        df_p = (df_p.with_columns(df_p.map_rows(lambda t: p_value_lambda(t,index_t,index_df),\n                                                return_dtype=pl.Float64))\n                    .rename({\"map\":coli})\n                    )\n\n\n    self.df_p = nw_type.from_polars(\n        df_p.select(self.join_on + cols_stats)\n    )\n\n    def _sort_and_fill_null(df:IntoFrameT) -&gt; IntoFrameT:\n        nw_type = NarwhalsType(join_list([df,df_sort],how=\"left\",on=self.join_on))\n        return nw_type.from_polars(\n            nw_type.to_polars()\n            .sort([col_sort],\n                    maintain_order=True)\n            .drop(col_sort)\n            .fill_nan(None)\n        )\n\n\n\n    self.df_estimates = _sort_and_fill_null(self.df_estimates)\n    self.df_ses = _sort_and_fill_null(self.df_ses)\n    self.df_df = _sort_and_fill_null(self.df_df)\n    self.df_t = _sort_and_fill_null(self.df_t)\n    self.df_p = _sort_and_fill_null(df_p)\n    self.df_rate_of_missing_information = _sort_and_fill_null(self.df_rate_of_missing_information)\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.compare","title":"compare","text":"<pre><code>compare(\n    other: ReplicateStats\n    | MultipleImputation\n    | StatCalculator,\n    difference: bool = True,\n    ratio: bool = True,\n    ratio_minus_1: bool = True,\n    replicate_name: str = \"___replicate___\",\n    compare_list_variables: list[Variable] | None = None,\n    compare_list_columns: list[Column] | None = None,\n) -&gt; dict[str, MultipleImputation]\n</code></pre> <p>Compare this set of MI estimates to another set of estimates.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>MultipleImputation | ReplicateStats | StatCalculator</code> <p>The other object to compare against.</p> required <code>difference</code> <code>bool</code> <p>Calculate and return differences. Default is True.</p> <code>True</code> <code>ratio</code> <code>bool</code> <p>Calculate and return ratios. Default is True.</p> <code>True</code> <code>ratio_minus_1</code> <code>bool</code> <p>Subtract 1 from ratios (for percentage change interpretation). Default is True.</p> <code>True</code> <code>replicate_name</code> <code>str</code> <p>Column name for replicate identifier. Default is \"replicate\".</p> <code>'___replicate___'</code> <code>compare_list_variables</code> <code>list[Variable] | None</code> <p>List of variable info to compare.</p> <code>None</code> <code>compare_list_columns</code> <code>list[Columns] | None</code> <p>List of column info to compare.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, MultipleImputation]</code> <p>Dictionary with keys [\"difference\", \"ratio\"] containing MultipleImputation objects with comparison results and proper MI standard errors for the comparisons.</p> <p>Examples:</p> <p>Compare mi_news to sc_survey (StatCalculator)</p> <pre><code>&gt;&gt;&gt; comparison = mi_news.compare(\n...     sc_survey\n... )\n</code></pre> <p>Compare means vs medians:</p> <pre><code>&gt;&gt;&gt; comparison = mi_means.compare(\n...     mi_medians,\n...     compare_list_columns=compare_list_variables=[\n...     ComparisonItem.Variable(\n...         \"mean\",\n...         \"median\",\n...         name=\"median_mean\"\n...     )]\n... )\n&gt;&gt;&gt; comparison[\"difference\"].print()\n</code></pre> <p>Compare specific variables:</p> <pre><code>&gt;&gt;&gt; comparison = mi_results1.compare(\n...     mi_results2,\n...     compare_list_variables=[ComparisonItem.Variable(\n...         value1=\"income\",\n...         value2=\"income_2\",\n...         name=\"income_comp\"\n...     )]\n... )\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def compare(self,\n            other:ReplicateStats | MultipleImputation | StatCalculator,\n            difference:bool=True,\n            ratio:bool=True,\n            ratio_minus_1:bool=True,\n            replicate_name:str=\"___replicate___\",\n            compare_list_variables:list[ComparisonItem.Variable] | None=None,\n            compare_list_columns:list[ComparisonItem.Column] | None=None) -&gt; dict[str,MultipleImputation]:\n    \"\"\"\n    Compare this set of MI estimates to another set of estimates.\n\n    Parameters\n    ----------\n    other : MultipleImputation | ReplicateStats | StatCalculator\n        The other object to compare against.\n    difference : bool, optional\n        Calculate and return differences. Default is True.\n    ratio : bool, optional\n        Calculate and return ratios. Default is True.\n    ratio_minus_1 : bool, optional\n        Subtract 1 from ratios (for percentage change interpretation).\n        Default is True.\n    replicate_name : str, optional\n        Column name for replicate identifier. Default is \"___replicate___\".\n    compare_list_variables : list[ComparisonItem.Variable] | None, optional\n        List of variable info to compare.\n    compare_list_columns : list[ComparisonItem.Columns] | None, optional\n        List of column info to compare.\n\n    Returns\n    -------\n    dict[str, MultipleImputation]\n        Dictionary with keys [\"difference\", \"ratio\"] containing\n        MultipleImputation objects with comparison results and proper\n        MI standard errors for the comparisons.\n\n    Examples\n    --------\n    Compare mi_news to sc_survey (StatCalculator)\n    &gt;&gt;&gt; comparison = mi_news.compare(\n    ...     sc_survey\n    ... )\n\n    Compare means vs medians:\n    &gt;&gt;&gt; comparison = mi_means.compare(\n    ...     mi_medians,\n    ...     compare_list_columns=compare_list_variables=[\n    ...     ComparisonItem.Variable(\n    ...         \"mean\",\n    ...         \"median\",\n    ...         name=\"median_mean\"\n    ...     )]\n    ... )\n    &gt;&gt;&gt; comparison[\"difference\"].print()\n\n    Compare specific variables:\n\n    &gt;&gt;&gt; comparison = mi_results1.compare(\n    ...     mi_results2,\n    ...     compare_list_variables=[ComparisonItem.Variable(\n    ...         value1=\"income\",\n    ...         value2=\"income_2\",\n    ...         name=\"income_comp\"\n    ...     )]\n    ... )\n    \"\"\"\n\n\n\n    return kit_comparisons.compare(stats1=self,\n                            stats2=other,\n                            join_on=self.join_on,\n                            rounding=self.rounding,\n                            difference=difference,\n                            ratio=ratio,\n                            ratio_minus_1=ratio_minus_1,\n                            replicate_name=replicate_name,\n                            compare_list_variables=compare_list_variables,\n                            compare_list_columns=compare_list_columns)\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.concat_with","title":"concat_with","text":"<pre><code>concat_with(\n    mi_concat: MultipleImputation, how: str = \"horizontal\"\n) -&gt; MultipleImputation\n</code></pre> <p>Concatenate this with another mi object</p> <p>Parameters:</p> Name Type Description Default <code>mi_concat</code> <code>MultipleImputation</code> <p>Other mi object to concatenate with.</p> required <code>how</code> <code>str</code> <p>horizontal or vertical? Horizontal will actually do a join and vertical will just stack them The default is \"horizontal\".</p> <code>'horizontal'</code> <p>Returns:</p> Type Description <code>MultipleImputation</code> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def concat_with(self,\n                mi_concat:MultipleImputation,\n                how:str=\"horizontal\") -&gt; MultipleImputation:\n    \"\"\"\n    Concatenate this with another mi object\n\n    Parameters\n    ----------\n    mi_concat : MultipleImputation\n        Other mi object to concatenate with.\n    how : str, optional\n        horizontal or vertical?\n        Horizontal will actually do a join and vertical will just stack them\n        The default is \"horizontal\".\n\n    Returns\n    -------\n    MultipleImputation\n\n    \"\"\"\n\n    self = self.copy()\n\n    def _concat_df(df:IntoFrameT,\n                   df_join:IntoFrameT) -&gt; IntoFrameT:\n        if how == \"horizontal\":\n            return join_wrapper(\n                df=df,\n                df_to=df_join,\n                how=\"left\",\n                left_on=self.join_on,\n                right_on=mi_concat.join_on\n            )\n        elif how == \"vertical\":\n            return concat_wrapper([df,df_join],how=\"horizontal\")\n\n\n    for dfi in self._df_attributes:\n        setattr(self, \n                dfi, \n                _concat_df(df=getattr(self,dfi),\n                           df_join=getattr(mi_concat,dfi)))\n\n    for repi in range(0,len(self.implicate_stats)):\n        self.implicate_stats[repi].concat_with(rs_concat=mi_concat.implicate_stats[repi],\n                                               join_on_self=self.join_on,\n                                               join_on_concat=mi_concat.join_on)\n\n    return self\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.pipe","title":"pipe","text":"<pre><code>pipe(\n    function: Callable, *args, **kwargs\n) -&gt; MultipleImputation\n</code></pre> <p>Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to pipe.</p> required <code>*args</code> <code>TYPE</code> <p>arguments to function</p> <code>()</code> <code>**kwargs</code> <code>TYPE</code> <p>keyword arguments to function</p> <code>{}</code> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def pipe(self,\n         function:Callable,\n         *args,\n         **kwargs) -&gt; MultipleImputation:\n    \"\"\"\n    Pipe a function to df_estimates, df_ses, and df_replicates (as necessary)\n\n    Parameters\n    ----------\n    function : Callable\n        Function to pipe.\n    *args : TYPE\n        arguments to function\n    **kwargs : TYPE\n        keyword arguments to function\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    self = self.copy()\n\n    for dfi_name in self._df_attributes:\n        dfi = getattr(self, dfi_name)\n        if dfi is not None:\n            setattr(self, dfi_name, nw.to_native(function(nw.from_native(dfi), *args, **kwargs)))            \n\n    for repi in range(0,len(self.implicate_stats)):\n        self.implicate_stats[repi].pipe(function,\n                                        *args,\n                                        **kwargs)\n\n    return self\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.print","title":"print","text":"<pre><code>print(\n    round_output: bool | int | None = None,\n    sub_log: logging = None,\n)\n</code></pre> <p>Print the MI estimates and standard errors to the log.</p> <p>Parameters:</p> Name Type Description Default <code>round_output</code> <code>bool | int | None</code> <p>Rounding rule (True for DRB, int for significant digits). Default is self.rounding configuration.</p> <code>None</code> <code>sub_log</code> <code>logging</code> <p>Alternative logger to use. Default is None (use standard logging).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mi_results.print(round_output=True)  # Use DRB rounding\n&gt;&gt;&gt; mi_results.print(round_output=3)     # 3 significant digits\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def print(self,\n          round_output:bool|int|None=None,\n          sub_log:logging=None):\n    \"\"\"\n    Print the MI estimates and standard errors to the log.\n\n    Parameters\n    ----------\n    round_output : bool|int|None, optional\n        Rounding rule (True for DRB, int for significant digits).\n        Default is self.rounding configuration.\n    sub_log : logging, optional\n        Alternative logger to use. Default is None (use standard logging).\n\n    Examples\n    --------\n    &gt;&gt;&gt; mi_results.print(round_output=True)  # Use DRB rounding\n    &gt;&gt;&gt; mi_results.print(round_output=3)     # 3 significant digits\n    \"\"\"\n\n    if sub_log is None:\n        sub_log = logger\n\n    #   Round?\n    if round_output:\n        if self.rounding is None:\n            rounding = Rounding()\n        else:\n            rounding = deepcopy(self.rounding)\n        rounding.set_round_digits(round_output)\n\n        df_estimates = self.round_results(df=self.df_estimates,\n                                          rounding=rounding,\n                                          display_only=True)\n        df_ses = self.round_results(df=self.df_ses,\n                                    rounding=rounding,\n                                    display_only=True)\n    else:\n        df_estimates = self.df_estimates\n        df_ses = self.df_ses\n\n\n    print_se_table(df_estimates=df_estimates,\n                   df_ses=df_ses,\n                   # display_all_vars=self.display_all_vars,\n                   # display_max_vars=self.display_max_vars,\n                   sort_vars=self.join_on,\n                   round_output=False,\n                   sub_log=sub_log)\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.round_results","title":"round_results","text":"<pre><code>round_results(\n    df: IntoFrameT = None,\n    rounding: Rounding | None = None,\n    display_only: bool = False,\n) -&gt; IntoFrameT\n</code></pre> <p>Apply rounding rules to MI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Table of estimates to round. Default is df_estimates.</p> <code>None</code> <code>rounding</code> <code>Rounding | None</code> <p>Rounding configuration (True for DRB rules, int for significant digits). Default is self.rounding.</p> <code>None</code> <code>display_only</code> <code>bool</code> <p>If True, converts numbers to strings for display only. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>The dataframe with rounding applied.</p> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def round_results(self,\n                  df:IntoFrameT=None,\n                  rounding:Rounding|None=None,\n                  display_only:bool=False) -&gt; IntoFrameT:\n\n    \"\"\"\n    Apply rounding rules to MI estimates.\n\n    Parameters\n    ----------\n    df : IntoFrameT, optional\n        Table of estimates to round. Default is df_estimates.\n    rounding : Rounding|None, optional\n        Rounding configuration (True for DRB rules, int for significant digits).\n        Default is self.rounding.\n    display_only : bool, optional\n        If True, converts numbers to strings for display only. Default is False.\n\n    Returns\n    -------\n    IntoFrameT\n        The dataframe with rounding applied.\n    \"\"\"\n\n\n    if df is None:\n        df = self.df_estimates\n\n    if rounding is None:\n        rounding = self.rounding\n\n    if df is not None:\n        df = drb_round_table(df=df,\n                             columns=rounding.cols_round,\n                             columns_n=rounding.cols_n,\n                             columns_exclude=rounding.cols_exclude,\n                             round_all=rounding.round_all,\n                             digits=rounding.round_digits,\n                             display_only=display_only)\n\n    return df\n</code></pre>"},{"location":"api/multiple_imputation/#survey_kit.statistics.multiple_imputation.MultipleImputation.table_of_estimates","title":"table_of_estimates","text":"<pre><code>table_of_estimates(\n    round_output: bool | int | None = None,\n    estimates_to_show: list[str] | None = None,\n    variable_prefix: str = \"\",\n    estimate_type_variable_name: str = \"Statistic\",\n    ci_level: float = 0.95,\n) -&gt; IntoFrameT\n</code></pre> <p>Create a formatted table combining different types of estimates.</p> <p>Reshapes estimates into a long format table with different statistic types (estimates, SEs, t-stats, p-values, etc.) as separate rows.</p> <p>Parameters:</p> Name Type Description Default <code>round_output</code> <code>bool | int | None</code> <p>Rounding rule for display.</p> <code>None</code> <code>estimates_to_show</code> <code>list[str] | None</code> <p>List of estimate types to include. Options: - \"estimate\": Point estimates - \"se\": Standard errors - \"t\": T-statistics - \"p\": P-values - \"ci\": Confidence intervals - \"df\": Degrees of freedom Default is [\"estimate\", \"se\"].</p> <code>None</code> <code>variable_prefix</code> <code>str</code> <p>Prefix to add to variable column names. Default is \"\".</p> <code>''</code> <code>estimate_type_variable_name</code> <code>str</code> <p>Name for column indicating statistic type. Default is \"Statistic\".</p> <code>'Statistic'</code> <code>ci_level</code> <code>float</code> <p>Confidence level for confidence intervals. Default is 0.95.</p> <code>0.95</code> <p>Returns:</p> Type Description <code>IntoFrameT</code> <p>Formatted table with estimates arranged by statistic type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; table = mi_results.table_of_estimates(\n...     estimates_to_show=[\"estimate\", \"se\", \"p\", \"ci\"],\n...     round_output=True\n... )\n</code></pre> Source code in <code>src\\survey_kit\\statistics\\multiple_imputation.py</code> <pre><code>def table_of_estimates(self,\n                       round_output:bool|int|None=None,\n                       estimates_to_show:list[str] | None=None,\n                       variable_prefix:str=\"\",\n                       estimate_type_variable_name:str=\"Statistic\",\n                       ci_level:float=0.95) -&gt; IntoFrameT:\n    \"\"\"\n    Create a formatted table combining different types of estimates.\n\n    Reshapes estimates into a long format table with different statistic\n    types (estimates, SEs, t-stats, p-values, etc.) as separate rows.\n\n    Parameters\n    ----------\n    round_output : bool|int|None, optional\n        Rounding rule for display.\n    estimates_to_show : list[str] | None, optional\n        List of estimate types to include. Options:\n        - \"estimate\": Point estimates\n        - \"se\": Standard errors  \n        - \"t\": T-statistics\n        - \"p\": P-values\n        - \"ci\": Confidence intervals\n        - \"df\": Degrees of freedom\n        Default is [\"estimate\", \"se\"].\n    variable_prefix : str, optional\n        Prefix to add to variable column names. Default is \"\".\n    estimate_type_variable_name : str, optional\n        Name for column indicating statistic type. Default is \"Statistic\".\n    ci_level : float, optional\n        Confidence level for confidence intervals. Default is 0.95.\n\n    Returns\n    -------\n    IntoFrameT\n        Formatted table with estimates arranged by statistic type.\n\n    Examples\n    --------\n    &gt;&gt;&gt; table = mi_results.table_of_estimates(\n    ...     estimates_to_show=[\"estimate\", \"se\", \"p\", \"ci\"],\n    ...     round_output=True\n    ... )\n    \"\"\"\n    if estimates_to_show is None:\n        estimates_to_show = [\"estimate\",\n                             \"se\"]\n\n\n    df_ordered = []\n    col_sort = \"__order_output_table__\"\n    for index, esti in enumerate(estimates_to_show):\n        if esti.lower() == \"estimate\":\n            df_ordered.append(\n                nw.from_native(self.df_estimates)\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .to_native()\n            )\n        elif esti.lower() == \"se\":\n            df_ordered.append(\n                nw.from_native(self.df_ses)\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .to_native()\n            )\n        elif esti.lower() == \"t\":\n            df_ordered.append(\n                nw.from_native(self.df_t)\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .to_native()\n            )\n        elif esti.lower() == \"p\":\n            cols_to_keep = safe_columns(\n                nw.from_native(self.df_estimates)\n                .lazy()\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .to_native()\n            )\n\n\n            df_p = (\n                nw.from_native(self.df_p)\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .select(cols_to_keep)\n                .to_native()\n            )\n\n            when_then_censor_absurd_values = []\n            absurd_value_threshold = 1E-10\n            #   I don't want to see p-values below a ridiculously low number\n            for coli in self.df_estimates.columns:\n                if coli not in self.join_on:\n                    ci = nw.col(coli)\n                    when_then_censor_absurd_values.append(\n                            ((nw.when(ci.lt(absurd_value_threshold))\n                                .then(nw.lit(0.0))\n                                .otherwise(ci)).alias(coli))\n                        )\n\n            df_ordered.append(\n                nw.from_native(df_p)\n                .with_columns(when_then_censor_absurd_values)\n                .to_native()\n            )\n\n        elif esti.lower() == \"ci\":\n            df_ordered.append(\n                nw.from_native(self._df_ci(ci_level=ci_level))\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n                .to_native()\n            )\n        elif esti.lower() == \"df\":\n            with_infinites = []\n            for coli in self.summarize_vars:\n                c_coli = nw.col(coli)\n\n                upper_limit = 10**7\n                with_infinites.append(\n                                (nw.when(c_coli.gt(upper_limit)).then(nw.lit(upper_limit))\n                                   .otherwise(c_coli).alias(coli)\n                               )\n                        )\n\n\n            df_ordered.append(\n                nw.from_native(self.df_df)\n                .with_columns(with_infinites)\n                .with_columns(\n                    [\n                        nw.lit(index).alias(col_sort),\n                        nw.lit(esti.lower()).alias(estimate_type_variable_name)\n                    ]\n                )\n            )\n        else:\n            message = f\"{esti} not allowed for estimates_to_show\"\n            logger.error(message)\n            raise Exception(message)\n\n    col_row_index = \"___estimate_row_count___\"\n    df_ordered[0] = (\n        nw.from_native(df_ordered[0])\n        .with_row_index(col_row_index)\n        .to_native()\n    )\n\n    df_display = concat_wrapper(\n        df_ordered,\n        how=\"diagonal\"\n    )\n\n    nw_type = NarwhalsType(df_display)\n    df_display = nw_type.to_polars()\n\n    sort_vars = self.join_on\n    df_display = (df_display.sort(sort_vars + [col_sort])\n                            .with_columns(pl.col(col_row_index).forward_fill()))\n    df_display = (df_display.sort([col_row_index] + [col_sort])\n                            .drop(col_row_index))\n\n\n\n\n    #   Clear extraneous information\n    with_clear = []\n    for coli in sort_vars:\n        c_col = pl.col(coli)\n        with_clear.append(pl.when(pl.col(col_sort) != 0)\n                            .then(pl.lit(\"\"))\n                            .otherwise(c_col.cast(pl.String))\n                            .alias(coli))\n\n\n    select_order = sort_vars + [estimate_type_variable_name]\n    remaining = []\n    rename = {}\n    for coli in df_display.columns:\n        if coli not in select_order and coli != col_sort:\n\n\n            if variable_prefix != \"\":\n                rename[coli] = f\"{variable_prefix}{coli}\"\n\n            remaining.append(coli)\n\n\n    df_display = (df_display.with_columns(with_clear)\n                      .select(select_order + remaining))\n    #   Round?\n    if round_output:\n        if self.rounding is not None:\n            rounding = deepcopy(self.rounding)\n        else:\n            rounding = Rounding()\n        rounding.set_round_digits(round_output)\n\n        if len(rounding.cols_n) == 0 and len(rounding.cols_round) == 0 and not rounding.round_all:\n            #   Nothing set to round, round all\n            rounding.cols_round = remaining\n\n        df_display = self.round_results(df=df_display,\n                                        rounding=rounding,\n                                        display_only=True)\n\n    if len(rename):\n        df_display = df_display.rename(rename)\n    return nw_type.from_polars(df_display)\n</code></pre>"},{"location":"api/srmi/","title":"Imputation/SRMI","text":""},{"location":"api/srmi/#survey_kit.imputation.srmi.SRMI","title":"SRMI","text":"<p>               Bases: <code>Serializable</code></p> <p>Sequential Regression Multiple Imputation (SRMI) class for handling missing data imputation.</p> <p>This class manages the complete SRMI process including variable setup, model configuration, parallel execution, and result management across multiple implicates and iterations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrameT</code> <p>Data to be used in the imputation</p> <code>None</code> <code>variables</code> <code>list[Variable]</code> <p>Variables to be imputed (as Variable class instances), by default None</p> <code>None</code> <code>model</code> <code>str | list</code> <p>R string formula that is the default for the imputation</p> <code>''</code> <code>selection</code> <code>Selection</code> <p>Variable selection method used within the imputation (if any), by default None</p> <code>None</code> <code>preselection</code> <code>Selection</code> <p>Variable selection done before SRMI starts to pre-prune inputs, by default None</p> <code>None</code> <code>modeltype</code> <code>ModelType</code> <p>Imputation model type from the ModelType enumeration, by default None</p> <code>None</code> <code>parameters</code> <code>dict</code> <p>Model parameters dictionary, by default None</p> <code>None</code> <code>joint</code> <code>dict</code> <p>Key-value pairs of variables to be included together (i.e. if one is selected     in the variable selection step, the other is too), by default None</p> <code>None</code> <code>ordered_categorical</code> <code>list</code> <p>List of categorical variables in model that are ordered, by default None     An example would be education (vs. a variable with no ordering like state or county code)</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for replicability, by default 0 (no seed)</p> <code>0</code> <code>weight</code> <code>str</code> <p>Weight variable name for imputation modeling, by default \"\"</p> <code>''</code> <code>n_implicates</code> <code>int</code> <p>Number of separate implicates to impute</p> <code>0</code> <code>n_iterations</code> <code>int</code> <p>Number of iterations in each implicate</p> <code>0</code> <code>bayesian_bootstrap</code> <code>bool</code> <p>Use Bayesian Bootstrap to account for uncertainty in coefficients, by default True</p> <code>True</code> <code>bootstrap_index</code> <code>list</code> <p>Index variables for resampling in Bayesian Bootstrap (i.e. if you want to resample by household, not person), by default None</p> <code>None</code> <code>bootstrap_where</code> <code>str</code> <p>SQL Condition for keeping observations when resampling, by default \"\"</p> <code>''</code> <code>index</code> <code>list</code> <p>Columns that uniquely identify observations such as [\"h_seq\",\"pppos\"], by default None</p> <code>None</code> <code>parallel</code> <code>bool</code> <p>Run implicates in parallel, by default True</p> <code>True</code> <code>parallel_variables_per_job</code> <code>int</code> <p>Number of variables per parallel job (for memory management and to deal with memory leaks, if there are any), by default 0</p> <code>0</code> <code>parallel_CallInputs</code> <code>CallInputs | None</code> <p>Parameters for parallel execution such as memory and CPU allocation, by default None</p> <code>None</code> <code>parallel_testing</code> <code>bool</code> <p>Test parallel jobs without running them, by default False</p> <code>False</code> <code>path_model</code> <code>str</code> <p>Directory to save model data and temporary files</p> <code>''</code> <code>model_name</code> <code>str</code> <p>Model name for continuing existing runs, by default \"\"</p> <code>''</code> <code>force_start</code> <code>bool</code> <p>Restart imputation even if existing run exists, by default False</p> <code>False</code> <code>save_every_variable</code> <code>bool</code> <p>Save data after each variable is imputed, by default False</p> <code>False</code> <code>save_every_iteration</code> <code>bool</code> <p>Save data after each iteration completes, by default True</p> <code>True</code> <code>from_load</code> <code>bool</code> <p>Flag indicating object created from saved state, by default False</p> <code>False</code> <code>imputation_stats</code> <code>list[str] | None</code> <p>List of statistics to calculate during imputation, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If n_implicates &lt; 1 or n_iterations &lt; 1 If path equals path_model_new in load_to_continue_prior</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; srmi = SRMI(\n...     df=data,\n...     variables=[var1, var2],\n...     n_implicates=5,\n...     n_iterations=10,\n...     path_model=\"/path/to/model\"\n... )\n&gt;&gt;&gt; srmi.run()\n</code></pre> <p>With parallel execution:</p> <pre><code>&gt;&gt;&gt; srmi = SRMI(\n...     df=data,\n...     variables=vars_list,\n...     n_implicates=5,\n...     n_iterations=10,\n...     parallel=True,\n...     parallel_CallInputs=CallInputs(CPUs=4, MemInMB=5000)\n... )\n</code></pre>"},{"location":"api/srmi/#survey_kit.imputation.srmi.SRMI.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Execute the SRMI imputation process.</p> <p>Orchestrates the complete imputation workflow including initialization, preprocessing, and execution in parallel or sequential mode.</p> Notes <p>The method performs these steps: 1. Creates folders and initializes implicates to be run 2. Preprocesses data (variable selection, hyperparameter tuning) 3. Runs imputation in parallel or sequential mode 4. Saves results and statistics</p> <p>For parallel execution, creates job files for each iteration and variable subset. For sequential execution, runs each implicate directly.</p>"},{"location":"api/srmi/#survey_kit.imputation.variable.Variable","title":"Variable","text":"<p>               Bases: <code>Serializable</code></p>"},{"location":"api/srmi/#survey_kit.imputation.variable.Variable.PrePost","title":"PrePost","text":"<p>Namespace within Variable class for handling pre and post     imputation operations.</p> <p>Currently that can be:     1) a Narwhals Expr (NarwhalsExpression)         which is anything you can put in nw.from_native(df).with_columns()     2) a python function handle and parameters         which allows you to call an arbitrary function</p>"},{"location":"api/srmi/#survey_kit.imputation.variable.Variable.PrePost.Function","title":"Function","text":""},{"location":"api/srmi/#survey_kit.imputation.variable.Variable.PrePost.NarwhalsExpression","title":"NarwhalsExpression","text":"<p>               Bases: <code>Serializable</code></p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters","title":"Parameters","text":"<p>Factory class for creating parameter dictionaries for different imputation methods.</p> <p>Provides static methods to generate properly formatted parameter dictionaries with validation and default values for each imputation approach.</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.ErrorDraw","title":"ErrorDraw","text":"<p>               Bases: <code>Enum</code></p> <p>Random = 0 pmm = 1</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.RegressionModel","title":"RegressionModel","text":"<p>               Bases: <code>Enum</code></p> <p>OLS = 0</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.RegressionModel--probit-1","title":"Probit = 1","text":"<p>Logit = 2</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.RegressionModel--twosampleregression-3","title":"TwoSampleRegression = 3","text":""},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.HotDeck","title":"HotDeck  <code>staticmethod</code>","text":"<pre><code>HotDeck(\n    model_list: list[str] | list[list[str]] = None,\n    donate_list: list | None = None,\n    n_hotdeck_array: int = 3,\n    sequential_drop: bool = True,\n) -&gt; dict\n</code></pre> <p>Parameters for hot HotDeck imputation</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list[str] | list[list[str]]</code> <p>Each model is a list of variables that are used as match keys. model_list can either be a list of strings (the model itself) or it can be a list of lists of strings (sequential hot deck to match on)</p> <code>None</code> <code>donate_list</code> <code>list</code> <p>Additional variables to impute together, by default None I.e., you can predict earnings amount to find a donor, but then also impute hours worked and weeks worked with it.</p> <code>None</code> <code>n_hotdeck_array</code> <code>int</code> <p>Size of hot deck donor arrays, by default 3</p> <code>3</code> <code>sequential_drop</code> <code>bool</code> <p>Drop variables sequentially until matches found, by default True     If model_list is a list of strings (one model), should we     sequentially drop the last variable until all recipients find     a donor?  Makes it easier to set the hot deck/stat match up.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Hot deck parameter dictionary</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.LightGBM","title":"LightGBM  <code>staticmethod</code>","text":"<pre><code>LightGBM(\n    tune: bool = False,\n    tune_overwrite: bool = False,\n    parameters: dict | None = None,\n    tuner=None,\n    tune_hyperparameter_path: str = \"\",\n    quantiles: list = None,\n    error: ErrorDraw = ErrorDraw.pmm,\n    parameters_pmm: dict | None = None,\n) -&gt; dict\n</code></pre> <p>Parameters for LightGBM-based imputation.</p> <p>Parameters:</p> Name Type Description Default <code>tune</code> <code>bool</code> <p>Whether to tune hyperparameters, by default False</p> <code>False</code> <code>tune_overwrite</code> <code>bool</code> <p>Overwrite existing tuned parameters, by default False</p> <code>False</code> <code>parameters</code> <code>dict | None</code> <p>LightGBM model parameters, by default None From NEWS.CodeUtilities.Python.LightGBM</p> <code>None</code> <code>tuner</code> <code>Tuner</code> <p>Hyperparameter tuner object, by default None From NEWS.CodeUtilities.Python.LightGBM</p> <code>None</code> <code>tune_hyperparameter_path</code> <code>str</code> <p>Path for saving/loading tuned parameters, by default \"\"</p> <code>''</code> <code>quantiles</code> <code>list</code> <p>Quantiles for quantile regression, by default None</p> <code>None</code> <code>error</code> <code>ErrorDraw</code> <p>How to convert yhat from LGBM into imputes. If pmm, draw from nearest yhat neighbors, for example. The default is ErrorDraw.pmm.</p> <code>pmm</code> <code>parameters_pmm</code> <code>dict | None</code> <p>PMM parameters if using PMM error drawing, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>LightGBM parameter dictionary</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If quantiles are not between 0 and 1</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.NearestNeighbor","title":"NearestNeighbor  <code>staticmethod</code>","text":"<pre><code>NearestNeighbor(\n    match_to: str | list[str], parameters_pmm: dict = None\n) -&gt; dict\n</code></pre> <p>Match directly to an x variable (or set) using nearest neighbor matching</p> <p>Parameters:</p> Name Type Description Default <code>match_to</code> <code>str | list[str]</code> <p>Variable or list to match to.</p> required <code>parameters_pmm</code> <code>dict</code> <p>Nearest neighbor match parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>nearest neighbor parameter dictionary</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.Regression","title":"Regression  <code>staticmethod</code>","text":"<pre><code>Regression(\n    model: RegressionModel = RegressionModel.OLS,\n    error: ErrorDraw = ErrorDraw.pmm,\n    random_share: float = 1.0,\n    parameters_pmm: dict = None,\n) -&gt; dict\n</code></pre> <p>Parameters for regression-based imputation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RegressionModel</code> <p>Type of regression model, by default RegressionModel.OLS</p> <code>OLS</code> <code>error</code> <code>ErrorDraw</code> <p>Method for drawing errors, by default ErrorDraw.Random If pmm, draw from nearest yhat neighbors, for example. If Random, draw from observed errors for modeled observations</p> <code>pmm</code> <code>random_share</code> <code>float</code> <p>Fraction of data to use for regression, by default 1.0 Use less memory by running the regression on a random subset?</p> <code>1.0</code> <code>parameters_pmm</code> <code>dict</code> <p>PMM parameters if using PMM error drawing, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Regression parameter dictionary</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.StatMatch","title":"StatMatch  <code>staticmethod</code>","text":"<pre><code>StatMatch(\n    model_list: list = None,\n    donate_list: list = None,\n    sequential_drop: bool = False,\n)\n</code></pre> <p>Parameters for hot statistical match imputation</p> <p>Stat match and hot deck are basically the same, but the hot deck     iterates over the data carrying arrays of possible donor values     whereas the stat match just does a join of donors and recipients</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list[str] | list[list[str]]</code> <p>Each model is a list of variables that are used as match keys. model_list can either be a list of strings (the model itself) or it can be a list of lists of strings (sequential hot deck to match on)</p> <code>None</code> <code>donate_list</code> <code>list</code> <p>Additional variables to impute together, by default None I.e., you can predict earnings amount to find a donor, but then also impute hours worked and weeks worked with it.</p> <code>None</code> <code>sequential_drop</code> <code>bool</code> <p>Drop variables sequentially until matches found, by default False     If model_list is a list of strings (one model), should we     sequentially drop the last variable until all recipients find     a donor?  Makes it easier to set the hot deck/stat match up.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>stat match parameter dictionary</p>"},{"location":"api/srmi/#survey_kit.imputation.parameters.Parameters.pmm","title":"pmm  <code>staticmethod</code>","text":"<pre><code>pmm(\n    knearest: int = 10,\n    model: RegressionModel = RegressionModel.OLS,\n    donate_list: list[str] = None,\n    winsor: tuple[float, float] = [0, 1],\n    share_leave_out: float = 0.0,\n    donate_by: list[str] | str | None = None,\n) -&gt; dict\n</code></pre> <p>Parameters for predictive mean matching imputation.</p> <p>Parameters:</p> Name Type Description Default <code>knearest</code> <code>int</code> <p>Number of nearest neighbors for matching, by default 10</p> <code>10</code> <code>model</code> <code>RegressionModel</code> <p>Regression model type, by default RegressionModel.OLS</p> <code>OLS</code> <code>donate_list</code> <code>list[str]</code> <p>Additional variables to impute together, by default None i.e., you can predict earnings amount to find a donor, but then also impute hours worked and weeks worked with it.</p> <code>None</code> <code>winsor</code> <code>tuple[float, float]</code> <p>Winsorization percentiles, by default [0, 1]</p> <code>[0, 1]</code> <code>share_leave_out</code> <code>float</code> <p>Leave out a share to adjust the predictions to make sure in a random sample they match the training sample yhat distribution (potentially relevant for LightGBM), by default 0.0</p> <code>0.0</code> <code>donate_by</code> <code>list[str] | str | None</code> <p>Grouping variables for donation, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>PMM parameter dictionary</p>"},{"location":"api/srmi/#survey_kit.imputation.selection.Selection","title":"Selection","text":"<p>               Bases: <code>Serializable</code></p> <p>Handles variable selection for imputation models.</p> <p>Supports various selection methods including LASSO, stepwise selection, and custom functions to reduce model dimensionality.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Method</code> <p>Selection method to use, by default Method.No</p> <code>No</code> <code>parameters</code> <code>dict</code> <p>Method-specific parameters, by default None</p> <code>None</code> <code>select_within_by</code> <code>bool</code> <p>Whether to run selection within each by-group, by default True</p> <code>True</code> <code>function</code> <code>callable</code> <p>Custom selection function, by default None</p> <code>None</code>"},{"location":"api/srmi/#survey_kit.imputation.selection.Selection.Parameters","title":"Parameters","text":""},{"location":"api/srmi/#survey_kit.imputation.selection.Selection.Parameters.lasso","title":"lasso","text":"<pre><code>lasso(\n    nfolds: int = 5,\n    type_measure: str = \"default\",\n    include_base_with_interaction: bool = True,\n    winsorize: tuple[float, float] | None = None,\n    continuous: bool = False,\n    binomial: bool = False,\n    missing_dummies: bool = True,\n    optimal_lambda: float = None,\n    optimal_lambda_from_pre: bool = True,\n    scale_lambda: float = 1.0,\n) -&gt; dict\n</code></pre> <p>Parameters for LASSO variable selection method.</p> <p>Parameters:</p> Name Type Description Default <code>nfolds</code> <code>int</code> <p>Number of folds for cross-validation during LASSO regression. Used to determine optimal lambda value through k-fold cross-validation.</p> <code>5</code> <code>type_measure</code> <code>str</code> <p>Type of measure to use for cross-validation error. Determines how model performance is evaluated during lambda selection. Options are \"default\", \"mse\", \"deviance\", \"class\", \"auc\", \"mae\".</p> <code>\"default\"</code> <code>include_base_with_interaction</code> <code>bool</code> <p>Whether to include base variables when interaction terms are selected. If True, base variables are automatically included when their interactions are selected by LASSO.</p> <code>True</code> <code>winsorize</code> <code>tuple[float, float] or None</code> <p>Percentile bounds for winsorizing the dependent variable. Tuple of (low_percentile, high_percentile) to cap extreme values. None means no winsorization.</p> <code>None</code> <code>continuous</code> <code>bool</code> <p>Force treatment of dependent variable as continuous, overriding automatic detection.</p> <code>False</code> <code>binomial</code> <code>bool</code> <p>Force treatment of dependent variable as binomial, overriding automatic detection.</p> <code>False</code> <code>missing_dummies</code> <code>bool</code> <p>Whether to create dummy variables for missing values in predictor variables.</p> <code>True</code> <code>optimal_lambda</code> <code>float or None</code> <p>Pre-specified optimal lambda value for LASSO regularization. If None, optimal lambda will be determined through cross-validation.</p> <code>None</code> <code>optimal_lambda_from_pre</code> <code>bool</code> <p>Whether to use optimal lambda from a previous preselection step.</p> <code>True</code> <code>scale_lambda</code> <code>float</code> <p>Scaling factor applied to the optimal lambda value. Values &gt; 1 make regularization stronger (fewer variables selected), values &lt; 1 make it weaker (more variables selected).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing all parameter values for LASSO selection.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>When type_measure is not one of the acceptable values.</p>"},{"location":"api/srmi/#survey_kit.imputation.selection.Selection.Parameters.stepwise","title":"stepwise","text":"<pre><code>stepwise(\n    nfolds: int = 5,\n    scoring: str = \"neg_mean_squared_error\",\n    include_base_with_interaction: bool = True,\n    winsorize: tuple[float, float] | None = None,\n    missing_dummies: bool = True,\n    min_features_to_select: int = 10,\n)\n</code></pre> <p>Parameters for stepwise variable selection method using Recursive Feature Elimination with Cross-Validation (RFECV).</p> <p>Parameters:</p> Name Type Description Default <code>nfolds</code> <code>int</code> <p>Number of folds for cross-validation during stepwise selection. Used in RFECV to evaluate feature importance.</p> <code>5</code> <code>scoring</code> <code>str</code> <p>Scoring metric used to evaluate model performance during cross-validation. Should be a valid sklearn scoring parameter.</p> <code>\"neg_mean_squared_error\"</code> <code>include_base_with_interaction</code> <code>bool</code> <p>Whether to include base variables when interaction terms are selected. If True, base variables are automatically included when their interactions are selected.</p> <code>True</code> <code>winsorize</code> <code>tuple[float, float] or None</code> <p>Percentile bounds for winsorizing the dependent variable. Tuple of (low_percentile, high_percentile) to cap extreme values. None means no winsorization.</p> <code>None</code> <code>missing_dummies</code> <code>bool</code> <p>Whether to create dummy variables for missing values in predictor variables.</p> <code>True</code> <code>min_features_to_select</code> <code>int</code> <p>Minimum number of features that must be selected by the stepwise procedure. Prevents over-reduction of the feature set.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing all parameter values for stepwise selection.</p>"},{"location":"user-guide/calibration/","title":"Calibration","text":""},{"location":"user-guide/calibration/#what-is-it","title":"What Is It","text":"<p>Calibration (often implemented via raking) is used to adjust weights in a sample to make them representative of some other population of interest.  That can be for a survey that's meant to represent the population of a place or a treatment sample that should be representative of some larger group or control group.</p> <p>The basic idea is to define some set of \"moments\" that you want your sample to match (such as population controls, share of individuals with certain education levels, share with income in certain bins, etc.) and then reweight your sample so that it matches each of those moments.  See the Accelerated Entropy Balancing repository for more details on how it works.</p>"},{"location":"user-guide/calibration/#why-use-it","title":"Why Use It","text":"<p>Calibration can help:</p> <ul> <li>Address frame bias - A group is overrepresented in the frame, such as if we accidentally oversampled a group with a certain characteristic (e.g., high income households)</li> <li>Address nonresponse bias - Different groups respond at different rates and the results would be biased without accounting for it, as in this example (or any critique of polling, maybe)</li> <li>Increase precision - By using auxiliary information to reduce variance, see Little and Vartivarian (2005)</li> <li>For causal estimates when comparing treatment and control groups - Reweight a group (treatment) to match the characteristics of a different group (control), see Hainmueller (2012)</li> </ul>"},{"location":"user-guide/calibration/#implementation","title":"Implementation","text":"<p>This package uses Carl Sanders's Accelerated Entropy Balancing package to implement calibration via entropy balancing. This implementation is both faster and more robust (converges reliably or can produce \"best possible\" weights when exact convergence isn't achievable) than other available tools (at least in my anecdotal experience).</p> <p>Key advantages:</p> <ol> <li>Handles large datasets efficiently</li> <li>Robust convergence even with challenging constraints.</li> <li>Supports bounded weights for practical applications where convergence isn't possible (i.e. slightly conflicting constraints)</li> </ol> <p>Advantages 2. and 3. can be incredibly important in practice.  Many surveys weight to state x race x age x gender cells (or something like that).  Let's say we have 3 race/ethnicity groups and 5 age bins, that's 1,530 moment constraints (51 (50 states + DC) x 3 x 5 x 2).  If you start adding in other things (race x income bins, or race x county, or income x county), the number of moments can grow even larger.  </p> <p>Plus, if you're doing replicate weights, you have to repeat this many times (for example, 160 times in the CPS ASEC).  We've found other tools to be impractically slow at scale and/or to have convergence issues (i.e. it just doesn't work and you don't get a weight at the end).</p>"},{"location":"user-guide/calibration/#api","title":"API","text":"<p>See the full Calibration API documentation </p>"},{"location":"user-guide/calibration/#exampletutorial","title":"Example/Tutorial","text":"CodeLog <pre><code>from pathlib import Path\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.formula_builder import FormulaBuilder\nfrom survey_kit.calibration.moment import Moment\nfrom survey_kit.calibration.calibration import Calibration\nfrom survey_kit.utilities.dataframe import summary\nimport narwhals as nw\nfrom survey_kit import logger\n\n# %%\nlogger.info(\"Generating data for weighting\")\nn_rows = 100_000\ndf_population = (\n    RandomData(n_rows=n_rows, seed=12332151)\n    .index(\"index\")\n    .integer(\"v_1\", 1, 10)\n    .np_distribution(\"v_f_continuous_0\", \"normal\", loc=10, scale=2)\n    .np_distribution(\"v_f_continuous_1\", \"normal\", loc=10, scale=2)\n    .np_distribution(\"v_f_continuous_2\", \"normal\", loc=10, scale=2)\n    .float(\"v_extra\", -1, 2)\n    .np_distribution(\"weight_0\", \"normal\", loc=10, scale=1)\n    .np_distribution(\"weight_1\", \"normal\", loc=10, scale=1)\n    .integer(\"year\", 2016, 2021)\n    .integer(\"month\", 1, 12)\n    .to_df()\n    .lazy()\n)\n\ndf_treatment = (\n    RandomData(n_rows=n_rows, seed=894654)\n    .index(\"index\")\n    .integer(\"v_1\", 1, 10)\n    #   Intentionally set the loc/scale as different than above\n    .np_distribution(\"v_f_continuous_0\", \"normal\", loc=11, scale=4)\n    .np_distribution(\"v_f_continuous_1\", \"normal\", loc=11, scale=4)\n    .np_distribution(\"v_f_continuous_2\", \"normal\", loc=11, scale=4)\n    .float(\"v_extra\", -1, 2)\n    .np_distribution(\"weight_0\", \"normal\", loc=10, scale=1)\n    .np_distribution(\"weight_1\", \"normal\", loc=10, scale=1)\n    .integer(\"year\", 2016, 2021)\n    .integer(\"month\", 1, 12)\n    .to_df()\n    .lazy()\n)\n\n# print(df.describe())\n\n# %%\nlogger.info(\"Weighting 'function'\")\nf = FormulaBuilder(df=df_population, constant=False)\nf.continuous(columns=[\"v_1\", \"v_f_continuous_*\", \"v_f_p2_*\"])\n#   f.simple_interaction(columns=[\"v_1\",\"v_f_continuous_0\"])\n\nlogger.info(\"Define the target moments that the weighting will match\")\nlogger.info(\"   This can be a dataset or a single row of pop controls\")\nm = Moment(\n    df=df_population,\n    formula=f.formula,\n    weight=\"weight_0\",\n    index=\"index\",\n    #    by=[\"year\"],\n    rescale=True,\n)\n\nlogger.info(\"You can save/reload moments if you want\")\n# m.save(\"/my/path/moment\")\n# m_loaded = Moment.load(\"/my/path/moment\")\n\n# %%\n#   Calibrate the data in df_treatment to the moment above\nc = Calibration(\n    df=df_treatment,\n    moments=m,\n    weight=\"weight_1\",\n    final_weight=\"weight_final\"\n)\n\nc.run(\n    #   Drop a moment if there are too few observations\n    min_obs=5,\n    # If it fails to converge, set bounds on the weights \n    #   final weights = (base*ratio) where the bounds are on the ratio\n    #   for \"best possible\" weights   \n    bounds=(0.001, 1000)\n)\n\n#   Merge the final weights back on the treatment data\ndf_treatment = c.get_final_weights(df_treatment)\n\n# %%\nlogger.info(\"'Population' estimates\")\n_ = summary(df_population,\n        weight=\"weight_0\")\n\n# %%\nlogger.info(\"\\n\\n'Treatment', original weights\")\n_ = summary(df_treatment,\n        weight=\"weight_1\")\n\n# %%\nlogger.info(\"\\n\\n'Treatment', calibrated\")\n_ = summary(df_treatment,\n        weight=\"weight_final\")\n</code></pre> <p>View in separate window </p>"},{"location":"user-guide/srmi/","title":"Imputation/SRMI","text":""},{"location":"user-guide/srmi/#what-is-it","title":"What Is It","text":"<p>Suppose you have some variable (\\(y\\)) that has missing observations (some people didn't respond to the survey or data was left blank in the administrative record, etc.).  You also have some set of characteristics that are observable, with no missing information (\\(X\\)).  The basic idea of imputation is to estimate the distribution of \\(y\\) conditional on \\(X\\), or \\(f(y|X)\\) and then use that estimate to draw plausible values of \\(y\\) for each missing observation so that you can get an estimate of some statistic of \\(Q(y)\\) (the mean, the median, a regression coefficient that conditions on \\(y\\), ...).  The imputation model must be \"congenial\" to estimate a given statistic (or \"proper\"), which means the imputes should be drawn such that \\(Q(\\hat{y})\\) is unbiased and the estimates have valid confidence intervals (i.e. uncertainty is accounted for correctly - see van Buren, 2018 for a much more accurate and thorough discussion).   </p> <p>When actually doing the imputation you have several choices:</p> <ol> <li>How will you estimate \\(f(y|X)\\)? - What functional form will you use and how will you estimate the parameters?  A regression, a hot deck, machine learning?  Embedded in this is what do you include in \\(X\\).</li> <li>How will you draw values of \\(y\\) given your estimate of \\(f(y|X)\\)? - draw from the error distribution from your regression (\\(\\hat{e}\\)), use the emprirical distribution of observed values with the same/similar expected values (\\(\\hat{y}\\)) as in predicted mean matching or a hot deck, etc.</li> <li>How will you account for the relationship between imputed variables in your model? Suppose you have several variables with missing values, such that you want to estimate \\(f(y_1|X, y_2)\\) and \\(f(y_2|X,y_1)\\), how will you account for the relationship between \\(y_1\\) and \\(y_2\\)?  This is what Sequential Regression Multivariate Imputation (SRMI) is for.  You iteratively run the estimation, where in the first iteration \\(^{(1)}\\), you estimate \\(f(y_1^{(1)}|X)\\) (ignoring the relationship between \\(y_1\\) and \\(y_2\\)), then impute \\(f(y_2^{(1)}|X,y_1^{(1)})\\) (not ignoring the relationship between \\(y_2\\) and \\(y_1\\)).  Then, in the second iteration \\(^{(2)}\\), you estimate \\(f(y_1^{(2)}|X,y_2^{(1)})\\), with the imputed \\(y_2\\) from the prior iteration to incorporate the relationship between the two \\(y\\) variables.  You then re-impute \\(y_2^{(2)}\\) with the newly imputed values for \\(y_1^{(2)}\\).  You continue with additional iterations (up to \\(k\\), for example) until you have converged to the covariate distributions of \\(f(y_1^{(k)}|X,y_2^{(k-1)})\\)=\\(f(y_1|X,y_2)\\) and \\(f(y_2^{(k)}|X,y_1^{(k)})\\)=\\(f(y_2|X,y_1)\\).  Many imputations (such as most done at the U.S. Census Bureau) do not do this and stop after the first iteration, where \\(f(y_1^{(1)}|X)\\) was estimated without conditioning on \\(y_2\\) (or other variables with missing information). </li> <li>How do account for uncertainty?  You should use multiple imputation (Rubin, 1987), which means that you take some number of independent draws of \\(y\\) for each missing observation.  This allows you to account for uncertainty in \\(f(y|X)\\).  However, most estimation methods involve estimating \\(f(y|X,\\theta)\\), where \\(\\theta\\) could be the regression coefficients from an OLS regression.  Since you are estimating \\(\\hat{\\theta}\\), the estimates have uncertainty as well, which should be incorporated into the imputation.  This package handles this through taking a Bayes Bootstrap of the sample at each step of the estimation.</li> </ol>"},{"location":"user-guide/srmi/#why-use-it","title":"Why Use It","text":"<p>This package was built to handle production-scale multiple imputation for complex survey data. It provides capabilities that go beyond typical imputation packages:</p> <ul> <li> <p>Speed and Scale - Built on polars to handle large datasets efficiently. Impute datasets with hundreds of thousands or millions of observations on a standard laptop in minutes rather than hours.  You can pass data in any form supported by Narwhals and you'll get your data back in that form (Pandas, Polars, DuckDB, PyArrow, etc.).  We just use polars under the hood for speed.</p> </li> <li> <p>Production-Grade Reliability - Designed for long-running imputation pipelines with managed state and checkpointing. If your imputation is interrupted (power outage, server maintenance, hitting time limits,  you want to use your laptop to play a game), you can resume exactly where you left off without losing work.</p> </li> <li> <p>Flexible Method Mixing - Different variables need different approaches. Impute demographic variables via hot deck, income with gradient boosting, and binary indicators with PMM\u2014all in one coherent framework with proper iteration.</p> </li> <li> <p>Complex Dependency Handling - Real survey data has intricate relationships. For example, you may want to:</p> <ul> <li>Impute an earnings flag first, then earnings source conditional on that flag (primary earnings from wage and salary, self-employment, or farm self-employment)</li> <li>Impute by subgroups (wage earners, self-employed, farm self-employed) based on previously imputed variables</li> <li>Impute spouse 1's income using spouse 2's, then vice versa, then recalculate household totals to use to impute other variables (like interest income, pensions, etc.)</li> </ul> <p>This package handles this naturally without requiring you to reshape your data or run totally separate imputation models (which would make it impossible to use SRMI properly).</p> </li> <li> <p>Arbitrary Pre/Post Processing - Execute custom functions at any point in the imputation sequence. Recalculate derived variables, apply business rules, update relationships\u2014- whatever your data requires. Keep your data in its natural structure (person-level, household-level) and use the flexibility of the package make that complexity easy to manage (it's easy to write a function that updates spousal earnings and just call that when you need it).</p> </li> <li> <p>Modern Methods - Native support for gradient boosting (LightGBM) with hyperparameter tuning, quantile regression for continuous variables, etc. Methods typically unavailable in other imputation packages.  </p> </li> </ul> <p>Other tools can be integrated into this package, but the default is LightGBM for its speed and accuracy (some other tools include XGBoost, Catboost, random forest).</p>"},{"location":"user-guide/srmi/#tested-at-scale","title":"Tested at Scale","text":"<p>This package integrates knowledge gained from years of research at the U.S. Census Bureau:</p> <ul> <li>Income imputation in the Current Population Survey's Annual Social and Economic Supplement</li> <li>Analysis of administrative data from the Supplemental Nutrition Assistance Program </li> <li>Development of the National Experimental Well-being Statistics Project</li> </ul> <p>It was designed to handle imputing hundreds of variables across multiple iterations (SRMI) with samples ranging from hundreds of thousands to hundreds of millions of records, with complex dependency structures between variables.</p>"},{"location":"user-guide/srmi/#when-to-use-this-vs-other-packages","title":"When to Use This vs. Other Packages","text":"<p>Use this package when you:</p> <ul> <li>Work with large datasets (&gt;100K rows) where performance matters</li> <li>Need production reliability with checkpoint/resume capability</li> <li>Want to mix different imputation methods intelligently</li> <li>Have complex variable dependencies that require custom logic</li> <li>Need modern ML approaches (gradient boosting, quantile regression)</li> <li>Require hot deck or statistical matching methods</li> </ul> <p>Use mice or similar packages when you:</p> <ul> <li>Work with smaller datasets (&lt;100K rows)</li> <li>Want extensive built-in convergence diagnostics</li> <li>Just want something that works and you can cite</li> <li>Want a simpler API with simpler defaults</li> </ul> <p>Note: This package assumes familiarity with imputation methodology. It provides powerful, flexible tools for implementing complex imputation strategies correctly at scale. If you need a point-and-click solution with extensive guardrails, traditional packages may be more appropriate.</p>"},{"location":"user-guide/srmi/#api","title":"API","text":"<p>See the full Imputation/SRMI API documentation </p>"},{"location":"user-guide/srmi/#examplestutorials","title":"Examples/Tutorials","text":"Hot Deck/Statistical MatchRegressionMachine Learning <p>Stat match uses a join and hot deck fill forward from an array across the file, but there is no real difference between them theoretically </p> CodeLog <pre><code>import sys\nimport os\nfrom pathlib import Path\n\nimport narwhals as nw\nimport polars as pl\nimport polars.selectors as cs\n\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary\n\nfrom survey_kit.imputation.variable import Variable\nfrom survey_kit.imputation.parameters import Parameters\nfrom survey_kit.imputation.srmi import SRMI\nfrom survey_kit.orchestration.config import Config\n\nfrom survey_kit import logger, config\nfrom survey_kit.utilities.dataframe import summary, columns_from_list\n\n\n\n# %%\n# Draw some random data\n\nn_rows = 10_000\nimpute_share = 0.25\n\ndf = (\n    RandomData(n_rows=n_rows, seed=32565437)\n    .index(\"index\")\n    .integer(\"year\", 2016, 2020)\n    .integer(\"month\", 1, 12)\n    .integer(\"var2\", 0, 10)\n    .integer(\"var3\", 0, 50)\n    .float(\"var4\", 0, 1)\n    .integer(\"var5\", 0, 1)\n    .np_distribution(\"epsilon_hd1\", \"normal\", scale=5)\n    .np_distribution(\"epsilon_hd2\", \"normal\", scale=5)\n    .float(\"missing_hd1\", 0, 1)\n    .float(\"missing_hd2\", 0, 1)\n    .to_df()\n)\n\n\n#   Convenience references to them for creating dependent variables\nc_var2 = pl.col(\"var2\")\nc_var3 = pl.col(\"var3\")\nc_var4 = pl.col(\"var4\")\nc_var5 = pl.col(\"var5\")\n\nc_e_hd1 = pl.col(\"epsilon_hd1\")\nc_e_hd2 = pl.col(\"epsilon_hd2\")\n\n\nlogger.info(\"var_hd1 is binary and conditional on other variables\")\nc_hd1 = ((c_var2 * 2 - c_var3 * 3 * c_var5 + c_e_hd1)  &gt; 0).alias(\"var_hd1\")\n\nlogger.info(\"var_hd2 is != 0 only if var_hd1 == True\")\nc_hd2 = (\n    pl.when(pl.col(\"var_hd1\"))\n      .then(((c_var2 * 1.5 - c_var3 * 1 * c_var4 + c_e_hd2)))\n      .otherwise(pl.lit(0))\n      .alias(\"var_hd2\")\n)\n#   Create a bunch of variables that are functions of the variables created above\ndf = (\n    df.with_columns(c_hd1)\n    .with_columns(c_hd2)\n    .drop(columns_from_list(df=df, columns=\"epsilon*\"))\n    .with_row_index(name=\"_row_index_\")\n)\ndf_original = df\n\n#   Set variables to missing according to the uniform random variables missing_\nclear_missing = []\nfor prefixi in [\"hd\"]:\n    for i in range(1, 3):\n        vari = f\"var_{prefixi}{i}\"\n        missingi = f\"missing_{prefixi}{i}\"\n\n        clear_missing.append(\n            pl.when(pl.col(missingi) &lt; impute_share)\n            .then(pl.lit(None))\n            .otherwise(pl.col(vari))\n            .alias(vari)\n        )\ndf = df.with_columns(clear_missing).drop(cs.starts_with(\"missing_\"))\n\nsummary(df)\n\n\n#   Actually do the imputation\n\n#       The list of variables to impute (eventually)\nvars_impute = []\n\n\n#   1) Impute some variables to impute using stat match/hot deck\nmodeltype = Variable.ModelType.StatMatch\nmodeltype_binary = Variable.ModelType.HotDeck\n\n#       Hot deck a continuous variable\n#           Each model has a set of possible parameters\n#           that determine what happens in the model\nparameters_hd1 = Parameters.HotDeck(\n    #   model_list - a list of variables to match\n    #       donors and recipients\n    model_list=[\"var2\", \"var3\", \"var5\"],\n    #   Donate anything other than the variable\n    #       (i.e. donate together)\n    #       In this case, it's redundant and does nothing...\n    donate_list=[\"var_hd1\"],\n)\n\n# %%\n# Set up the variable to be imputed\n\nlogger.info(\"Impute the boolean variable (var_hd1)\")\nlogger.info(\"   by setting the model type (a stat match)\")\nlogger.info(\"   and the list of match variables\")\nv_hd1 = Variable(\n    impute_var=\"var_hd1\",\n    modeltype=Variable.ModelType.StatMatch,\n    parameters=Parameters.HotDeck(\n        model_list=[\"var2\", \"var3\", \"var5\"]\n    )\n)\n\nlogger.info(\"Add the variable to the list to be imputed\")\nvars_impute.append(v_hd1)\n\n\nlogger.info(\"Impute the continuous variable (var_hd2) \")\nlogger.info(\"   conditional on var_hd1, using narwhals (nw.col('var_hd1'))\")\nlogger.info(\"   by setting the model type (a hot deck)\")\nlogger.info(\"   and the list of match variables\")\nlogger.info(\"   as well as a post-processing edit to set var_hd2=0 when var_hd1==0\")\n\nv_hd2 = Variable(\n    impute_var=\"var_hd2\",\n    Where=nw.col(\"var_hd1\"),\n    By=[\"year\", \"month\"],\n    modeltype=Variable.ModelType.HotDeck,\n    parameters=Parameters.HotDeck(\n        model_list=[\"var2\", \"var3\", \"var5\"]\n    ),\n    postFunctions=(\n        nw.when(nw.col(\"var_hd1\"))\n          .then(nw.col(\"var_hd2\"))\n          .otherwise(nw.lit(0))\n          .alias(\"var_hd2\")\n    )\n)\nvars_impute.append(v_hd2)\n\n\n# %%\nlogger.info(\"Set up the imputation\")\nsrmi = SRMI(\n    df=df,\n    variables=vars_impute,\n    n_implicates=2,\n    n_iterations=1,\n    parallel=False,\n    bayesian_bootstrap=True,\n    parallel_testing=False,\n    path_model=f\"{config.path_temp_files}/py_srmi_test_hd\",\n    force_start=True,\n)\n\n# %%\nlogger.info(\"Run it\")\nsrmi.run()\n\n# %%\nlogger.info(\"Get the results\")\n_ = df_list = srmi.df_implicates\n\n# %%\nlogger.info(\"\\n\\nLook at the original\")\n_ = summary(df_original)\n\nlogger.info(\"\\n\\nLook at the imputes\")\n_ = df_list.pipe(summary)\n\nlogger.info(\"\\n\\nLook at the imputes | var_hd1 == 0\")\n_ = df_list.filter(~nw.col(\"var_hd1\")).pipe(summary)\n\nlogger.info(\"\\n\\nLook at the imputes | var_hd1 == 1\")\n_ = df_list.filter(nw.col(\"var_hd1\")).pipe(summary)\n</code></pre> <p>View in separate window </p> <p>Logit and/or OLS-based imputation</p> CodeLog <pre><code>import sys\nimport os\nfrom pathlib import Path\n\nimport narwhals as nw\nimport polars as pl\nimport polars.selectors as cs\n\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary\n\nfrom survey_kit.imputation.variable import Variable\nfrom survey_kit.imputation.parameters import Parameters\nfrom survey_kit.imputation.srmi import SRMI\nfrom survey_kit.imputation.selection import Selection\n\nfrom survey_kit import logger, config\nfrom survey_kit.utilities.dataframe import summary, columns_from_list\nfrom survey_kit.utilities.formula_builder import FormulaBuilder\n\n\npath = Path(config.code_root)\nsys.path.append(os.path.normpath(path.parent.parent / \"tests\"))\nfrom scratch import path_scratch\n\n\nconfig.data_root = path_scratch(temp_file_suffix=False)\n\n\n# %%\n# Draw some random data\n\nn_rows = 10_000\nimpute_share = 0.25\n\n\ndf = (\n    RandomData(n_rows=n_rows, seed=32565437)\n    .index(\"index\")\n    .integer(\"year\", 2016, 2020)\n    .integer(\"month\", 1, 12)\n    .integer(\"var2\", 0, 10)\n    .integer(\"var3\", 0, 50)\n    .float(\"var4\", 0, 1)\n    .integer(\"var5\", 0, 1)\n    .float(\"unrelated_1\", 0, 1)\n    .float(\"unrelated_2\", 0, 1)\n    .float(\"unrelated_3\", 0, 1)\n    .float(\"unrelated_4\", 0, 1)\n    .float(\"unrelated_5\", 0, 1)\n    .np_distribution(\"epsilon_reg1\", \"normal\", scale=5)\n    .np_distribution(\"epsilon_reg2\", \"normal\", scale=5)\n    .float(\"missing_reg1\", 0, 1)\n    .float(\"missing_reg2\", 0, 1)\n    .to_df()\n)\n\n\n#   Convenience references to them for creating dependent variables\nc_var2 = pl.col(\"var2\")\nc_var3 = pl.col(\"var3\")\nc_var4 = pl.col(\"var4\")\nc_var5 = pl.col(\"var5\")\n\nc_e_reg1 = pl.col(\"epsilon_reg1\")\nc_e_reg2 = pl.col(\"epsilon_reg2\")\n\n\n#   Convenience references to them for creating dependent variables\nc_var2 = pl.col(\"var2\")\nc_var3 = pl.col(\"var3\")\nc_var4 = pl.col(\"var4\")\nc_var5 = pl.col(\"var5\")\n\n\nlogger.info(\"var_reg1 is binary and conditional on other variables\")\nc_reg1 = ((c_var2 * 2 - c_var3 * 3 * c_var5 + c_e_reg1)  &gt; 0).alias(\"var_reg1\")\n\nlogger.info(\"var_reg2 is != 0 only if var_reg1 == True\")\nc_reg2 = (\n    pl.when(pl.col(\"var_reg1\"))\n      .then(((c_var2 * 1.5 - c_var3 * 1 * c_var4 + c_e_reg2)))\n      .otherwise(pl.lit(0))\n      .alias(\"var_reg2\")\n)\n#   Create a bunch of variables that are functions of the variables created above\ndf = (\n    df.with_columns(c_reg1)\n    .with_columns(c_reg2)\n    .drop(columns_from_list(df=df, columns=\"epsilon*\"))\n    .with_row_index(name=\"_row_index_\")\n)\n\ndf_original = df\n\n#   Set variables to missing according to the uniform random variables missing_\nclear_missing = []\nfor prefixi in [\"reg\"]:\n    for i in range(1, 3):\n        vari = f\"var_{prefixi}{i}\"\n        missingi = f\"missing_{prefixi}{i}\"\n\n        clear_missing.append(\n            pl.when(pl.col(missingi) &lt; impute_share)\n            .then(pl.lit(None))\n            .otherwise(pl.col(vari))\n            .alias(vari)\n        )\ndf = df.with_columns(clear_missing).drop(cs.starts_with(\"missing_\"))\n\n#   Make a fully collinear var for testing\ndf = df.with_columns(pl.col(\"unrelated_1\").alias(\"repeat_1\"))\n\n\nsummary(df)\n\n\n#   Actually do the imputation\n\n\n# %%\nlogger.info(\"Define the regression model (intentionally include some extraneous variables\")\n\nf_model = FormulaBuilder(df=df)\nf_model.formula_with_varnames_in_brackets(\n    \"~1+{var_*}+var2+var4+var4*var3*C(var5)+{unrelated_*}+{repeat_*}\"\n)\nlogger.info(f_model.formula)\n\n\n# %%\n# Set up the variable to be imputed\nvars_impute = []\n\nlogger.info(\"Impute the boolean variable (var_reg1)\")\nlogger.info(\"   to the default setup for predicted mean matching\")\nlogger.info(\"   using logit regression\")\nv_reg1 = Variable(\n    impute_var=\"var_reg1\",\n    modeltype=Variable.ModelType.pmm,\n    model=f_model.formula,\n    parameters=Parameters.Regression(model=Parameters.RegressionModel.Logit)\n)\nlogger.info(\"Add the variable to the list to be imputed\")\nvars_impute.append(v_reg1)\n\nlogger.info(\"Impute the continuous variable (var_reg2) \")\nlogger.info(\"   conditional on var_reg1, using narwhals (nw.col('var_reg1'))\")\nlogger.info(\"   by setting the model type\")\nlogger.info(\"   and the formula\")\nlogger.info(\"   as well as a post-processing edit to set var_reg2=0 when var_reg1==0\")\nv_reg2 = Variable(\n    impute_var=\"var_reg2\",\n    Where=nw.col(\"var_reg1\"),\n    modeltype=Variable.ModelType.pmm,\n    model=f_model.formula,\n    #   Default parameters\n    parameters=Parameters.Regression(),\n    postFunctions=(\n        nw.when(nw.col(\"var_reg1\"))\n          .then(nw.col(\"var_reg2\"))\n          .otherwise(nw.lit(0))\n          .alias(\"var_reg2\")\n    )\n)\n\nvars_impute.append(v_reg2)\n\n\n# %%\nlogger.info(\"Set up the imputation\")\nlogger.info(\"Add LASSO selection before each imputation\")\nsrmi = SRMI(\n    df=df,\n    variables=vars_impute,\n    n_implicates=2,\n    n_iterations=2,\n    parallel=False,\n    selection=Selection(method=Selection.Method.LASSO),\n    modeltype=Variable.ModelType.pmm,\n    model=f_model.formula,\n    bayesian_bootstrap=True,\n    path_model=f\"{config.path_temp_files}/py_srmi_test_regression\",\n    force_start=True,\n)\n\n# %%\nlogger.info(\"Run it\")\nsrmi.run()\n\n\n# %%\nlogger.info(\"Get the results\")\n_ = df_list = srmi.df_implicates\n\n# %%\nlogger.info(\"\\n\\nLook at the original\")\n_ = summary(df_original)\n\nlogger.info(\"\\n\\nLook at the imputes\")\n_ = df_list.pipe(summary)\n\nlogger.info(\"\\n\\nLook at the imputes | var_reg1 == 0\")\n_ = df_list.filter(~nw.col(\"var_reg1\")).pipe(summary)\n\nlogger.info(\"\\n\\nLook at the imputes | var_reg1 == 1\")\n_ = df_list.filter(nw.col(\"var_reg1\")).pipe(summary)\n</code></pre> <p>View in separate window </p> <p>Imputation with LightGBM, see the LightGBM documentation for additional information on some of the options.</p> CodeLog <pre><code>import sys\nimport os\nfrom pathlib import Path\n\nimport narwhals as nw\nimport polars as pl\nimport polars.selectors as cs\n\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary\n\nfrom survey_kit.imputation.variable import Variable\nfrom survey_kit.imputation.parameters import Parameters\nfrom survey_kit.imputation.srmi import SRMI\nfrom survey_kit.imputation.selection import Selection\nimport survey_kit.imputation.utilities.lightgbm_wrapper as rep_lgbm\nfrom survey_kit.imputation.utilities.lightgbm_wrapper import Tuner_optuna\n\nfrom survey_kit import logger, config\nfrom survey_kit.utilities.dataframe import summary, columns_from_list\n\n\n# %%\n# Draw some random data\n\nn_rows = 10_000\nimpute_share = 0.25\n\n\ndf = (\n    RandomData(n_rows=n_rows, seed=32565437)\n    .index(\"index\")\n    .integer(\"year\", 2016, 2020)\n    .integer(\"month\", 1, 12)\n    .integer(\"var2\", 0, 10)\n    .integer(\"var3\", 0, 50)\n    .float(\"var4\", 0, 1)\n    .integer(\"var5\", 0, 1)\n    .float(\"unrelated_1\", 0, 1)\n    .float(\"unrelated_2\", 0, 1)\n    .float(\"unrelated_3\", 0, 1)\n    .float(\"unrelated_4\", 0, 1)\n    .float(\"unrelated_5\", 0, 1)\n    .np_distribution(\"epsilon_gbm1\", \"normal\", scale=5)\n    .np_distribution(\"epsilon_gbm2\", \"normal\", scale=5)\n    .np_distribution(\"epsilon_gbm3\", \"normal\", scale=5)\n    .float(\"missing_gbm1\", 0, 1)\n    .float(\"missing_gbm2\", 0, 1)\n    .float(\"missing_gbm3\", 0, 1)\n    .to_df()\n)\n\n\n#   Convenience references to them for creating dependent variables\nc_var2 = pl.col(\"var2\")\nc_var3 = pl.col(\"var3\")\nc_var4 = pl.col(\"var4\")\nc_var5 = pl.col(\"var5\")\n\nc_e_gbm1 = pl.col(\"epsilon_gbm1\")\nc_e_gbm2 = pl.col(\"epsilon_gbm2\")\n\n\n#   Convenience references to them for creating dependent variables\nc_var2 = pl.col(\"var2\")\nc_var3 = pl.col(\"var3\")\nc_var4 = pl.col(\"var4\")\nc_var5 = pl.col(\"var5\")\n\n\nlogger.info(\"var_gbm1 is binary and conditional on other variables\")\nc_gbm1 = ((c_var2 * 2 - c_var3 * 3 * c_var5 + c_e_gbm1)  &gt; 0).alias(\"var_gbm1\")\n\nlogger.info(\"var_gbm2 is != 0 only if var_gbm1 == True\")\nc_gbm2 = (\n    pl.when(pl.col(\"var_gbm1\"))\n      .then(((c_var2 * 1.5 - c_var3 * 1 * c_var4 + c_e_gbm2)))\n      .otherwise(pl.lit(0))\n      .alias(\"var_gbm2\")\n)\n\nc_gbm3 = (\n    pl.when(pl.col(\"var_gbm1\"))\n      .then(((c_var2 * 1.5 - c_var3 * 1 * c_var4 + c_e_gbm2)))\n      .otherwise(pl.lit(0))\n      .alias(\"var_gbm3\")\n)\n#   Create a bunch of variables that are functions of the variables created above\ndf = (\n    df.with_columns(c_gbm1)\n    .with_columns(c_gbm2, c_gbm3)\n    .drop(columns_from_list(df=df, columns=\"epsilon*\"))\n    .with_row_index(name=\"_row_index_\")\n)\ndf_original = df\n\n#   Set variables to missing according to the uniform random variables missing_\nclear_missing = []\nfor prefixi in [\"gbm\"]:\n    for i in range(1, 4):\n        vari = f\"var_{prefixi}{i}\"\n        missingi = f\"missing_{prefixi}{i}\"\n\n        clear_missing.append(\n            pl.when(pl.col(missingi) &lt; impute_share)\n            .then(pl.lit(None))\n            .otherwise(pl.col(vari))\n            .alias(vari)\n        )\ndf = df.with_columns(clear_missing).drop(cs.starts_with(\"missing_\"))\n\n#   Make a fully collinear var for testing\ndf = df.with_columns(pl.col(\"unrelated_1\").alias(\"repeat_1\"))\n\n\nsummary(df)\n\n\n# %%\nlogger.info(\"Define some dummy functions to run after imputation of 2\")\n#   Test a simple pre-post function\n#       These would get run gets run in each iteration (in each implicate)\n#           before (preFunctions) or after (postFunctions) this variable is imputed\n#   Notes for these functions:\n#       1) No type hints on imported package types (will throw an error)\n#           i.e. no df:pl.DataFrame or -&gt; pl.DataFrame\n#       2) Must be completely self-contained (i.e. all imports within the function)\n#           This has to do with how it gets saved and loaded in async calls\n#       3) Effectively, you have to assume it'll be called\n#           in an environment with no imports before it\ndef square_var(df, var_to_square: str, name: str):\n    import narwhals as nw\n\n    return (\n        nw.from_native(df)\n        .with_columns((nw.col(var_to_square) ** 2).alias(name))\n        .to_native()\n    )\n\n\ndef recalculate_interaction(df, var1: str, var2: str, name: str):\n    import narwhals as nw\n\n    return (\n        nw.from_native(df)\n        .with_columns((nw.col(var1) * nw.col(var2)).alias(name))\n        .to_native()\n    )\n\n\n# %%\nlogger.info(\"Set up hyperparameter tuning\")\ntuner = Tuner_optuna(\n    n_trials=50, objective=rep_lgbm.Tuner.Objectives.mae, test_size=0.25\n)\n\nlogger.info(\"   Set the tuner parameters to the defaults\")\ntuner.parameters()\n\nlogger.info(\"   Then specify ranges to check between as follow\")\ntuner.hyperparameters[\"num_leaves\"] = [2, 256]\ntuner.hyperparameters[\"max_depth\"] = [2, 256]\ntuner.hyperparameters[\"min_data_in_leaf\"] = [10, 250]\ntuner.hyperparameters[\"num_iterations\"] = [25, 200]\ntuner.hyperparameters[\"bagging_fraction\"] = [0.5, 1]\ntuner.hyperparameters[\"bagging_freq\"] = [1, 5]\n\n\n\n\nvars_impute = []\n\n# %%\nlogger.info(\"Impute the boolean variable (var_gbm1)\")\nlogger.info(\"   to the default setup for predicted mean matching\")\nlogger.info(\"   using lightgbm\")\nlogger.info(\"   (you can pass a formula, but you don't need to)\")\n\nlogger.info(\"First, set up the lightgbm parameters\")\nlogger.info(\"   This says, do hyperparameter tuning first (tune)\")\nlogger.info(\"   Redo it at each run (tune_overwrite)\")\nlogger.info(\"   And sets the lightgbm parameter defaults (parameters) that the tuning can overwrite\")\nparameters_lgbm1 = Parameters.LightGBM(\n    tune=True,\n    tune_hyperparameter_path=f\"{config.data_root}/tuner_outputs\",\n    tuner=tuner,\n    tune_overwrite=True,\n    parameters={\n        \"objective\": \"binary\",\n        \"num_leaves\": 32,\n        \"min_data_in_leaf\": 20,\n        \"num_iterations\": 100,\n        \"test_size\": 0.2,\n        \"boosting\": \"gbdt\",\n        \"categorical_feature\": [\"var5\"],\n        \"verbose\": -1,  # ,\n    },\n    error=Parameters.ErrorDraw.pmm,\n)\n\n\nlogger.info(\"Actually define the variable and the model\")\nv_gbm1 = Variable(\n    impute_var=\"var_gbm1\",\n    model=[\"var_*\", \"var4\", \"var3\", \"var5\", \"unrelated_*\", \"repeat_*\"],\n    modeltype=Variable.ModelType.LightGBM,\n    parameters=parameters_lgbm1\n)\nlogger.info(\"Add the variable to the list to be imputed\")\nvars_impute.append(v_gbm1)\n\n\n\n\n\n\nlogger.info(\"Impute the continuous variable (var_gbm2) \")\nlogger.info(\"   conditional on var_gbm1, using narwhals (nw.col('var_gbm1'))\")\nlogger.info(\"   as well as a post-processing edit to set var_gbm2=0 when var_gbm1==0\")\nlogger.info(\"   and some other random post-processing\")\nlogger.info(\"Different parameters for the continuous variable\")\nparameters_lgbm2 = Parameters.LightGBM(\n    tune=True,\n    tune_hyperparameter_path=f\"{config.data_root}/tuner_outputs\",\n    tuner=tuner,\n    tune_overwrite=True,\n    parameters={\n        \"objective\": \"regression\",\n        \"num_leaves\": 32,\n        \"min_data_in_leaf\": 20,\n        \"num_iterations\": 100,\n        \"test_size\": 0.2,\n        \"boosting\": \"gbdt\",\n        \"categorical_feature\": [\"var5\"],\n        \"verbose\": -1,  # ,\n    },\n    error=Parameters.ErrorDraw.pmm,\n)\n\nv_gbm2 = Variable(\n    impute_var=\"var_gbm2\",\n    Where=nw.col(\"var_gbm1\"),\n    model=[\"var_*\", \"var4\", \"var3\", \"var5\", \"unrelated_*\", \"repeat_*\"],\n    modeltype=Variable.ModelType.LightGBM,\n    parameters=parameters_lgbm2,\n    postFunctions=[\n        (\n            nw.when(nw.col(\"var_gbm1\"))\n            .then(nw.col(\"var_gbm2\"))\n            .otherwise(nw.lit(0))\n            .alias(\"var_gbm2\")\n        ),\n        Variable.PrePost.Function(\n            recalculate_interaction,\n            parameters=dict(\n                var1=\"var_gbm1\",\n                var2=\"var_gbm2\",\n                name=\"var_gbm12\"\n            ),\n        ),\n        Variable.PrePost.Function(\n            square_var,\n            parameters=dict(\n                var_to_square=\"var_gbm2\", \n                name=\"var_gbm2_sq\"\n            )\n        ),\n    ]\n)\n\nvars_impute.append(v_gbm2)\n\n\n\n\nlogger.info(\"Now do one with the quantile-regression lightgbm\")\nlogger.info(\"   To do this, pass quantiles and set objective='quantile'\")\nparameters_lgbm3 = Parameters.LightGBM(\n    tune=True,\n    tune_hyperparameter_path=f\"{config.data_root}/tuner_outputs\",\n    tuner=tuner,\n    tune_overwrite=True,\n    quantiles=[0.25,0.5,0.75],\n    parameters={\n        \"objective\": \"quantile\",\n        \"num_leaves\": 32,\n        \"min_data_in_leaf\": 20,\n        \"num_iterations\": 100,\n        \"test_size\": 0.2,\n        \"boosting\": \"gbdt\",\n        \"categorical_feature\": [\"var5\"],\n        \"verbose\": -1,  # ,\n    },\n    error=Parameters.ErrorDraw.pmm,\n)\n\nv_gbm3 = Variable(\n    impute_var=\"var_gbm3\",\n    Where=nw.col(\"var_gbm1\"),\n    model=[\"var_*\", \"var4\", \"var3\", \"var5\", \"unrelated_*\", \"repeat_*\"],\n    modeltype=Variable.ModelType.LightGBM,\n    parameters=parameters_lgbm3\n)\n\nvars_impute.append(v_gbm3)\n\n\n# %%\nlogger.info(\"Set up the imputation\")\nsrmi = SRMI(\n    df=df,\n    variables=vars_impute,\n    n_implicates=2,\n    n_iterations=2,\n    parallel=False,\n    index=[\"index\"],\n    modeltype=Variable.ModelType.pmm,\n    bayesian_bootstrap=True,\n    path_model=f\"{config.path_temp_files}/py_srmi_test_gbm\",\n    force_start=True,\n)\n\n# %%\nlogger.info(\"Run it\")\nsrmi.run()\n\nlogger.info(\"It's automatically saved and can be loaded with (see path_model above):\")\nlogger.info(\"path_model = f'{config.path_temp_files}/py_srmi_test_gbm'\")\nlogger.info(\"srmi = SRMI.load(path_model)\")\n\n\n# %%\nlogger.info(\"Get the results\")\n_ = df_list = srmi.df_implicates\n\n# %%\nlogger.info(\"\\n\\nLook at the original\")\n_ = summary(df_original,detailed=True,drb_round=True)\n\nlogger.info(\"\\n\\nLook at the imputes\")\n_ = df_list.pipe(summary,detailed=True,drb_round=True)\n\nlogger.info(\"\\n\\nLook at the imputes | var_gbm1 == 0\")\n_ = df_list.filter(~nw.col(\"var_gbm1\")).pipe(summary,detailed=True,drb_round=True)\n\nlogger.info(\"\\n\\nLook at the imputes | var_gbm1 == 1\")\n_ = df_list.filter(nw.col(\"var_gbm1\")).pipe(summary,detailed=True,drb_round=True)\n</code></pre> <p>View in separate window </p>"},{"location":"user-guide/statistics/","title":"Statistics &amp; Standard Errors","text":""},{"location":"user-guide/statistics/#what-is-it","title":"What Is It","text":"<p>The Statistics module provides tools for calculating summary statistics with proper standard errors for complex survey data and multiple imputation. It handles three key sources of uncertainty:</p> <ul> <li>Sampling variance - From complex survey designs (stratification, clustering, unequal weights) with replicate weights (for examples, see CPS ASEC documentation, ACS documentation, Consumer Expenditure Survey documentation, SCF documentation, MEPS documentation, ...)</li> <li>Imputation uncertainty - When working with multiply imputed data</li> <li>Both combined - Full uncertainty accounting when you have imputed data with replicate weights</li> </ul> <p>The package supports: - Quick exploratory summaries with <code>summary()</code> - Replicate weight standard errors (Bootstrap and Balanced Repeated Replication) - Multiple imputation inference using Rubin's rules - Statistical comparisons with proper standard errors - Custom analysis functions with automatic variance estimation</p>"},{"location":"user-guide/statistics/#why-use-it","title":"Why Use It","text":"<p>Standard statistical software often gets uncertainty wrong:</p> <p>The problem: - Using design weights without replicate weights \u2192 Standard errors too small - Simple variance formulas with complex samples \u2192 Wrong confidence intervals - Ignoring imputation uncertainty \u2192 Overstated precision - Ad-hoc comparisons \u2192 Invalid hypothesis tests.  </p> <p>For example, replicate weights are intended to account for correlation over space and time in sample selection, so using them for comparisons will account for correlation in estimates across the different replicate factors.  To give a concrete example, addresses in the CPS are in sample for consecutive months, so if you want to get the correct confidence intervals on the change in unemployment, you need to use the replicate weights in the month-to-month comparison.  The standard errors will be much narrower than if you compared the unemployment rates as if they were two independent samples.</p> <p>This package: - Implements proper variance estimation for complex surveys (as used by Census Bureau, BLS, etc.) - Correctly combines uncertainty from multiple imputations - Makes it easy to do the right thing (comparisons, custom functions, etc.) - Fast enough for production use with millions of observations (assuming you use polars)</p>"},{"location":"user-guide/statistics/#key-features","title":"Key Features","text":"<ul> <li>DataFrame agnostic - Works with Polars, Pandas, Arrow, DuckDB via Narwhals</li> <li>Fast - Optimized for large datasets (100K+ rows, tested on millions)</li> <li>Flexible - Custom functions get variance estimation for free</li> <li>Parallel processing - Run imputations in parallel</li> <li>Proper inference - Implements Rubin's rules for MI, supports BRR/bootstrap for replicate weights</li> <li>Production-ready - Used in Census Bureau's National Experimental Wellbeing Statistics</li> </ul>"},{"location":"user-guide/statistics/#implementation-notes","title":"Implementation Notes","text":""},{"location":"user-guide/statistics/#replicate-weights","title":"Replicate Weights","text":"<p>Two variance estimation approaches: - Bootstrap (<code>bootstrap=True</code>) - Standard bootstrap resampling variance - BRR (<code>bootstrap=False</code>) - Balanced Repeated Replication as used by Census Bureau</p>"},{"location":"user-guide/statistics/#multiple-imputation","title":"Multiple Imputation","text":"<p>Implements Rubin's (1987) combining rules: - Combined estimate = average across implicates - Total variance = within-imputation variance + between-imputation variance - Degrees of freedom account for finite number of implicates - Missing information rate quantifies uncertainty from imputation</p>"},{"location":"user-guide/statistics/#when-to-use-what","title":"When to Use What","text":"Use Case Tool Why Quick exploration <code>summary()</code> Fast, no SEs needed Survey data with replicate weights <code>StatCalculator</code> + <code>Replicates</code> Proper complex survey variance Imputed data <code>mi_ses_from_function()</code> Accounts for imputation uncertainty Imputed survey data Both combined Full uncertainty (sampling + imputation) Custom analysis <code>StatCalculator.from_function()</code> Any function gets proper SEs"},{"location":"user-guide/statistics/#api","title":"API","text":"<p>See the Basic Statistics and Standard Errors documentation estimating summary stats and standard errors. See the Multiple Imputation documentation estimating statistics with multiply imputed data.</p>"},{"location":"user-guide/statistics/#exampletutorial","title":"Example/Tutorial","text":"Quick SummariesBootstrap/Replicate Standard ErrorsMultiple Imputation Standard Errors <p>The <code>summary()</code> function provides easy data summaries (with more control than the default <code>describe()</code> methods in pandas or polars)</p> CodeLog <pre><code>from survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary\nfrom survey_kit.statistics.statistics import Statistics\nfrom survey_kit import logger\n\n# %%\nlogger.info(\"Draw some random data\")\nn_rows = 1_000\ndf = (\n    RandomData(n_rows=n_rows, seed=12332151)\n    .index(\"index\")\n    .integer(\"v_int\", 0, 10)\n    .boolean(\"v_bool\")\n    .float(\"v_float\", -1, 1)\n    .integer(\"weight_0\", 100, 1_000_000)\n    .integer(\"year\", 2016, 2018)\n    .integer(\"quarter\", 1, 4)\n    .to_df()\n    .lazy()\n)\n\n# %%\nlogger.info(\"The simplest option: just call summary(df)\")\nlogger.info(\"  Note the '_ =' is to prevent it from being printed twice in jupyter when generating the html, you can ignore\")\n_ = summary(df)\n\n# %%\nlogger.info(\"\\n\\n + Weighted\")\n_ = summary(df,weight=\"weight_0\")\n\n\n# %%\nlogger.info(\"\\n\\n + by something\")\n_ = summary(\n    df,\n    weight=\"weight_0\",\n    by=\"year\"\n)\n_ = summary(\n    df,\n    weight=\"weight_0\",\n    by=[\"quarter\",\"year\"]\n)\n\n\n\n# %%\nlogger.info(\"\\n\\n with detailed stats and 4-sig digit rounding\")\n_ = summary(\n    df,\n    weight=\"weight_0\",\n    detailed=True,\n    drb_round=True\n)\n\n\n# %%\nlogger.info(\"\\n\\n with additional stats\")\nlogger.info(\"What is available:\")\nlogger.info(Statistics.available_stats())\n_ = summary(\n    df,\n    weight=\"weight_0\",\n    additional_stats=[\"q10\",\"q95\",\"n|not0\",\"share|not0\"]\n)\n\n\nlogger.info(\"Get them (but no need to print):\")\n\ndf_stats = summary(\n    df,\n    weight=\"weight_0\",\n    additional_stats=[\"q10\",\"q95\",\"n|not0\",\"share|not0\"],\n    print=False,\n)\n\nlogger.info(df_stats.collect())\n</code></pre> <p>View in separate window </p> <p>For proper variance estimation with complex survey designs or using bootstrap weights</p> Code (basic stats)Log (basic stats)Code (any custom stat)Log (any custom stat) <pre><code>from __future__ import annotations\n\nimport polars as pl\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary\nfrom survey_kit.statistics.calculator import StatCalculator,ComparisonItem\nfrom survey_kit.statistics.statistics import Statistics\nfrom survey_kit.statistics.replicates import Replicates\nfrom survey_kit.statistics.bootstrap import bayes_bootstrap\nfrom survey_kit.utilities.random import set_seed, generate_seed\n\nfrom survey_kit import logger, config\n\n# %%\n# Draw some random data\ndef gen_random_table(n_rows: int, n_replicates: int, seed: int):\n    set_seed(seed)\n    df = (\n        RandomData(n_rows=n_rows, seed=generate_seed())\n        .index(\"index\")\n        .integer(\"v_1\", 0, 10)\n        .integer(\"year\", 2016, 2021)\n        .integer(\"month\", 1, 12)\n        .integer(\"income\", 0, 100_000)\n        .integer(\"income_2\", 0, 120_000)\n    ).to_df()\n\n\n    #   Attach bootstrap weights to the data\n    df = (\n            pl.concat(\n                [\n                    df,\n                    bayes_bootstrap(\n                        n_rows=n_rows,\n                        n_draws=n_replicates+1,\n                        seed=generate_seed(),\n                        initial_weight_index=0,\n                        prefix=\"weight_\"\n                    )\n                ],\n                how=\"horizontal\"\n            )\n            .lazy()\n    )\n\n    df = df.with_columns(\n        pl.when(pl.col(\"year\").ne(2016)).then(pl.col(\"income\")).otherwise(pl.lit(0))\n    )\n\n    return df\n\nn_rows = 1_000\nn_replicates = 10\n\nlogger.info(\"Create two datasets to compare, with bootstrap weights (or repliate weights)\")\ndf = gen_random_table(n_rows, n_replicates, seed=1230)\ndf_compare = gen_random_table(n_rows, n_replicates, seed=9324)\n\n# %%\nlogger.info(\"What's the data look like\")\nlogger.info(df.head(10).collect())\nlogger.info(df_compare.head(10).collect())\n\n# %%\nlogger.info(\"Use the stat calculator class to get stats\")\nlogger.info(\"   The basic option, just get some stats\")\nstats = statistics=Statistics(\n    stats=[\"mean\", \"median|not0\"], \n    columns=[\"v_1\", \"income\", \"income_2\"]\n)\n\nlogger.info(\"What is available:\")\nlogger.info(Statistics.available_stats())\n\n# %%\nlogger.info(\"Estimate them\")\nsc = StatCalculator(\n    df,\n    statistics=stats,\n    weight=\"weight_0\",\n)\n\n\n\n# %%\nlogger.info(\"   Want standard errors?\")\nlogger.info(\"   Set up 'Replicates'\")\nlogger.info(\"       from data (with df)\")\nlogger.info(\"       NOTE:   if pass 'bootstrap=True', it will calculate \")\nlogger.info(\"               regular bootstrap SEs rather than balanced repeated replication SEs\")\nlogger.info(\"               (if you don't know what that means, pass bootstrap=True)\")\nreplicates_from_df = Replicates(\n    df=df,\n    weight_stub=\"weight_\",\n    bootstrap=True\n)\n\nlogger.info(\"       or just tell it the number (n_replicates)\")\nreplicates = Replicates(\n    weight_stub=\"weight_\",\n    n_replicates=n_replicates,\n    bootstrap=True\n)\nlogger.info(\"       Same either way\")\nlogger.info(replicates_from_df.rep_list)\nlogger.info(replicates.rep_list)\ndel replicates_from_df\n\n# %%\nlogger.info(\"\\n\\nNow let's calculate the stats with replicate-weighted standard errors\")\nlogger.info(\"   for df\")\nlogger.info(\"   (where each estimate has the SE below the estimate)\")\nsc = StatCalculator(\n    df,\n    statistics=stats,\n    weight=\"weight_0\",\n    replicates=replicates\n)\n\nlogger.info(\"   for df_compare\")\nsc_compare = StatCalculator(\n    df_compare,\n    statistics=stats,\n    weight=\"weight_0\",\n    replicates=replicates\n)\n\n\nlogger.info(\"Save them for later\")\nsc.save(f\"{config.path_temp_files}/sc\")\nsc_compare.save(f\"{config.path_temp_files}/sc_compare\")\n\ndel sc\ndel sc_compare\n\n\n# %%\nlogger.info(\"Load them back up\")\nsc = StatCalculator.load(f\"{config.path_temp_files}/sc\")\nsc_compare = StatCalculator.load(f\"{config.path_temp_files}/sc_compare\")\n\n\n\n# %%\nlogger.info(\"\\n\\nWe can compare them easily\")\nd_comparisons = sc.compare(sc_compare)\n\nlogger.info(\"Which gives us a dictionary of comparisons, with keys=['difference','ratio']\")\nlogger.info(d_comparisons['difference'])\nlogger.info(d_comparisons['ratio'])\n\n\n# %%\nlogger.info(\"\\n\\nLet's just get the difference between the means to medians, within the same data\")\nlogger.info(\"   For df\")\nsc_mean_median = sc.compare(\n    sc,\n    ratio=False,\n    compare_list_columns=[\n        ComparisonItem.Column(\n            \"mean\",\n            \"median (not 0)\",\n            name=\"median_mean\"\n        )\n    ])[\"difference\"]\n\nlogger.info(\"   For df_compare\")\nsc_compare_mean_median = sc_compare.compare(\n    sc_compare,\n    ratio=False,\n    compare_list_columns=[\n        ComparisonItem.Column(\n            \"mean\",\n            \"median (not 0)\",\n            name=\"median_mean\"\n        )\n    ])[\"difference\"]\n\n# %%\nlogger.info(\"\\n\\nNow, we can use the same tool to get a difference in difference comparison\")\nlogger.info(\"   i.e. [df(median|not0) - df(mean)] - [df_compare(median|not0) - df_compare(mean)]\")\nd_diff_in_diffs = sc_mean_median.compare(sc_compare_mean_median,ratio=False)\n\n\n\n# %%\nlogger.info(\"\\n\\nLet's get the ratio two variables\")\nlogger.info(\"   rather than the difference between two statistics for the same variable)\")\n\nlogger.info(\"   For df\")\nsc_inc_inc_2 = sc.compare(\n    sc,\n    difference=False,\n    compare_list_variables=[\n        ComparisonItem.Variable(\n            value1=\"income\",\n            value2=\"income_2\",\n            name=\"income_comp\"\n        )\n    ])[\"ratio\"]\n\nlogger.info(\"   For df_compare\")\nsc_inc_inc_2_compare = sc_compare.compare(\n    sc_compare,\n    difference=False,\n    compare_list_variables=[\n        ComparisonItem.Variable(\n            value1=\"income\",\n            value2=\"income_2\",\n            name=\"income_comp\"\n        )\n    ])[\"ratio\"]\n\n\n# %%\nlogger.info(\"\\n\\nAnd for fun, the diff-in-diff\")\nlogger.info(\"   i.e. [df(income_2)/df(income)] - [df_compare(income_2)/df_compare(income)]\")\n\nd_diff_in_diffs_2 = sc_inc_inc_2.compare(sc_inc_inc_2_compare,ratio=False)\n</code></pre> <p>View in separate window </p> <p>This works with any function that takes a dataframe and weight and returns estimates. You get proper variance estimation automatically.</p> <pre><code>from __future__ import annotations\n\nimport polars as pl\nfrom sklearn.linear_model import LinearRegression\n\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import summary, columns_from_list\nfrom survey_kit.statistics.calculator import StatCalculator\nfrom survey_kit.statistics.replicates import Replicates\nfrom survey_kit.statistics.bootstrap import bayes_bootstrap\nfrom survey_kit.utilities.random import set_seed, generate_seed\n\nfrom survey_kit import logger\n\n# %%\n# Draw some random data\ndef gen_random_table(n_rows: int, n_replicates: int, seed: int):\n    set_seed(seed)\n    df = (\n        RandomData(n_rows=n_rows, seed=generate_seed())\n        .index(\"index\")\n        .integer(\"v_1\", 0, 10)\n        .float(\"v_2\", 0, 100)\n        .float(\"v_3\", 0, 100)\n        .float(\"v_4\", 0, 100)\n\n        .np_distribution(\"e\", \"normal\", loc=0, scale=100)\n    )\n\n    df = df.to_df()\n\n    c_v1 = pl.col(\"v_1\")\n    c_v2 = pl.col(\"v_2\")\n    c_v3 = pl.col(\"v_3\")\n    c_v4 = pl.col(\"v_4\")\n    e = pl.col(\"e\")\n    df = df.with_columns(\n        y=10*c_v1-5*c_v2+100*c_v3+c_v4 + e\n    )\n\n    df = (\n            pl.concat(\n                [\n                    df,\n                    bayes_bootstrap(\n                        n_rows=n_rows,\n                        n_draws=n_replicates+1,\n                        seed=generate_seed(),\n                        initial_weight_index=0,\n                        prefix=\"weight_\"\n                    )\n                ],\n                how=\"horizontal\"\n            )\n            .lazy()\n    )\n\n    return df\n\n# %%\nn_rows = 1_000\nn_replicates = 10\n\nlogger.info(\"Create two datasets to compare, with replicate weights (or bootstrap weights)\")\ndf = gen_random_table(n_rows, n_replicates, seed=1230)\ndf_compare = gen_random_table(n_rows, n_replicates, seed=3254)\n\nlogger.info(\"The data\")\n_ = summary(df)\n\n\n# %%\nlogger.info(\"To run an arbitrary stat, you need\")\nlogger.info(\"   any function that takes a dataframe and a weight and returns a dataframe\")\nlogger.info(\"The function will be run for each boostrap/replicate weight\")\ndef run_regression(df:pl.LazyFrame | pl.DataFrame,\n                   y:str,\n                   X:list[str],\n                   weight:str) -&gt; pl.LazyFrame | pl.DataFrame:\n    df = df.lazy().collect()\n    model = LinearRegression()\n    model.fit(\n            X=df.select(X),\n            y=df.select(y),\n            sample_weight=df[weight]\n        )\n\n\n    df_betas = pl.DataFrame(\n            dict(\n                Variable=df.select(X).lazy().collect_schema().names() + [\"_Intercept_\"],\n                Beta=[\n                    float(vali)\n                    for vali in [float(coefi) for coefi in model.coef_[0]] + [float(model.intercept_[0])]\n                ],\n            )\n        )\n\n    return df_betas\n\n\n\n# %%\nlogger.info(\"The betas from the regression\")\ndf_betas = run_regression(df=df,y=\"y\",X=columns_from_list(df,[\"v_*\"]),weight=\"weight_0\")\nlogger.info(df_betas)\n\n\n\nlogger.info(\"Call StatCalculator.from_function\")\nlogger.info(\"   Pass in the data\")\nlogger.info(\"   'estimate_ids': the name of the column(s) that index the estimates\")\nlogger.info(\"   'arguments':    stable (non df and non-weight) arguments\")\nlogger.info(\"   'replicates':   weights to loop over\")\nsc = StatCalculator.from_function(\n    run_regression,\n    df=df,\n    estimate_ids=[\"Variable\"],\n    arguments=dict(\n        y=\"y\",\n        X=columns_from_list(df,[\"v_*\"])\n    ),\n    replicates=Replicates(\n        weight_stub=\"weight_\",\n        n_replicates=n_replicates,\n        bootstrap=True\n    ),\n    display=False\n)\n\n\nlogger.info(\"The result\")\nsc.print()\n\n\n\n\n# %%\nlogger.info(\"You can compare it like any other stat, run the same regression on another sample\")\nlogger.info(\"   Run the regression on a different random draw\")\nsc_compare = StatCalculator.from_function(\n    run_regression,\n    df=df_compare,\n    estimate_ids=[\"Variable\"],\n    arguments=dict(\n        y=\"y\",\n        X=columns_from_list(df,[\"v_*\"])\n    ),\n    replicates=Replicates(\n        weight_stub=\"weight_\",\n        n_replicates=n_replicates,\n        bootstrap=True\n    ),\n    display=False\n)\n\n# %%\nlogger.info(\"   And compare the results\")\nd_comp = sc.compare(sc_compare)\n</code></pre> <p>This works with any function that takes a dataframe and weight and returns estimates. You get proper variance estimation automatically.</p> <p>View in separate window </p> <p>Combine estimates properly across multiply imputed datasets using Rubin's rule.  This incorporates the bootstrap/replicate factor code internally.</p> Code (basic stats)Log (basic stats)Code for (any custom stat)Log (any custom stat) <pre><code>import sys\nimport os\nfrom pathlib import Path\nimport narwhals as nw\nimport polars as pl\n\nfrom survey_kit.utilities.dataframe import summary\nfrom survey_kit.imputation.srmi import SRMI\nfrom survey_kit import logger, config\n\nfrom survey_kit.statistics.multiple_imputation import MultipleImputation, mi_ses_from_function\nfrom survey_kit.statistics.calculator import StatCalculator\nfrom survey_kit.statistics.statistics import Statistics\nfrom survey_kit.statistics.replicates import Replicates\nfrom survey_kit.utilities.random import RandomData\nfrom survey_kit.utilities.dataframe import safe_height\nfrom survey_kit.statistics.bootstrap import bayes_bootstrap\nfrom survey_kit.utilities.random import set_seed, generate_seed\n\n\n# %%\nlogger.info(\"Load the SRMI results from that tutorial\")\npath_model = f\"{config.path_temp_files}/py_srmi_test_gbm\"\nsrmi = SRMI.load(path_model)\ndf_implicates = srmi.df_implicates\n\n# %%\nlogger.info(\"Draw bootstrap weights\")\nset_seed(8345)\nn_rows = safe_height(df_implicates[0])\nn_replicates = 10\n\ndf_weights = (\n        bayes_bootstrap(\n        n_rows=n_rows,\n        n_draws=n_replicates+1,\n        seed=generate_seed(),\n        prefix=\"weight_\",\n        initial_weight_index=0\n    )\n    .with_row_index(\"index\")\n)\n\n\n# %%\nlogger.info(\"What's the data look like\")\n_ = df_implicates.pipe(summary)\n_ = summary(df_weights)\n\n\n# %%\nlogger.info(\"What statistics do I want:\")\nlogger.info(\"   In this case, the mean of all variables that start with var_\")\nstats = Statistics(\n    stats=[\"mean\"],\n    columns=\"var_*\",\n)\n\n# %%\nlogger.info(\"Define the 'replicate' object, which tell is what the weight variables are\")\nreplicates = Replicates(weight_stub=\"weight_\", n_replicates=n_replicates)\n\n\n# %%\nlogger.info(\"Arguments that are getting passed to StatCalculator at each run\")\narguments = dict(\n    statistics=stats,\n    replicates=replicates\n)\n\n\n# %%\nlogger.info(\"Get the multiple imputation standard errofs by calling StatCalculator\")\nlogger.info(\"   for each implicate and each replicate factor\")\nlogger.info(\"   If you had 100 bootstrap weights and 5 imputation draws (implicates)\")\nlogger.info(\"   the StatCalculator calculation would run 5*100=500 times\")\nmi_results_seq = mi_ses_from_function(\n    delegate=StatCalculator,\n    df_implicates=df_implicates,\n    df_noimputes=df_weights,\n    index=[\"index\"],\n    arguments=arguments,\n    join_on=[\"Variable\"],\n    parallel=False,\n)\n\n\n# %%\nlogger.info(\"Do it again, but run each implicate in it's own process and collect the results\")\nlogger.info(\"   This will split up the job and \")\nlogger.info(\"   use all your cpus (or what you set in config.cpus), dividing them up among the jobs\")\nmi_results = mi_ses_from_function(\n    delegate=StatCalculator,\n    df_implicates=df_implicates,\n    df_noimputes=df_weights,\n    index=[\"index\"],\n    arguments=arguments,\n    join_on=[\"Variable\"],\n    parallel=True,\n)\n\n# %%\nlogger.info(\"The sequential (non-parallel) job results\")\nmi_results.print()\n\n# %%\nlogger.info(\"The parallel job results\")\nmi_results_seq.print()\n\n# %%\nlogger.info(\"Save them for later\")\nmi_results.save(f\"{config.path_temp_files}/mi_sequential\")\nmi_results_seq.save(f\"{config.path_temp_files}/mi_parallel\")\n\ndel mi_results\ndel mi_results_seq\n\n# %%\nlogger.info(\"Load them back up\")\nmi_results = MultipleImputation.load(f\"{config.path_temp_files}/mi_sequential\")\nmi_results_seq = MultipleImputation.load(f\"{config.path_temp_files}/mi_parallel\")\n\n\n# %%\nlogger.info(\"Compare them\")\nd_comparison = mi_results.compare(mi_results_seq)\nlogger.info(\"   Print the mi_results_seq - mi_results\")\nd_comparison[\"difference\"].print()\nlogger.info(\"   Print the (mi_results_seq-mi_results)/mi_results\")\nd_comparison[\"ratio\"].print()\n</code></pre> <p>View in separate window </p> <p>This works with any function that takes a dataframe and weight and returns estimates. You get proper variance estimation (replicate weights + MI) automatically.</p> <pre><code>from __future__ import annotations\n\nimport polars as pl\n\nfrom survey_kit.utilities.dataframe import summary, join_wrapper\nfrom survey_kit.imputation.srmi import SRMI\nfrom survey_kit import logger, config\n\nfrom survey_kit.statistics.multiple_imputation import mi_ses_from_function\nfrom survey_kit.statistics.calculator import StatCalculator\nfrom survey_kit.statistics.replicates import Replicates\nfrom survey_kit.utilities.dataframe import safe_height\nfrom survey_kit.statistics.bootstrap import bayes_bootstrap\n\n# %%\nlogger.info(\"Load the data from the SRMI tutorial\")\npath_model = f\"{config.path_temp_files}/py_srmi_test_gbm\"\nsrmi = SRMI.load(path_model)\ndf_implicates = srmi.df_implicates\n\n# %%\nlogger.info(\"Make random weights for the bootstrap\")\nn_rows = safe_height(df_implicates[0])\nn_replicates = 10\n\ndf_weights = (\n        bayes_bootstrap(\n        n_rows=n_rows,\n        n_draws=n_replicates+1,\n        seed=8345,\n        prefix=\"weight_\",\n        initial_weight_index=0,\n    )\n    .with_row_index(\"index\")\n)\n\n# %%\nlogger.info(\"What's the data look like\")\nlogger.info(\"   The SRMI data - both implicates\")\n_ = df_implicates.pipe(summary)\nlogger.info(\"   The weights\")\n_ = summary(df_weights)\n\n# %%\nlogger.info(\"To run an arbitrary stat, you need\")\nlogger.info(\"   any function that takes a dataframe and a weight and returns a dataframe\")\nlogger.info(\"The function will be run for each boostrap/replicate weight\")\nlogger.info(\"   Note: you may need to import packages within the function\")\nlogger.info(\"   if executed in parallel (it will get pickled and reloaded)\")\ndef run_regression(df:pl.LazyFrame | pl.DataFrame,\n                   y:str,\n                   X:list[str],\n                   weight:str=\"\") -&gt; pl.LazyFrame | pl.DataFrame:\n    import polars as pl\n    from sklearn.linear_model import LinearRegression\n\n    df = df.lazy().collect()\n    model = LinearRegression()\n    d_weight = {}\n    if weight != \"\":\n        d_weight[\"sample_weight\"] = df[weight]\n\n    model.fit(\n        X=df.select(X),\n        y=df.select(y),\n        **d_weight\n    )\n\n    df_betas = pl.DataFrame(\n            dict(\n                Variable=df.select(X).lazy().collect_schema().names() + [\"_Intercept_\"],\n                Beta=[\n                    float(vali)\n                    for vali in [float(coefi) for coefi in model.coef_[0]] + [float(model.intercept_[0])]\n                ],\n            )\n        )\n\n    return df_betas\n\n\n\n# %%\nlogger.info(\"The betas from the regression (no weights, implicate 0)\")\ny = \"var_gbm2\"\nX = [\"var2\",\"var3\",\"var4\",\"var5\"]\ndf_betas = run_regression(df=df_implicates[0],y=y,X=X)\nlogger.info(df_betas)\n\n\n\n\n# %%\nlogger.info(\"Confirming how to run it on one implicate\")\narguments_sc = dict(\n    y=y,\n    X=X,\n)\nreplicates = Replicates(\n    weight_stub=\"weight_\",\n    n_replicates=n_replicates,\n    bootstrap=True\n)\nsc_compare = StatCalculator.from_function(\n    run_regression,\n    df=join_wrapper(df_implicates[0],df_weights,on=[\"index\"],how=\"left\"),\n    estimate_ids=[\"Variable\"],\n    arguments=arguments_sc,\n    replicates=replicates,\n    display=False\n)\nsc_compare.print()\n\n\n\n# %%\nlogger.info(\"Now the full function\")\nlogger.info(\"   Note the nested arguments...\")\nlogger.info(\"       Maybe not the best API, but it does mean\")\nlogger.info(\"       that you can run ANYTHING that takes in a\")\nlogger.info(\"       dataframe and a weight and returns a dataframe of estiamtes\")\nlogger.info(\"        and run it through the bootstrap + MI process\")\narguments_sc = dict(\n    y=y,\n    X=X,\n)\narguments_mi = dict(\n    delegate=run_regression,\n    arguments=arguments_sc,\n    estimate_ids=[\"Variable\"],\n    replicates=replicates,\n)\n\nmi_results_seq = mi_ses_from_function(\n    delegate=StatCalculator.from_function,\n    df_implicates=df_implicates,\n    path_srmi=path_model,\n    df_noimputes=df_weights,\n    index=[\"index\"],\n    arguments=arguments_mi,\n    join_on=[\"Variable\"],\n    parallel=False,\n)\n\nmi_results_seq.print()\n\n# %%\nlogger.info(\"For parallel, it's better to pass the path of the model\")\nlogger.info(\"   To avoid unnecessary I/O\")\nlogger.info(\"   (For sequential it doesn't really matter)\")\nmi_results = mi_ses_from_function(\n    delegate=StatCalculator.from_function,\n    # df_implicates=df_implicates,\n    path_srmi=path_model,\n    df_noimputes=df_weights,\n    index=[\"index\"],\n    arguments=arguments_mi,\n    join_on=[\"Variable\"],\n    parallel=True,\n)\n\n# %%\nlogger.info(\"The results\")\nmi_results.print()\nmi_results_seq.print()\n\n# %%\nlogger.info(\"Compare two sets of results\")\nd_comparison = mi_results.compare(mi_results_seq)\nd_comparison[\"difference\"].print()\nd_comparison[\"ratio\"].print()\n</code></pre> <p>This works with any function that takes a dataframe and weight and returns estimates. You get proper variance estimation (replicate weights + MI) automatically.</p> <p>View in separate window </p>"}]}